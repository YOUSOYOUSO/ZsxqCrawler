"""
çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ - FastAPI åç«¯æœåŠ¡
æä¾›RESTful APIæ¥å£æ¥æ“ä½œç°æœ‰çš„çˆ¬è™«åŠŸèƒ½
"""

import os
import sys
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
from contextlib import asynccontextmanager
import json
import requests

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import uvicorn
import random
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆapp/main.py åœ¨ app ç›®å½•ä¸‹ï¼‰
project_root = str(Path(__file__).resolve().parents[1])
if project_root not in sys.path:
    sys.path.append(project_root)
from modules.shared.paths import get_config_path

# å¯¼å…¥ç°æœ‰çš„ä¸šåŠ¡é€»è¾‘æ¨¡å—
from modules.zsxq.zsxq_interactive_crawler import ZSXQInteractiveCrawler, load_config
from modules.zsxq.zsxq_database import ZSXQDatabase
from modules.zsxq.zsxq_file_database import ZSXQFileDatabase
from modules.shared.db_path_manager import get_db_path_manager
# ä½¿ç”¨SQLè´¦å·ç®¡ç†å™¨
from modules.accounts.accounts_sql_manager import get_accounts_sql_manager
from modules.accounts.account_info_db import get_account_info_db
from modules.zsxq.zsxq_columns_database import ZSXQColumnsDatabase
from modules.shared.logger_config import log_info, log_warning, log_error, log_exception, log_debug, ensure_configured
from api.app_factory import register_core_routers
from api.services.account_resolution_service import (
    build_account_group_detection,
    clear_account_detect_cache,
    fetch_groups_from_api,
    get_account_summary_for_group_auto,
    get_cookie_for_group,
)
from api.deps.container import get_task_runtime
from api.schemas.models import (
    AccountCreateRequest,
    AssignGroupAccountRequest,
    ColumnsSettingsRequest,
    ConfigModel,
    CrawlBehaviorSettingsRequest,
    CrawlHistoricalRequest,
    CrawlSettingsRequest,
    CrawlTimeRangeRequest,
    CrawlerSettingsRequest,
    DownloaderSettingsRequest,
    FileDownloadRequest,
    GlobalCrawlRequest,
    GlobalFileCollectRequest,
    GlobalFileDownloadRequest,
    ScanFilterConfigRequest,
)

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
ensure_configured()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šå¯åŠ¨æ—¶æ‰«ææœ¬åœ°ç¾¤"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    try:
        await asyncio.to_thread(scan_local_groups)
    except Exception as e:
        print(f"âš ï¸ å¯åŠ¨æ‰«ææœ¬åœ°ç¾¤å¤±è´¥: {e}")
    yield
    # å…³é—­æ—¶æ‰§è¡Œï¼ˆå¦‚éœ€è¦å¯æ·»åŠ æ¸…ç†é€»è¾‘ï¼‰


app = FastAPI(
    title="çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ API",
    description="ä¸ºçŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨æä¾›RESTful APIæ¥å£",
    version="1.0.0",
    lifespan=lifespan
)
register_core_routers(app)

def _parse_cors_origins() -> List[str]:
    """
    è§£æ CORS ç™½åå•ï¼Œé»˜è®¤ä»…å…è®¸æœ¬åœ°å¼€å‘ç«¯å£ã€‚
    é€šè¿‡ç¯å¢ƒå˜é‡ CORS_ALLOW_ORIGINS ä»¥é€—å·åˆ†éš”è¦†ç›–ã€‚
    """
    raw = os.environ.get(
        "CORS_ALLOW_ORIGINS",
        "http://localhost:3060,http://127.0.0.1:3060"
    )
    origins = [origin.strip().rstrip("/") for origin in raw.split(",") if origin.strip()]
    return origins or ["http://localhost:3060"]

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=_parse_cors_origins(),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡å­˜å‚¨çˆ¬è™«å®ä¾‹å’Œä»»åŠ¡çŠ¶æ€
crawler_instance: Optional[ZSXQInteractiveCrawler] = None
task_runtime = get_task_runtime()
current_tasks: Dict[str, Dict[str, Any]] = task_runtime.tasks
task_logs: Dict[str, List[str]] = task_runtime.logs  # å­˜å‚¨ä»»åŠ¡æ—¥å¿—
task_counter = 0  # legacy task id counter (to be removed after full router migration)
sse_connections: Dict[str, List] = {}  # å­˜å‚¨SSEè¿æ¥
task_stop_flags: Dict[str, bool] = task_runtime.stop_flags  # ä»»åŠ¡åœæ­¢æ ‡å¿—
file_downloader_instances: Dict[str, Any] = {}  # å­˜å‚¨æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹

# è°ƒåº¦å™¨æ—¥å¿—é’©å­
def scheduler_log_callback(msg: str):
    task_runtime.set_scheduler_log(msg, cap=500)

# è°ƒåº¦å™¨çŠ¶æ€æ›´æ–°é’©å­
def scheduler_status_callback(status: str, message: str):
    update_task("scheduler", status, message)

# å»¶è¿Ÿå¯¼å…¥å¹¶åˆå§‹åŒ–è°ƒåº¦å™¨å›è°ƒ
try:
    from app.scheduler.auto_scheduler import get_scheduler
    sc = get_scheduler()
    sc.set_log_callback(scheduler_log_callback)
    sc.set_status_callback(scheduler_status_callback)
    initial_status = sc.get_status()
    current_tasks["scheduler"]["status"] = initial_status.get("state", "stopped")
    current_tasks["scheduler"]["message"] = "è‡ªåŠ¨è°ƒåº¦ç³»ç»Ÿ"
    current_tasks["scheduler"]["updated_at"] = datetime.now().isoformat()
except ImportError:
    pass

# =========================
# æœ¬åœ°ç¾¤æ‰«æï¼ˆoutput ç›®å½•ï¼‰
# =========================

# å¯é…ç½®ï¼šé»˜è®¤ ./outputï¼›å¯é€šè¿‡ç¯å¢ƒå˜é‡ OUTPUT_DIR è¦†ç›–
LOCAL_OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "output")
# å¤„ç†ä¸Šé™ä¿æŠ¤ï¼Œé»˜è®¤ 10000ï¼›å¯é€šè¿‡ LOCAL_GROUPS_SCAN_LIMIT è¦†ç›–
try:
    LOCAL_SCAN_LIMIT = int(os.environ.get("LOCAL_GROUPS_SCAN_LIMIT", "10000"))
except Exception:
    LOCAL_SCAN_LIMIT = 10000

# æœ¬åœ°ç¾¤ç¼“å­˜
_local_groups_cache = {
    "ids": set(),     # set[int]
    "scanned_at": 0.0 # epoch ç§’
}


def _safe_listdir(path: str):
    """å®‰å…¨åˆ—ç›®å½•ï¼Œå¼‚å¸¸ä¸æŠ›å‡ºï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶å‘Šè­¦"""
    try:
        return os.listdir(path)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–ç›®å½• {path}: {e}")
        return []


def _collect_numeric_dirs(base: str, limit: int) -> set:
    r"""
    æ‰«æ base çš„ä¸€çº§å­ç›®å½•ï¼Œæ”¶é›†çº¯æ•°å­—ç›®å½•åï¼ˆ^\d+$ï¼‰ä½œä¸ºç¾¤IDã€‚
    å¿½ç•¥ï¼šéç›®å½•ã€è½¯é“¾æ¥ã€éšè—ç›®å½•ï¼ˆä»¥ . å¼€å¤´ï¼‰ã€‚
    """
    ids = set()
    if not base:
        return ids

    base_abs = os.path.abspath(base)
    if not (os.path.exists(base_abs) and os.path.isdir(base_abs)):
        # è§†ä¸ºç©ºé›†åˆï¼Œä¸æŠ¥é”™
        print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨æˆ–ä¸å¯è¯»: {base_abs}ï¼Œè§†ä¸ºç©ºé›†åˆ")
        return ids

    processed = 0
    for name in _safe_listdir(base_abs):
        # éšè—ç›®å½•
        if not name or name.startswith('.'):
            continue

        path = os.path.join(base_abs, name)
        try:
            # è½¯é“¾æ¥/éç›®å½•å¿½ç•¥
            if os.path.islink(path) or not os.path.isdir(path):
                continue

            # ä»…çº¯æ•°å­—ç›®å½•å
            if name.isdigit():
                ids.add(int(name))
                processed += 1
                if processed >= limit:
                    print(f"âš ï¸ å­ç›®å½•æ•°é‡è¶…è¿‡ä¸Šé™ {limit}ï¼Œå·²æˆªæ–­")
                    break
        except Exception:
            # å•é¡¹å¤±è´¥å®‰å…¨é™çº§
            continue

    return ids


def scan_local_groups(output_dir: str = None, limit: int = None) -> set:
    """
    æ‰«ææœ¬åœ° output çš„ä¸€çº§å­ç›®å½•ï¼Œè·å–ç¾¤IDé›†åˆã€‚
    åŒæ—¶å…¼å®¹ output/databases ç»“æ„ï¼ˆå¦‚å­˜åœ¨ï¼‰ã€‚
    åŒæ­¥æ‰§è¡Œï¼ˆç”¨äºæ‰‹åŠ¨åˆ·æ–°æˆ–å¼ºåˆ¶åˆ·æ–°ï¼‰ï¼Œå¼‚å¸¸å®‰å…¨é™çº§ã€‚
    """
    try:
        odir = output_dir or LOCAL_OUTPUT_DIR
        lim = int(limit or LOCAL_SCAN_LIMIT)

        # ä¸»è·¯å¾„ï¼šä»…æ‰«æ output çš„ä¸€çº§å­ç›®å½•
        ids_primary = _collect_numeric_dirs(odir, lim)

        # å…¼å®¹è·¯å¾„ï¼šoutput/databases çš„ä¸€çº§å­ç›®å½•ï¼ˆè‹¥å­˜åœ¨ï¼‰
        ids_secondary = _collect_numeric_dirs(os.path.join(odir, "databases"), lim)

        ids = set(ids_primary) | set(ids_secondary)

        # æ›´æ–°ç¼“å­˜
        _local_groups_cache["ids"] = ids
        _local_groups_cache["scanned_at"] = time.time()

        return ids
    except Exception as e:
        print(f"âš ï¸ æœ¬åœ°ç¾¤æ‰«æå¼‚å¸¸: {e}")
        # å®‰å…¨é™çº§ä¸ºæ—§ç¼“å­˜
        return _local_groups_cache.get("ids", set())


def get_cached_local_group_ids(force_refresh: bool = False) -> set:
    """
    è·å–ç¼“å­˜ä¸­çš„æœ¬åœ°ç¾¤IDé›†åˆï¼›å¯é€‰å¼ºåˆ¶åˆ·æ–°ã€‚
    æœªæ‰«æè¿‡æˆ–è¦æ±‚å¼ºæ›´æ—¶è§¦å‘åŒæ­¥æ‰«æã€‚
    """
    if force_refresh or not _local_groups_cache.get("ids"):
        return scan_local_groups()
    return _local_groups_cache.get("ids", set())


# Pydanticæ¨¡å‹å®šä¹‰å·²è¿ç§»åˆ° api/schemas/models.py

# è¾…åŠ©å‡½æ•°
def get_crawler(log_callback=None) -> ZSXQInteractiveCrawler:
    """è·å–çˆ¬è™«å®ä¾‹"""
    global crawler_instance
    if crawler_instance is None:
        config = load_config()
        if not config:
            raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")

        auth_config = config.get('auth', {})

        cookie = auth_config.get('cookie', '')
        group_id = auth_config.get('group_id', '')

        if cookie == "your_cookie_here" or group_id == "your_group_id_here" or not cookie or not group_id:
            raise HTTPException(status_code=400, detail="è¯·å…ˆåœ¨ config/app.toml ä¸­é…ç½®Cookieå’Œç¾¤ç»„ID")

        # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler_instance = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)

    return crawler_instance

def get_crawler_for_group(group_id: str, log_callback=None) -> ZSXQInteractiveCrawler:
    """ä¸ºæŒ‡å®šç¾¤ç»„è·å–çˆ¬è™«å®ä¾‹"""
    config = load_config()
    if not config:
        raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")

    # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
    cookie = get_cookie_for_group(group_id)

    if not cookie or cookie == "your_cookie_here":
        raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆåœ¨è´¦å·ç®¡ç†æˆ– config/app.toml ä¸­é…ç½®")

    # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æŒ‡å®šç¾¤ç»„çš„æ•°æ®åº“è·¯å¾„
    path_manager = get_db_path_manager()
    db_path = path_manager.get_topics_db_path(group_id)

    return ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)

def get_crawler_safe() -> Optional[ZSXQInteractiveCrawler]:
    """å®‰å…¨è·å–çˆ¬è™«å®ä¾‹ï¼Œé…ç½®æœªè®¾ç½®æ—¶è¿”å›None"""
    try:
        return get_crawler()
    except HTTPException:
        return None

def get_primary_cookie() -> Optional[str]:
    """
    è·å–å½“å‰ä¼˜å…ˆä½¿ç”¨çš„Cookieï¼š
    1. è‹¥è´¦å·ç®¡ç†ä¸­å­˜åœ¨è´¦å·ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè´¦å·çš„Cookie
    2. å¦åˆ™å›é€€åˆ° config/app.toml ä¸­çš„ Cookieï¼ˆè‹¥å·²é…ç½®ï¼‰
    """
    # 1. ç¬¬ä¸€ä¸ªè´¦å·
    try:
        sql_mgr = get_accounts_sql_manager()
        first_acc = sql_mgr.get_first_account(mask_cookie=False)
        if first_acc:
            cookie = (first_acc.get("cookie") or "").strip()
            if cookie:
                return cookie
    except Exception:
        pass

    # 2. config/app.toml ä¸­çš„ Cookie
    try:
        config = load_config()
        if not config:
            return None
        auth_config = config.get("auth", {}) or {}
        cookie = (auth_config.get("cookie") or "").strip()
        if cookie and cookie != "your_cookie_here":
            return cookie
    except Exception:
        return None

    return None


def is_configured() -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²é…ç½®è‡³å°‘ä¸€ä¸ªå¯ç”¨çš„è®¤è¯Cookieï¼ˆè´¦å·ç®¡ç†æˆ– config/app.toml å‡å¯ï¼‰"""
    return get_primary_cookie() is not None

def create_task(task_type: str, description: str) -> str:
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    task_id = task_runtime.create_task(task_type=task_type, message=description, status="pending")
    add_task_log(task_id, f"ä»»åŠ¡åˆ›å»º: {description}")
    return task_id

def add_task_log(task_id: str, log_message: str):
    """æ·»åŠ ä»»åŠ¡æ—¥å¿—"""
    task_runtime.append_log(task_id, log_message)
    logs = task_logs.get(task_id, [])
    formatted_log = logs[-1] if logs else log_message

    # å¹¿æ’­æ—¥å¿—åˆ°æ‰€æœ‰SSEè¿æ¥
    broadcast_log(task_id, formatted_log)

def broadcast_log(task_id: str, log_message: str):
    """å¹¿æ’­æ—¥å¿—åˆ°SSEè¿æ¥"""
    # è¿™ä¸ªå‡½æ•°ç°åœ¨ä¸»è¦ç”¨äºå­˜å‚¨æ—¥å¿—ï¼Œå®é™…çš„SSEå¹¿æ’­åœ¨streamç«¯ç‚¹ä¸­å®ç°
    pass

def build_stealth_headers(cookie: str) -> Dict[str, str]:
    """æ„é€ æ›´æ¥è¿‘å®˜ç½‘çš„è¯·æ±‚å¤´ï¼Œæå‡æˆåŠŸç‡"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    ]
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7",
        "Cache-Control": "no-cache",
        "Cookie": cookie,
        "Origin": "https://wx.zsxq.com",
        "Pragma": "no-cache",
        "Priority": "u=1, i",
        "Referer": "https://wx.zsxq.com/",
        "Sec-Ch-Ua": "\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"",
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": "\"Windows\"",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-site",
        "User-Agent": random.choice(user_agents),
        "X-Aduid": "a3be07cd6-dd67-3912-0093-862d844e7fe",
        "X-Request-Id": f"dcc5cb6ab-1bc3-8273-cc26-{random.randint(100000000000, 999999999999)}",
        "X-Signature": "733fd672ddf6d4e367730d9622cdd1e28a4b6203",
        "X-Timestamp": str(int(time.time())),
        "X-Version": "2.77.0",
    }
    return headers

def update_task(task_id: str, status: str, message: str, result: Optional[Dict[str, Any]] = None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    task_runtime.update_task(task_id=task_id, status=status, message=message, result=result)
    if task_id in current_tasks:
        add_task_log(task_id, f"çŠ¶æ€æ›´æ–°: {message}")


def _to_iso_datetime(value: Any) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, datetime):
        return value.isoformat()
    if isinstance(value, str):
        v = value.strip()
        if not v:
            return None
        try:
            return datetime.fromisoformat(v).isoformat()
        except Exception:
            return v
    return str(value)


def _task_sort_key(task: Dict[str, Any]) -> str:
    return (
        _to_iso_datetime(task.get("updated_at"))
        or _to_iso_datetime(task.get("created_at"))
        or ""
    )


def _normalize_task_snapshot(task: Dict[str, Any]) -> Dict[str, Any]:
    return {
        **task,
        "created_at": _to_iso_datetime(task.get("created_at")),
        "updated_at": _to_iso_datetime(task.get("updated_at")),
    }


def _task_category(task_type: str) -> str:
    t = str(task_type or "").strip()
    if t == "scheduler":
        return "scheduler"
    if t.startswith("global_crawl") or t.startswith("crawl_"):
        return "crawl"
    if t.startswith("global_files_collect") or t.startswith("global_files_download"):
        return "files"
    if t.startswith("global_analyze_performance") or t.startswith("global_analyze") or t.startswith("stock_scan_"):
        return "analyze"
    return "other"


def _build_task_summary() -> Dict[str, Any]:
    running_status = {"pending", "running", "stopping"}
    terminal_status = {"completed", "failed", "cancelled", "stopped", "idle"}

    running_by_type: Dict[str, Dict[str, Any]] = {}
    latest_by_type: Dict[str, Dict[str, Any]] = {}
    running_by_task_type: Dict[str, Dict[str, Any]] = {}
    latest_by_task_type: Dict[str, Dict[str, Any]] = {}

    grouped: Dict[str, List[Dict[str, Any]]] = {"crawl": [], "files": [], "analyze": [], "scheduler": [], "other": []}
    for raw_task in current_tasks.values():
        task = _normalize_task_snapshot(raw_task)
        task_type = str(task.get("type", ""))
        grouped[_task_category(str(task.get("type", "")))].append(task)
        if task_type:
            prev_running = running_by_task_type.get(task_type)
            if str(task.get("status", "")) in running_status and (prev_running is None or _task_sort_key(task) > _task_sort_key(prev_running)):
                running_by_task_type[task_type] = task
            prev_latest = latest_by_task_type.get(task_type)
            if prev_latest is None or _task_sort_key(task) > _task_sort_key(prev_latest):
                latest_by_task_type[task_type] = task

    for category, items in grouped.items():
        if not items:
            continue
        items_sorted = sorted(items, key=_task_sort_key, reverse=True)
        running_items = [t for t in items_sorted if str(t.get("status", "")) in running_status]
        if running_items:
            running_by_type[category] = running_items[0]

        terminal_items = [t for t in items_sorted if str(t.get("status", "")) in terminal_status]
        latest_by_type[category] = terminal_items[0] if terminal_items else items_sorted[0]

    try:
        from app.scheduler.auto_scheduler import get_scheduler
        scheduler_snapshot = get_scheduler().get_status()
    except Exception:
        scheduler_snapshot = {}

    return {
        "running_by_type": running_by_type,
        "latest_by_type": latest_by_type,
        "running_by_task_type": running_by_task_type,
        "latest_by_task_type": latest_by_task_type,
        "scheduler": scheduler_snapshot,
    }

def stop_task(task_id: str) -> bool:
    """åœæ­¢ä»»åŠ¡"""
    if task_id not in current_tasks:
        return False

    task = current_tasks[task_id]

    if task["status"] not in ["pending", "running"]:
        return False

    # è®¾ç½®åœæ­¢æ ‡å¿—
    task_runtime.request_stop(task_id)
    add_task_log(task_id, "ğŸ›‘ æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œæ­£åœ¨åœæ­¢ä»»åŠ¡...")

    # å¦‚æœæœ‰çˆ¬è™«å®ä¾‹ï¼Œä¹Ÿè®¾ç½®çˆ¬è™«çš„åœæ­¢æ ‡å¿—
    global crawler_instance, file_downloader_instances
    if crawler_instance:
        crawler_instance.set_stop_flag()

    # å¦‚æœæœ‰æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹ï¼Œä¹Ÿè®¾ç½®åœæ­¢æ ‡å¿—
    if task_id in file_downloader_instances:
        downloader = file_downloader_instances[task_id]
        downloader.set_stop_flag()

    # ç‰¹æ®Šå¤„ç†è°ƒåº¦å™¨ï¼šè°ƒç”¨å…¶å†…éƒ¨ stop æ–¹æ³•
    if task_id == "scheduler":
        try:
            from app.scheduler.auto_scheduler import get_scheduler
            update_task(task_id, "stopping", "è°ƒåº¦å™¨åœæ­¢è¯·æ±‚å·²å‘é€ï¼Œæ­£åœ¨æ”¶å°¾...")
            # ä½¿ç”¨ create_task å¼‚æ­¥åœæ­¢ï¼Œé¿å…é˜»å¡ API
            asyncio.create_task(get_scheduler().stop())
            return True
        except Exception as e:
            log_error(f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {e}")
            return False

    update_task(task_id, "cancelled", "ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")

    return True

def is_task_stopped(task_id: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢"""
    return task_runtime.is_stopped(task_id)

# åº”ç”¨è®¾ç½®ï¼ˆæŒä¹…åŒ–ï¼‰
CRAWL_SETTINGS_DEFAULTS = {
    "crawl_interval_min": 2.0,
    "crawl_interval_max": 5.0,
    "long_sleep_interval_min": 180.0,
    "long_sleep_interval_max": 300.0,
    "pages_per_batch": 15,
}

APP_SETTINGS_PATH = os.path.join(get_db_path_manager().base_dir, "app_settings.json")


def _load_app_settings() -> Dict[str, Any]:
    """è¯»å–åº”ç”¨è®¾ç½®ï¼ˆå¤±è´¥æ—¶é™çº§ä¸ºç©ºé…ç½®ï¼‰"""
    try:
        if not os.path.exists(APP_SETTINGS_PATH):
            return {}
        with open(APP_SETTINGS_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, dict) else {}
    except Exception as e:
        log_warning(f"è¯»å–åº”ç”¨è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return {}


def _save_app_settings(settings: Dict[str, Any]):
    """ä¿å­˜åº”ç”¨è®¾ç½®"""
    os.makedirs(os.path.dirname(APP_SETTINGS_PATH), exist_ok=True)
    with open(APP_SETTINGS_PATH, "w", encoding="utf-8") as f:
        json.dump(settings, f, ensure_ascii=False, indent=2)


def _get_crawl_settings() -> Dict[str, Any]:
    """è¯»å–å¹¶åˆå¹¶çˆ¬å–è®¾ç½®"""
    settings = _load_app_settings()
    crawl_settings = settings.get("crawl", {}) if isinstance(settings, dict) else {}
    merged = dict(CRAWL_SETTINGS_DEFAULTS)
    if isinstance(crawl_settings, dict):
        merged.update({k: v for k, v in crawl_settings.items() if k in CRAWL_SETTINGS_DEFAULTS})
    return merged


def _update_crawl_settings(settings: Dict[str, Any]) -> Dict[str, Any]:
    """æ›´æ–°å¹¶æŒä¹…åŒ–çˆ¬å–è®¾ç½®"""
    all_settings = _load_app_settings()
    if not isinstance(all_settings, dict):
        all_settings = {}
    all_settings["crawl"] = settings
    _save_app_settings(all_settings)
    return settings


def _resolve_crawl_interval_values(request_obj: Optional[Any]) -> Dict[str, Any]:
    """
    è®¡ç®—å®é™…ç”Ÿæ•ˆçš„çˆ¬å–é—´éš”å‚æ•°ï¼š
    - ä¼˜å…ˆä½¿ç”¨è¯·æ±‚é‡Œçš„æ˜¾å¼å€¼
    - æœªæä¾›æ—¶å›é€€åˆ°æŒä¹…åŒ–è®¾ç½®
    """
    persisted = _get_crawl_settings()
    return {
        "crawl_interval_min": getattr(request_obj, "crawlIntervalMin", None) or persisted["crawl_interval_min"],
        "crawl_interval_max": getattr(request_obj, "crawlIntervalMax", None) or persisted["crawl_interval_max"],
        "long_sleep_interval_min": getattr(request_obj, "longSleepIntervalMin", None) or persisted["long_sleep_interval_min"],
        "long_sleep_interval_max": getattr(request_obj, "longSleepIntervalMax", None) or persisted["long_sleep_interval_max"],
        "pages_per_batch": getattr(request_obj, "pagesPerBatch", None) or persisted["pages_per_batch"],
    }

# APIè·¯ç”±å®šä¹‰
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ API æœåŠ¡", "version": "1.0.0"}

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now()}

@app.get("/api/meta/features")
async def get_meta_features():
    """å‰ç«¯èƒ½åŠ›æ¢æµ‹ï¼Œé¿å…ç‰ˆæœ¬ä¸ä¸€è‡´å¯¼è‡´çš„404/å­—æ®µç¼ºå¤±ã€‚"""
    return {
        "global_sector_topics": True,
        "scheduler_v2_status": True,
        "scheduler_next_runs": True,
        "global_scan_filter": True,
        "market_data_persistence": True,
    }

@app.get("/api/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    try:
        config = load_config()
        auth_config = (config or {}).get('auth', {}) if config else {}
        cookie = auth_config.get('cookie', '') if auth_config else ''

        configured = is_configured()

        # éšè—æ•æ„Ÿä¿¡æ¯ï¼Œä»…è¿”å›é…ç½®çŠ¶æ€å’Œä¸‹è½½ç›¸å…³é…ç½®
        return {
            "configured": configured,
            "auth": {
                "cookie": "***" if cookie and cookie != "your_cookie_here" else "æœªé…ç½®",
            },
            "database": config.get('database', {}) if config else {},
            "download": config.get('download', {}) if config else {}
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é…ç½®å¤±è´¥: {str(e)}")

@app.post("/api/config")
async def update_config(config: ConfigModel):
    """æ›´æ–°é…ç½®"""
    try:
        # åˆ›å»ºé…ç½®å†…å®¹
        config_content = f"""# çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨é…ç½®æ–‡ä»¶
# é€šè¿‡Webç•Œé¢è‡ªåŠ¨ç”Ÿæˆ

[auth]
# çŸ¥è¯†æ˜Ÿçƒç™»å½•Cookie
cookie = "{config.cookie}"

[download]
# ä¸‹è½½ç›®å½•
dir = "downloads"

[market_data]
enabled = true
db_path = "output/databases/akshare_market.db"
adjust = "qfq"
close_finalize_time = "15:05"
bootstrap_mode = "full_history"
bootstrap_batch_size = 200
sync_retry_max = 3
sync_retry_backoff_seconds = 1.0
"""

        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = str(get_config_path("app.toml"))
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)

        # é‡ç½®çˆ¬è™«å®ä¾‹ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®
        global crawler_instance
        crawler_instance = None

        return {"message": "é…ç½®æ›´æ–°æˆåŠŸ", "success": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")

# è´¦å·ç®¡ç† API
@app.get("/api/accounts")
async def list_accounts():
    """è·å–æ‰€æœ‰è´¦å·åˆ—è¡¨"""
    try:
        sql_mgr = get_accounts_sql_manager()
        accounts = sql_mgr.get_accounts(mask_cookie=True)
        return {"accounts": accounts}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve account list: {str(e)}")

@app.post("/api/accounts")
async def create_account(request: AccountCreateRequest):
    """åˆ›å»ºæ–°è´¦å·"""
    try:
        sql_mgr = get_accounts_sql_manager()
        acc = sql_mgr.add_account(request.cookie, request.name)
        safe_acc = sql_mgr.get_account_by_id(acc.get("id"), mask_cookie=True)
        # æ¸…é™¤è´¦å·ç¾¤ç»„æ£€æµ‹ç¼“å­˜ï¼Œä½¿æ–°è´¦å·çš„ç¾¤ç»„ç«‹å³å¯è§
        clear_account_detect_cache()
        return {"account": safe_acc}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create account: {str(e)}")

@app.delete("/api/accounts/{account_id}")
async def remove_account(account_id: str):
    """åˆ é™¤è´¦å·"""
    try:
        sql_mgr = get_accounts_sql_manager()
        ok = sql_mgr.delete_account(account_id)
        if not ok:
            raise HTTPException(status_code=404, detail="Account does not exist")
        # æ¸…é™¤è´¦å·ç¾¤ç»„æ£€æµ‹ç¼“å­˜
        clear_account_detect_cache()
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete account: {str(e)}")

@app.post("/api/groups/{group_id}/assign-account")
async def assign_account_to_group(group_id: str, request: AssignGroupAccountRequest):
    """åˆ†é…ç¾¤ç»„åˆ°æŒ‡å®šè´¦å·"""
    try:
        sql_mgr = get_accounts_sql_manager()
        ok, msg = sql_mgr.assign_group_account(group_id, request.account_id)
        if not ok:
            raise HTTPException(status_code=400, detail=msg)
        return {"success": True, "message": msg}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to assign account: {str(e)}")

@app.get("/api/groups/{group_id}/account")
async def get_group_account(group_id: str):
    try:
        summary = get_account_summary_for_group_auto(group_id)
        return {"account": summary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è´¦å·å¤±è´¥: {str(e)}")

# è´¦å·â€œè‡ªæˆ‘ä¿¡æ¯â€æŒä¹…åŒ– (/v3/users/self)
@app.get("/api/accounts/{account_id}/self")
async def get_account_self(account_id: str):
    """è·å–å¹¶è¿”å›æŒ‡å®šè´¦å·çš„å·²æŒä¹…åŒ–è‡ªæˆ‘ä¿¡æ¯ï¼›è‹¥æ— åˆ™å°è¯•æŠ“å–å¹¶ä¿å­˜"""
    try:
        db = get_account_info_db()
        info = db.get_self_info(account_id)
        if info:
            return {"self": info}

        # è‹¥æ•°æ®åº“æ— è®°å½•åˆ™æŠ“å–
        sql_mgr = get_accounts_sql_manager()
        acc = sql_mgr.get_account_by_id(account_id, mask_cookie=False)
        if not acc:
            raise HTTPException(status_code=404, detail="Account does not exist")

        cookie = acc.get("cookie", "")
        if not cookie:
            raise HTTPException(status_code=400, detail="Account has no configured Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="API returned failure")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"Network request failed: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve account info: {str(e)}")

@app.post("/api/accounts/{account_id}/self/refresh")
async def refresh_account_self(account_id: str):
    """å¼ºåˆ¶æŠ“å– /v3/users/self å¹¶æ›´æ–°æŒä¹…åŒ–"""
    try:
        sql_mgr = get_accounts_sql_manager()
        acc = sql_mgr.get_account_by_id(account_id, mask_cookie=False)
        if not acc:
            raise HTTPException(status_code=404, detail="Account does not exist")

        cookie = acc.get("cookie", "")
        if not cookie:
            raise HTTPException(status_code=400, detail="Account has no configured Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="API returned failure")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db = get_account_info_db()
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"Network request failed: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to refresh account info: {str(e)}")

@app.get("/api/groups/{group_id}/self")
async def get_group_account_self(group_id: str):
    """è·å–ç¾¤ç»„å½“å‰ä½¿ç”¨è´¦å·çš„è‡ªæˆ‘ä¿¡æ¯ï¼ˆè‹¥æ— åˆ™å°è¯•æŠ“å–å¹¶ä¿å­˜ï¼‰"""
    try:
        summary = get_account_summary_for_group_auto(group_id)
        cookie = get_cookie_for_group(group_id)
        account_id = (summary or {}).get('id', 'default')

        if not cookie:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·æˆ–é»˜è®¤Cookie")

        db = get_account_info_db()
        info = db.get_self_info(account_id)
        if info:
            return {"self": info}

        # æŠ“å–å¹¶å†™å…¥
        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.post("/api/groups/{group_id}/self/refresh")
async def refresh_group_account_self(group_id: str):
    """å¼ºåˆ¶æŠ“å–ç¾¤ç»„å½“å‰ä½¿ç”¨è´¦å·çš„è‡ªæˆ‘ä¿¡æ¯å¹¶æŒä¹…åŒ–"""
    try:
        summary = get_account_summary_for_group_auto(group_id)
        cookie = get_cookie_for_group(group_id)
        account_id = (summary or {}).get('id', 'default')

        if not cookie:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·æˆ–é»˜è®¤Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db = get_account_info_db()
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°ç¾¤ç»„è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

# migrated to api/routers/groups.py: @app.get("/api/database/stats")
async def get_database_stats():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        configured = is_configured()
        if not configured:
            return {
                "configured": False,
                "topic_database": {
                    "stats": {},
                    "timestamp_info": {
                        "total_topics": 0,
                        "oldest_timestamp": "",
                        "newest_timestamp": "",
                        "has_data": False,
                    },
                },
                "file_database": {
                    "stats": {},
                },
            }

        # èšåˆæ‰€æœ‰æœ¬åœ°ç¾¤ç»„çš„æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        path_manager = get_db_path_manager()
        groups_info = path_manager.list_all_groups()

        if not groups_info:
            # å·²é…ç½®ä½†å°šæœªäº§ç”Ÿæœ¬åœ°æ•°æ®
            return {
                "configured": True,
                "topic_database": {
                    "stats": {},
                    "timestamp_info": {
                        "total_topics": 0,
                        "oldest_timestamp": "",
                        "newest_timestamp": "",
                        "has_data": False,
                    },
                },
                "file_database": {
                    "stats": {},
                },
            }

        aggregated_topic_stats: Dict[str, int] = {}
        aggregated_file_stats: Dict[str, int] = {}

        oldest_ts: Optional[str] = None
        newest_ts: Optional[str] = None
        total_topics = 0
        has_data = False

        for gi in groups_info:
            group_id = gi.get("group_id")
            topics_db_path = gi.get("topics_db")
            if not topics_db_path:
                continue

            # è¯é¢˜æ•°æ®åº“ç»Ÿè®¡
            db = ZSXQDatabase(topics_db_path)
            try:
                topic_stats = db.get_database_stats()
                ts_info = db.get_timestamp_range_info()
            finally:
                db.close()

            for table, count in (topic_stats or {}).items():
                aggregated_topic_stats[table] = aggregated_topic_stats.get(table, 0) + int(count or 0)

            if ts_info.get("has_data"):
                has_data = True
                ot = ts_info.get("oldest_timestamp")
                nt = ts_info.get("newest_timestamp")
                if ot:
                    if oldest_ts is None or ot < oldest_ts:
                        oldest_ts = ot
                if nt:
                    if newest_ts is None or nt > newest_ts:
                        newest_ts = nt
                total_topics += int(ts_info.get("total_topics") or 0)

            # æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡ï¼ˆå¦‚å­˜åœ¨ï¼‰
            db_paths = path_manager.list_group_databases(str(group_id))
            files_db_path = db_paths.get("files")
            if files_db_path:
                fdb = ZSXQFileDatabase(files_db_path)
                try:
                    file_stats = fdb.get_database_stats()
                finally:
                    fdb.close()

                for table, count in (file_stats or {}).items():
                    aggregated_file_stats[table] = aggregated_file_stats.get(table, 0) + int(count or 0)

        timestamp_info = {
            "total_topics": total_topics,
            "oldest_timestamp": oldest_ts or "",
            "newest_timestamp": newest_ts or "",
            "has_data": has_data,
        }

        return {
            "configured": True,
            "topic_database": {
                "stats": aggregated_topic_stats,
                "timestamp_info": timestamp_info,
            },
            "file_database": {
                "stats": aggregated_file_stats,
            },
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")

async def get_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    return list(current_tasks.values())

async def get_tasks_summary():
    """æŒ‰ä¸šåŠ¡ç±»åˆ«è¿”å›è¿è¡Œä¸­ + æœ€è¿‘ä¸€æ¬¡ä»»åŠ¡å¿«ç…§ï¼Œç”¨äº Dashboard æ¢å¤çŠ¶æ€ã€‚"""
    return _build_task_summary()

async def get_task(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡çŠ¶æ€"""
    if task_id not in current_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return current_tasks[task_id]

async def stop_task_api(task_id: str):
    """åœæ­¢ä»»åŠ¡"""
    if stop_task(task_id):
        return {"message": "ä»»åŠ¡åœæ­¢è¯·æ±‚å·²å‘é€", "task_id": task_id}
    else:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•åœæ­¢")

# æŠ“å–åè‡ªåŠ¨æå–è‚¡ç¥¨æåŠå¹¶åˆ·æ–°æ”¶ç›Šï¼ˆé¿å…â€œå·²æŠ“åˆ°æ–°å¸–ä½†æœªåšè‚¡ç¥¨åˆ†æâ€ï¼‰
# migrated to api/routers/crawl.py: legacy crawl implementation removed

# migrated to api/routers/files.py: files domain legacy implementation removed

# migrated to api/routers/topics.py: topics read/write endpoints removed

# migrated to api/routers/files.py: @app.get("/api/files/{group_id}")
async def get_files(group_id: str, page: int = 1, per_page: int = 20, status: Optional[str] = None):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL
        if status:
            query = """
                SELECT file_id, name, size, download_count, create_time, download_status
                FROM files
                WHERE download_status = ?
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (status, per_page, offset)
        else:
            query = """
                SELECT file_id, name, size, download_count, create_time, download_status
                FROM files
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (per_page, offset)

        downloader.file_db.cursor.execute(query, params)
        files = downloader.file_db.cursor.fetchall()

        # è·å–æ€»æ•°
        if status:
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files WHERE download_status = ?", (status,))
        else:
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
        total = downloader.file_db.cursor.fetchone()[0]

        return {
            "files": [
                {
                    "file_id": file[0],
                    "name": file[1],
                    "size": file[2],
                    "download_count": file[3],
                    "create_time": file[4],
                    "download_status": file[5] if len(file) > 5 else "unknown"
                }
                for file in files
            ],
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

# ç¾¤ç»„ç›¸å…³APIç«¯ç‚¹
@app.post("/api/local-groups/refresh")
async def refresh_local_groups():
    """
    æ‰‹åŠ¨åˆ·æ–°æœ¬åœ°ç¾¤ï¼ˆoutputï¼‰æ‰«æç¼“å­˜ï¼›ä¸æŠ›é”™ï¼Œå¼‚å¸¸æ—¶è¿”å›æ—§ç¼“å­˜ã€‚
    """
    try:
        ids = await asyncio.to_thread(scan_local_groups)
        try:
            from modules.analyzers.global_analyzer import get_global_analyzer
            get_global_analyzer().invalidate_cache()
        except Exception:
            pass
        return {"success": True, "count": len(ids), "groups": sorted(list(ids))}
    except Exception as e:
        cached = get_cached_local_group_ids(force_refresh=False) or set()
        # ä¸æŠ¥é”™ï¼Œè¿”å›é™çº§ç»“æœ
        return {"success": False, "count": len(cached), "groups": sorted(list(cached)), "error": str(e)}

def _persist_group_meta_local(group_id: int, info: Dict[str, Any]):
    """
    å°†ç¾¤ç»„çš„å°é¢ã€åç§°ã€ç¾¤ä¸»ä¸æ—¶é—´ç­‰å…ƒä¿¡æ¯æŒä¹…åŒ–åˆ°æœ¬åœ°ç›®å½•ã€‚
    è¿™æ ·å³ä½¿åç»­è´¦å· Cookie å¤±æ•ˆï¼Œä»…ä¿ç•™æœ¬åœ°æ•°æ®æ—¶ï¼Œä¹Ÿèƒ½å±•ç¤ºå®Œæ•´ä¿¡æ¯ã€‚
    """
    try:
        from pathlib import Path

        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_data_dir(str(group_id))
        meta_path = Path(group_dir) / "group_meta.json"

        meta = {
            "group_id": group_id,
            "name": info.get("name") or f"æœ¬åœ°ç¾¤ï¼ˆ{group_id}ï¼‰",
            "type": info.get("type", ""),
            "background_url": info.get("background_url", ""),
            "owner": info.get("owner", {}) or {},
            "statistics": info.get("statistics", {}) or {},
            "create_time": info.get("create_time"),
            "subscription_time": info.get("subscription_time"),
            "expiry_time": info.get("expiry_time"),
            "join_time": info.get("join_time"),
            "last_active_time": info.get("last_active_time"),
            "description": info.get("description", ""),
            "is_trial": info.get("is_trial", False),
            "trial_end_time": info.get("trial_end_time"),
            "membership_end_time": info.get("membership_end_time"),
        }

        with meta_path.open("w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ å†™å…¥æœ¬åœ°ç¾¤ç»„å…ƒæ•°æ®å¤±è´¥: {e}")


@app.get("/api/groups")
async def get_groups():
    """è·å–ç¾¤ç»„åˆ—è¡¨ï¼šè´¦å·ç¾¤ âˆª æœ¬åœ°ç›®å½•ç¾¤ï¼ˆå»é‡åˆå¹¶ï¼‰"""
    try:
        # è‡ªåŠ¨æ„å»ºç¾¤ç»„â†’è´¦å·æ˜ å°„ï¼ˆå¤šè´¦å·æ”¯æŒï¼‰
        group_account_map = build_account_group_detection()
        local_ids = get_cached_local_group_ids(force_refresh=False)

        # è·å–â€œå½“å‰è´¦å·â€çš„ç¾¤åˆ—è¡¨ï¼ˆä¼˜å…ˆè´¦å·é»˜è®¤è´¦å·ï¼Œå…¶æ¬¡ config/app.tomlï¼›è‹¥æœªé…ç½®åˆ™è§†ä¸ºç©ºé›†åˆï¼‰
        groups_data: List[dict] = []
        try:
            primary_cookie = get_primary_cookie()
            if primary_cookie:
                groups_data = fetch_groups_from_api(primary_cookie)
        except Exception as e:
            # ä¸é˜»æ–­ï¼Œè®°å½•å‘Šè­¦
            print(f"âš ï¸ è·å–è´¦å·ç¾¤å¤±è´¥ï¼Œé™çº§ä¸ºæœ¬åœ°é›†åˆ: {e}")
            groups_data = []

        # ç»„è£…è´¦å·ä¾§ç¾¤ä¸ºå­—å…¸ï¼ˆid -> infoï¼‰
        by_id: Dict[int, dict] = {}

        for group in groups_data or []:
            # æå–ç”¨æˆ·ç‰¹å®šä¿¡æ¯
            user_specific = group.get('user_specific', {}) or {}
            validity = user_specific.get('validity', {}) or {}
            trial = user_specific.get('trial', {}) or {}

            # è¿‡æœŸä¿¡æ¯ä¸çŠ¶æ€
            actual_expiry_time = trial.get('end_time') or validity.get('end_time')
            is_trial = bool(trial.get('end_time'))

            status = None
            if actual_expiry_time:
                from datetime import datetime, timezone
                try:
                    end_time = datetime.fromisoformat(actual_expiry_time.replace('Z', '+00:00'))
                    now = datetime.now(timezone.utc)
                    days_until_expiry = (end_time - now).days
                    if days_until_expiry < 0:
                        status = 'expired'
                    elif days_until_expiry <= 7:
                        status = 'expiring_soon'
                    else:
                        status = 'active'
                except Exception:
                    pass

            gid = group.get('group_id')
            try:
                gid = int(gid)
            except Exception:
                continue

            info = {
                "group_id": gid,
                "name": group.get('name', ''),
                "type": group.get('type', ''),
                "background_url": group.get('background_url', ''),
                "owner": group.get('owner', {}) or {},
                "statistics": group.get('statistics', {}) or {},
                "status": status,
                "create_time": group.get('create_time'),
                "subscription_time": validity.get('begin_time'),
                "expiry_time": actual_expiry_time,
                "join_time": user_specific.get('join_time'),
                "last_active_time": user_specific.get('last_active_time'),
                "description": group.get('description', ''),
                "is_trial": is_trial,
                "trial_end_time": trial.get('end_time'),
                "membership_end_time": validity.get('end_time'),
                "account": group_account_map.get(str(gid)),
                "source": "account"
            }
            by_id[gid] = info

        # åˆå¹¶æœ¬åœ°ç›®å½•ç¾¤
        for gid in local_ids or []:
            try:
                gid_int = int(gid)
            except Exception:
                continue
            if gid_int in by_id:
                # æ ‡æ³¨æ¥æºä¸º account|localï¼Œå¹¶æŒä¹…åŒ–ä¸€ä»½å…ƒä¿¡æ¯åˆ°æœ¬åœ°
                src = by_id[gid_int].get("source", "account")
                if "local" not in src:
                    by_id[gid_int]["source"] = "account|local"
                _persist_group_meta_local(gid_int, by_id[gid_int])
            else:
                # ä»…å­˜åœ¨äºæœ¬åœ°ï¼šä¼˜å…ˆä» group_meta.json è¯»å–å…ƒä¿¡æ¯ï¼Œå…¶æ¬¡ä»æœ¬åœ°æ•°æ®åº“è¡¥å…¨
                local_name = f"æœ¬åœ°ç¾¤ï¼ˆ{gid_int}ï¼‰"
                local_type = "local"
                local_bg = ""
                owner: Dict[str, Any] = {}
                join_time = None
                expiry_time = None
                last_active_time = None
                description = ""
                statistics: Dict[str, Any] = {}

                # 1. ä¼˜å…ˆè¯»å–æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœä¹‹å‰æœ‰è´¦å·+æœ¬åœ°æ—¶å·²ç»è½ç›˜ï¼‰
                try:
                    from pathlib import Path

                    path_manager = get_db_path_manager()
                    group_dir = path_manager.get_group_data_dir(str(gid_int))
                    meta_path = Path(group_dir) / "group_meta.json"
                    if meta_path.exists():
                        with meta_path.open("r", encoding="utf-8") as f:
                            meta = json.load(f)
                        local_name = meta.get("name", local_name)
                        local_type = meta.get("type", local_type)
                        local_bg = meta.get("background_url", local_bg)
                        owner = meta.get("owner", {}) or owner
                        statistics = meta.get("statistics", {}) or statistics
                        join_time = meta.get("join_time", join_time)
                        expiry_time = meta.get("expiry_time", expiry_time)
                        last_active_time = meta.get("last_active_time", last_active_time)
                        description = meta.get("description", description)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æœ¬åœ°ç¾¤ç»„ {gid_int} å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")

                # 2. è‹¥å…ƒæ•°æ®æ–‡ä»¶ä¸­ä»ç¼ºå°‘ä¿¡æ¯ï¼Œå†ä»æœ¬åœ°æ•°æ®åº“è¡¥å……
                try:
                    path_manager = get_db_path_manager()
                    db_paths = path_manager.list_group_databases(str(gid_int))
                    topics_db = db_paths.get("topics")
                    if topics_db and os.path.exists(topics_db):
                        db = ZSXQDatabase(topics_db)
                        try:
                            cur = db.cursor
                            # ç¾¤ç»„åŸºç¡€ä¿¡æ¯
                            if not local_bg or local_name.startswith("æœ¬åœ°ç¾¤ï¼ˆ"):
                                cur.execute(
                                    "SELECT name, type, background_url FROM groups WHERE group_id = ? LIMIT 1",
                                    (gid_int,),
                                )
                                row = cur.fetchone()
                                if row:
                                    if row[0]:
                                        local_name = row[0]
                                    if row[1]:
                                        local_type = row[1]
                                    if row[2]:
                                        local_bg = row[2]

                            # æœ¬åœ°æ•°æ®æ—¶é—´èŒƒå›´ï¼ˆä»¥è¯é¢˜æ—¶é—´æ›¿ä»£â€œåŠ å…¥/è¿‡æœŸæ—¶é—´â€çš„è¿‘ä¼¼ï¼‰
                            if not join_time or not expiry_time:
                                cur.execute(
                                    """
                                    SELECT MIN(create_time), MAX(create_time)
                                    FROM topics
                                    WHERE group_id = ? AND create_time IS NOT NULL AND create_time != ''
                                    """,
                                    (gid_int,),
                                )
                                trow = cur.fetchone()
                                if trow:
                                    if not join_time:
                                        join_time = trow[0]
                                    if not expiry_time:
                                        expiry_time = trow[1]
                                    if not last_active_time:
                                        last_active_time = trow[1]

                            # ç®€å•ç»Ÿè®¡ï¼šè¯é¢˜æ•°é‡
                            if not statistics:
                                cur.execute(
                                    "SELECT COUNT(*) FROM topics WHERE group_id = ?",
                                    (gid_int,),
                                )
                                topics_count = cur.fetchone()[0] or 0
                                statistics = {
                                    "topics": {
                                        "topics_count": topics_count,
                                        "answers_count": 0,
                                        "digests_count": 0,
                                    }
                                }
                        finally:
                            db.close()
                except Exception as e:
                    # å‡ºé”™æ—¶é™çº§ä¸ºå ä½ä¿¡æ¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªæ¥å£
                    print(f"âš ï¸ è¯»å–æœ¬åœ°ç¾¤ç»„ {gid_int} å…ƒæ•°æ®å¤±è´¥: {e}")

                by_id[gid_int] = {
                    "group_id": gid_int,
                    "name": local_name,
                    "type": local_type,
                    "background_url": local_bg,
                    "owner": owner,
                    "statistics": statistics,
                    "status": None,
                    "create_time": join_time,
                    "subscription_time": None,
                    "expiry_time": expiry_time,
                    "join_time": join_time,
                    "last_active_time": last_active_time,
                    "description": description,
                    "is_trial": False,
                    "trial_end_time": None,
                    "membership_end_time": None,
                    "account": None,
                    "source": "local",
                }

        # æ’åºï¼šæŒ‰ç¾¤IDå‡åºï¼›å¦‚éœ€äºŒçº§æ’åºå†æŒ‰æ¥æºï¼ˆè´¦å·ä¼˜å…ˆï¼‰
        merged = [by_id[k] for k in sorted(by_id.keys())]

        return {
            "groups": merged,
            "total": len(merged)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„åˆ—è¡¨å¤±è´¥: {str(e)}")

# migrated to api/routers/topics.py: topic detail/refresh/comments/delete/fetch-single/tags endpoints removed

# migrated to api/routers/media.py: proxy/cache/local media endpoints removed


# migrated to api/routers/settings.py: @app.get("/api/settings/crawl")
# migrated to api/routers/settings.py: @app.post("/api/settings/crawl")
# legacy settings(crawl) implementation removed

# migrated to api/routers/columns.py: @app.get("/api/groups/{group_id}/columns/summary")
# legacy implementation removed

@app.get("/api/groups/{group_id}/info")
async def get_group_info(group_id: str):
    """è·å–ç¾¤ç»„ä¿¡æ¯ï¼ˆå¸¦æœ¬åœ°å›é€€ï¼Œé¿å…401/500å¯¼è‡´å‰ç«¯æŠ¥é”™ï¼‰"""
    try:
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        # æœ¬åœ°å›é€€æ•°æ®æ„é€ ï¼ˆä¸è®¿é—®å®˜æ–¹APIï¼‰
        def build_fallback(source: str = "fallback", note: str = None) -> dict:
            files_count = 0
            try:
                crawler = get_crawler_for_group(group_id)
                downloader = crawler.get_file_downloader()
                try:
                    downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
                    row = downloader.file_db.cursor.fetchone()
                    files_count = (row[0] or 0) if row else 0
                except Exception:
                    files_count = 0
            except Exception:
                files_count = 0

            try:
                gid = int(group_id)
            except Exception:
                gid = group_id

            result = {
                "group_id": gid,
                "name": f"ç¾¤ç»„ {group_id}",
                "description": "",
                "statistics": {"files": {"count": files_count}},
                "background_url": None,
                "account": get_account_summary_for_group_auto(group_id),
                "source": source,
            }
            if note:
                result["note"] = note
            return result

        # è‹¥æ²¡æœ‰å¯ç”¨ Cookieï¼Œç›´æ¥è¿”å›æœ¬åœ°å›é€€ï¼Œé¿å…æŠ› 400/500
        if not cookie:
            return build_fallback(note="no_cookie")

        # è°ƒç”¨å®˜æ–¹æ¥å£
        url = f"https://api.zsxq.com/v2/groups/{group_id}"
        headers = {
            'Cookie': cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code == 200:
            data = response.json()
            if data.get('succeeded'):
                group_data = data.get('resp_data', {}).get('group', {})
                return {
                    "group_id": group_data.get('group_id'),
                    "name": group_data.get('name'),
                    "description": group_data.get('description'),
                    "statistics": group_data.get('statistics', {}),
                    "background_url": group_data.get('background_url'),
                    "account": get_account_summary_for_group_auto(group_id),
                    "source": "remote"
                }
            # å®˜æ–¹è¿”å›é succeededï¼Œä¹Ÿèµ°å›é€€
            return build_fallback(note="remote_response_failed")
        else:
            # æˆæƒå¤±è´¥/æƒé™ä¸è¶³ â†’ ä½¿ç”¨æœ¬åœ°å›é€€ï¼ˆ200è¿”å›ï¼Œå‡å°‘å‰ç«¯å‘Šè­¦ï¼‰
            if response.status_code in (401, 403):
                return build_fallback(note=f"remote_api_{response.status_code}")
            # å…¶ä»–çŠ¶æ€ç ä¹Ÿå›é€€
            return build_fallback(note=f"remote_api_{response.status_code}")

    except Exception:
        # ä»»ä½•å¼‚å¸¸éƒ½å›é€€ä¸ºæœ¬åœ°ä¿¡æ¯ï¼Œé¿å… 500
        return build_fallback(note="exception_fallback")

# migrated to api/routers/topics.py: group topics/tags/stats/database-info endpoints removed

async def get_task_logs(task_id: str):
    """è·å–ä»»åŠ¡æ—¥å¿—"""
    if task_id not in task_logs:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return {
        "task_id": task_id,
        "logs": task_logs[task_id]
    }

async def stream_task_logs(task_id: str):
    """SSEæµå¼ä¼ è¾“ä»»åŠ¡æ—¥å¿—"""
    async def event_stream():
        # åˆå§‹åŒ–è¿æ¥
        if task_id not in sse_connections:
            sse_connections[task_id] = []

        # å‘é€å†å²æ—¥å¿—
        if task_id in task_logs:
            for log in task_logs[task_id]:
                yield f"data: {json.dumps({'type': 'log', 'message': log})}\n\n"

        # å‘é€ä»»åŠ¡çŠ¶æ€
        last_status = None
        last_message = None
        if task_id in current_tasks:
            task = current_tasks[task_id]
            last_status = task.get('status')
            last_message = task.get('message')
            yield f"data: {json.dumps({'type': 'status', 'status': task['status'], 'message': task['message']})}\n\n"

        # è®°å½•å½“å‰æ—¥å¿—æ•°é‡ï¼Œç”¨äºæ£€æµ‹æ–°æ—¥å¿—
        last_log_count = len(task_logs.get(task_id, []))

        # ä¿æŒè¿æ¥æ´»è·ƒ
        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ—¥å¿—
                current_log_count = len(task_logs.get(task_id, []))
                if current_log_count > last_log_count:
                    # å‘é€æ–°æ—¥å¿—
                    new_logs = task_logs[task_id][last_log_count:]
                    for log in new_logs:
                        yield f"data: {json.dumps({'type': 'log', 'message': log})}\n\n"
                    last_log_count = current_log_count

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å˜åŒ–
                if task_id in current_tasks:
                    task = current_tasks[task_id]
                    status = task.get('status')
                    message = task.get('message')

                    # ä»…åœ¨çŠ¶æ€æˆ–æ¶ˆæ¯å‘ç”Ÿå˜åŒ–æ—¶æ¨é€ï¼Œé¿å…å‰ç«¯æŒç»­æŠ–åŠ¨
                    if status != last_status or message != last_message:
                        yield f"data: {json.dumps({'type': 'status', 'status': status, 'message': message})}\n\n"
                        last_status = status
                        last_message = message

                    if status in ['completed', 'failed', 'cancelled', 'stopped', 'idle']:
                        break

                # å‘é€å¿ƒè·³
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                await asyncio.sleep(0.5)  # æ›´é¢‘ç¹çš„æ£€æŸ¥

        except asyncio.CancelledError:
            # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
            pass

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# migrated to api/routers/media.py: /api/proxy/image endpoint removed

# è®¾ç½®ç›¸å…³APIè·¯ç”±
@app.get("/api/settings/crawler")
async def get_crawler_settings():
    """è·å–çˆ¬è™«è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            return {
                "min_delay": 2.0,
                "max_delay": 5.0,
                "long_delay_interval": 15,
                "timestamp_offset_ms": 1,
                "debug_mode": False
            }

        return {
            "min_delay": crawler.min_delay,
            "max_delay": crawler.max_delay,
            "long_delay_interval": crawler.long_delay_interval,
            "timestamp_offset_ms": crawler.timestamp_offset_ms,
            "debug_mode": crawler.debug_mode
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çˆ¬è™«è®¾ç½®å¤±è´¥: {str(e)}")

@app.post("/api/settings/crawler")
async def update_crawler_settings(request: CrawlerSettingsRequest):
    """æ›´æ–°çˆ¬è™«è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            raise HTTPException(status_code=404, detail="çˆ¬è™«æœªåˆå§‹åŒ–")

        # éªŒè¯è®¾ç½®
        if request.min_delay >= request.max_delay:
            raise HTTPException(status_code=400, detail="æœ€å°å»¶è¿Ÿå¿…é¡»å°äºæœ€å¤§å»¶è¿Ÿ")

        # æ›´æ–°è®¾ç½®
        crawler.min_delay = request.min_delay
        crawler.max_delay = request.max_delay
        crawler.long_delay_interval = request.long_delay_interval
        crawler.timestamp_offset_ms = request.timestamp_offset_ms
        crawler.debug_mode = request.debug_mode

        return {
            "message": "çˆ¬è™«è®¾ç½®å·²æ›´æ–°",
            "settings": {
                "min_delay": crawler.min_delay,
                "max_delay": crawler.max_delay,
                "long_delay_interval": crawler.long_delay_interval,
                "timestamp_offset_ms": crawler.timestamp_offset_ms,
                "debug_mode": crawler.debug_mode
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°çˆ¬è™«è®¾ç½®å¤±è´¥: {str(e)}")

@app.get("/api/settings/downloader")
async def get_downloader_settings():
    """è·å–æ–‡ä»¶ä¸‹è½½å™¨è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            return {
                "download_interval_min": 30,
                "download_interval_max": 60,
                "long_delay_interval": 10,
                "long_delay_min": 300,
                "long_delay_max": 600
            }

        downloader = crawler.get_file_downloader()
        return {
            "download_interval_min": downloader.download_interval_min,
            "download_interval_max": downloader.download_interval_max,
            "long_delay_interval": downloader.long_delay_interval,
            "long_delay_min": downloader.long_delay_min,
            "long_delay_max": downloader.long_delay_max
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¸‹è½½å™¨è®¾ç½®å¤±è´¥: {str(e)}")

@app.post("/api/settings/downloader")
async def update_downloader_settings(request: DownloaderSettingsRequest):
    """æ›´æ–°æ–‡ä»¶ä¸‹è½½å™¨è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            raise HTTPException(status_code=404, detail="çˆ¬è™«æœªåˆå§‹åŒ–")

        # éªŒè¯è®¾ç½®
        if request.download_interval_min >= request.download_interval_max:
            raise HTTPException(status_code=400, detail="æœ€å°ä¸‹è½½é—´éš”å¿…é¡»å°äºæœ€å¤§ä¸‹è½½é—´éš”")

        if request.long_delay_min >= request.long_delay_max:
            raise HTTPException(status_code=400, detail="æœ€å°é•¿ä¼‘çœ æ—¶é—´å¿…é¡»å°äºæœ€å¤§é•¿ä¼‘çœ æ—¶é—´")

        downloader = crawler.get_file_downloader()

        # æ›´æ–°è®¾ç½®
        downloader.download_interval_min = request.download_interval_min
        downloader.download_interval_max = request.download_interval_max
        downloader.long_delay_interval = request.long_delay_interval
        downloader.long_delay_min = request.long_delay_min
        downloader.long_delay_max = request.long_delay_max

        return {
            "message": "ä¸‹è½½å™¨è®¾ç½®å·²æ›´æ–°",
            "settings": {
                "download_interval_min": downloader.download_interval_min,
                "download_interval_max": downloader.download_interval_max,
                "long_delay_interval": downloader.long_delay_interval,
                "long_delay_min": downloader.long_delay_min,
                "long_delay_max": downloader.long_delay_max
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸‹è½½å™¨è®¾ç½®å¤±è´¥: {str(e)}")

# account auto-resolution helpers migrated to api/services/account_resolution_service.py

# =========================
# æ–°å¢ï¼šæŒ‰æ—¶é—´åŒºé—´çˆ¬å–
# =========================


# migrated to api/routers/crawl.py: @app.post("/api/crawl/range/{group_id}")
# legacy crawl(range) implementation removed

@app.delete("/api/groups/{group_id}")
async def delete_group_local(group_id: str):
    """
    åˆ é™¤æŒ‡å®šç¤¾ç¾¤çš„æœ¬åœ°æ•°æ®ï¼ˆæ•°æ®åº“ã€ä¸‹è½½æ–‡ä»¶ã€å›¾ç‰‡ç¼“å­˜ï¼‰ï¼Œä¸å½±å“è´¦å·å¯¹è¯¥ç¤¾ç¾¤çš„è®¿é—®æƒé™
    """
    try:
        details = {
            "topics_db_removed": False,
            "files_db_removed": False,
            "downloads_dir_removed": False,
            "images_cache_removed": False,
            "group_dir_removed": False,
        }

        # å°è¯•å…³é—­æ•°æ®åº“è¿æ¥ï¼Œé¿å…æ–‡ä»¶å ç”¨
        try:
            crawler = get_crawler_for_group(group_id)
            try:
                if hasattr(crawler, "file_downloader") and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, "file_db") and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                        print(f"âœ… å·²å…³é—­æ–‡ä»¶æ•°æ®åº“è¿æ¥ï¼ˆç¾¤ {group_id}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ å…³é—­æ–‡ä»¶æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
            try:
                if hasattr(crawler, "db") and crawler.db:
                    crawler.db.close()
                    print(f"âœ… å·²å…³é—­è¯é¢˜æ•°æ®åº“è¿æ¥ï¼ˆç¾¤ {group_id}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ å…³é—­è¯é¢˜æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
        except Exception as e:
            print(f"âš ï¸ è·å–çˆ¬è™«å®ä¾‹ä»¥å…³é—­è¿æ¥å¤±è´¥: {e}")

        # åƒåœ¾å›æ”¶ + ç­‰å¾…ç‰‡åˆ»ï¼Œç¡®ä¿å¥æŸ„é‡Šæ”¾
        import gc, time, shutil
        gc.collect()
        time.sleep(0.3)

        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_dir(group_id)
        topics_db = path_manager.get_topics_db_path(group_id)
        files_db = path_manager.get_files_db_path(group_id)

        # åˆ é™¤è¯é¢˜æ•°æ®åº“
        try:
            if os.path.exists(topics_db):
                os.remove(topics_db)
                details["topics_db_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤è¯é¢˜æ•°æ®åº“: {topics_db}")
        except PermissionError as pe:
            raise HTTPException(status_code=500, detail=f"è¯é¢˜æ•°æ®åº“è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {e}")

        # åˆ é™¤æ–‡ä»¶æ•°æ®åº“
        try:
            if os.path.exists(files_db):
                os.remove(files_db)
                details["files_db_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ–‡ä»¶æ•°æ®åº“: {files_db}")
        except PermissionError as pe:
            raise HTTPException(status_code=500, detail=f"æ–‡ä»¶æ•°æ®åº“è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {e}")

        # åˆ é™¤ä¸‹è½½ç›®å½•
        downloads_dir = os.path.join(group_dir, "downloads")
        if os.path.exists(downloads_dir):
            try:
                shutil.rmtree(downloads_dir, ignore_errors=False)
                details["downloads_dir_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸‹è½½ç›®å½•: {downloads_dir}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ä¸‹è½½ç›®å½•å¤±è´¥: {e}")

        # æ¸…ç©ºå¹¶åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•ï¼ŒåŒæ—¶é‡Šæ”¾ç¼“å­˜ç®¡ç†å™¨
        try:
            from app.runtime.image_cache_manager import get_image_cache_manager, clear_group_cache_manager
            cache_manager = get_image_cache_manager(group_id)
            ok, msg = cache_manager.clear_cache()
            if ok:
                details["images_cache_removed"] = True
                print(f"ğŸ—‘ï¸ å›¾ç‰‡ç¼“å­˜æ¸…ç©º: {msg}")
            images_dir = os.path.join(group_dir, "images")
            if os.path.exists(images_dir):
                try:
                    shutil.rmtree(images_dir, ignore_errors=True)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•: {images_dir}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            clear_group_cache_manager(group_id)
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å›¾ç‰‡ç¼“å­˜å¤±è´¥: {e}")

        # è‹¥ç¾¤ç»„ç›®å½•å·²ç©ºï¼Œåˆ™åˆ é™¤è¯¥ç›®å½•
        try:
            if os.path.exists(group_dir) and len(os.listdir(group_dir)) == 0:
                os.rmdir(group_dir)
                details["group_dir_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºç¾¤ç»„ç›®å½•: {group_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤ç¾¤ç»„ç›®å½•å¤±è´¥: {e}")

        # æ›´æ–°æœ¬åœ°ç¾¤ç¼“å­˜ï¼ˆä»ç¼“å­˜é›†åˆç§»é™¤ï¼‰
        try:
            gid_int = int(group_id)
            if gid_int in _local_groups_cache.get("ids", set()):
                _local_groups_cache["ids"].discard(gid_int)
                _local_groups_cache["scanned_at"] = time.time()
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°æœ¬åœ°ç¾¤ç¼“å­˜å¤±è´¥: {e}")

        any_removed = any(details.values())
        return {
            "success": True,
            "message": f"ç¾¤ç»„ {group_id} æœ¬åœ°æ•°æ®" + ("å·²åˆ é™¤" if any_removed else "ä¸å­˜åœ¨"),
            "details": details,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ç¾¤ç»„æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}")


# =========================
# ä¸“æ ç›¸å…³ API
# =========================

# migrated to api/routers/columns.py: columns domain legacy implementation removed

# ========== è‚¡ç¥¨èˆ†æƒ…åˆ†æ API ==========

from modules.analyzers.stock_analyzer import StockAnalyzer


# migrated to api/routers/stocks.py: @app.post("/api/groups/{group_id}/stock/scan")
def scan_group_stocks(group_id: str, background_tasks: BackgroundTasks, force: bool = False):
    """æ‰«æç¾¤ç»„å¸–å­ï¼Œæå–è‚¡ç¥¨æåŠå¹¶è®¡ç®—åç»­è¡¨ç°ï¼ˆåå°ä»»åŠ¡ï¼‰"""
    task_id = create_task(f"stock_scan_{group_id}", f"è‚¡ç¥¨æåŠæ‰«æ: {group_id}")

    def _scan_task():
        try:
            update_task(task_id, "running", "æ­£åœ¨æ‰«æ...")
            add_task_log(task_id, "ğŸš€ å¼€å§‹è‚¡ç¥¨æåŠæ‰«æ...")
            add_task_log(task_id, "ğŸ§­ åˆ†æå¼•æ“ç‰ˆæœ¬: dict-log-v2")
            update_task(task_id, "running", "æ­£åœ¨å‡†å¤‡è‚¡ç¥¨å­—å…¸...")

            def _log_progress(msg: str):
                add_task_log(task_id, msg)
                # å°†å…³é”®è¿›åº¦åŒæ­¥åˆ°ä»»åŠ¡æ‘˜è¦ï¼Œä¾¿äºå‰ç«¯ä¾§è¾¹æ å±•ç¤º
                if any(k in msg for k in ["å¼€å§‹æ‰«æ", "å·²æ‰«æ", "å¼€å§‹è®¡ç®—", "å·²è®¡ç®—", "æ‰«æå®Œæˆ", "å…¨éƒ¨å®Œæˆ"]):
                    update_task(task_id, "running", msg)

            analyzer = StockAnalyzer(group_id, log_callback=_log_progress)
            result = analyzer.scan_group(force=force)

            add_task_log(task_id, f"âœ… æ‰«æå®Œæˆ: {result['mentions_extracted']} æ¬¡æåŠ, {result['unique_stocks']} åªè‚¡ç¥¨")
            update_task(task_id, "completed",
                        f"å®Œæˆ: {result['topics_scanned']} å¸–å­, {result['mentions_extracted']} æ¬¡æåŠ, "
                        f"{result['unique_stocks']} åªè‚¡ç¥¨, {result['performance_calculated']} æ¡è¡¨ç°è®¡ç®—")
        except Exception as e:
            add_task_log(task_id, f"âŒ æ‰«æå¤±è´¥: {e}")
            update_task(task_id, "failed", f"æ‰«æå¤±è´¥: {e}")

    background_tasks.add_task(_scan_task)
    return {"task_id": task_id, "message": "è‚¡ç¥¨æ‰«æä»»åŠ¡å·²å¯åŠ¨"}


# ========== å…¨å±€çœ‹æ¿ API ==========


def _parse_global_crawl_time(raw: Optional[str], field_name: str) -> Optional[datetime]:
    """è§£æå¹¶æ ¡éªŒå…¨åŒº range æ¨¡å¼æ—¶é—´å‚æ•°ã€‚"""
    if raw is None:
        return None
    text = str(raw).strip()
    if not text:
        return None
    try:
        # å…¼å®¹ datetime-localï¼ˆæ— ç§’ï¼‰
        if "T" in text and len(text) == 16:
            text = text + ":00"
        if text.endswith("Z"):
            text = text[:-1] + "+00:00"
        if len(text) >= 24 and text[-5] in ["+", "-"] and text[-3] != ":":
            text = text[:-2] + ":" + text[-2:]
        dt = datetime.fromisoformat(text)
        return dt
    except Exception:
        raise HTTPException(
            status_code=422,
            detail=f"{field_name} æ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ ISO8601ï¼ˆä¾‹å¦‚ 2026-02-21T10:00:00+08:00ï¼‰",
        )


def api_global_crawl(request: GlobalCrawlRequest, background_tasks: BackgroundTasks):
    """å…¨åŒºè¯é¢˜é‡‡é›†ï¼ˆè½®è¯¢æ‰€æœ‰ç¾¤ç»„ï¼‰"""
    if request.mode == "range":
        has_last_days = request.last_days is not None
        has_time_range = bool((request.start_time or "").strip() or (request.end_time or "").strip())
        if has_last_days and int(request.last_days) < 1:
            raise HTTPException(status_code=422, detail="last_days å¿…é¡»å¤§äº 0")
        if has_last_days and has_time_range:
            raise HTTPException(
                status_code=422,
                detail="range æ¨¡å¼ä¸‹ï¼Œâ€œæœ€è¿‘å¤©æ•°(last_days)â€ä¸â€œå¼€å§‹/ç»“æŸæ—¶é—´(start_time/end_time)â€å¿…é¡»äºŒé€‰ä¸€",
            )
        if request.start_time:
            _parse_global_crawl_time(request.start_time, "start_time")
        if request.end_time:
            _parse_global_crawl_time(request.end_time, "end_time")

    global task_counter
    task_counter += 1
    task_id = f"global_crawl_{task_counter}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": "global_crawl",
        "status": "running",
        "message": "æ­£åœ¨åˆå§‹åŒ–å…¨åŒºè¯é¢˜é‡‡é›†...",
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "result": None
    }
    task_logs[task_id] = []
    task_stop_flags[task_id] = False

    def _global_crawl_task(task_id: str):
        from api.services.global_crawl_service import GlobalCrawlService

        GlobalCrawlService().run(
            task_id=task_id,
            request=request,
            add_task_log=add_task_log,
            update_task=update_task,
            is_task_stopped=is_task_stopped,
            get_cookie_for_group=get_cookie_for_group,
        )

    background_tasks.add_task(_global_crawl_task, task_id)
    return {"task_id": task_id, "message": "å…¨åŒºé‡‡é›†ä»»åŠ¡å·²å¯åŠ¨"}

def api_global_files_collect(request: GlobalFileCollectRequest, background_tasks: BackgroundTasks):
    """å…¨åŒºæ–‡ä»¶åˆ—è¡¨æ”¶é›†"""
    global task_counter
    task_counter += 1
    task_id = f"global_files_collect_{task_counter}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": "global_files_collect",
        "status": "running",
        "message": "æ­£åœ¨åˆå§‹åŒ–å…¨åŒºæ–‡ä»¶åˆ—è¡¨æ”¶é›†...",
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "result": None
    }
    task_logs[task_id] = []
    task_stop_flags[task_id] = False

    def _global_collect_task(task_id: str):
        from api.services.global_file_task_service import GlobalFileTaskService

        GlobalFileTaskService().run_collect(
            task_id=task_id,
            add_task_log=add_task_log,
            update_task=update_task,
            is_task_stopped=is_task_stopped,
            get_cookie_for_group=get_cookie_for_group,
            file_downloader_instances=file_downloader_instances,
        )

    background_tasks.add_task(_global_collect_task, task_id)
    return {"task_id": task_id, "message": "å…¨åŒºæ”¶é›†ä»»åŠ¡å·²å¯åŠ¨"}

def api_global_files_download(request: GlobalFileDownloadRequest, background_tasks: BackgroundTasks):
    """å…¨åŒºæ–‡ä»¶ä¸‹è½½"""
    # æˆ‘ä»¬å¯ä»¥å¤ç”¨ run_file_download_task_logic
    global task_counter
    task_counter += 1
    task_id = f"global_files_download_{task_counter}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": "global_files_download",
        "status": "running",
        "message": "æ­£åœ¨åˆå§‹åŒ–å…¨åŒºæ–‡ä»¶ä¸‹è½½...",
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "result": None
    }
    task_logs[task_id] = []
    task_stop_flags[task_id] = False

    def _global_download_task(task_id: str):
        from api.services.global_file_task_service import GlobalFileTaskService

        GlobalFileTaskService().run_download(
            task_id=task_id,
            request=request,
            add_task_log=add_task_log,
            update_task=update_task,
            is_task_stopped=is_task_stopped,
            get_cookie_for_group=get_cookie_for_group,
            file_downloader_instances=file_downloader_instances,
        )

    background_tasks.add_task(_global_download_task, task_id)
    return {"task_id": task_id, "message": "å…¨åŒºä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨"}

def api_global_analyze_performance(background_tasks: BackgroundTasks, force: bool = False):
    """å…¨åŒºæ”¶ç›Šåˆ·æ–°"""
    global task_counter
    task_counter += 1
    task_id = f"global_analyze_performance_{task_counter}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": "global_analyze_performance",
        "status": "running",
        "message": "æ­£åœ¨åˆå§‹åŒ–å…¨åŒºæ”¶ç›Šè®¡ç®—...",
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "result": None
    }
    task_logs[task_id] = []
    task_stop_flags[task_id] = False

    def _global_analyze_task(task_id: str):
        from api.services.global_analyze_service import GlobalAnalyzePerformanceService

        GlobalAnalyzePerformanceService().run(
            task_id=task_id,
            add_task_log=add_task_log,
            update_task=update_task,
            is_task_stopped=is_task_stopped,
            calc_window_days=365,
        )

    background_tasks.add_task(_global_analyze_task, task_id)
    return {"task_id": task_id, "message": "å…¨åŒºè®¡ç®—ä»»åŠ¡å·²å¯åŠ¨"}

async def cleanup_excluded_stocks(scope: str = "all", group_id: Optional[str] = None):
    """æ¸…ç†è¢« stock_exclude.json å‘½ä¸­çš„å†å²è‚¡ç¥¨æ•°æ®"""
    try:
        from modules.shared.stock_exclusion import build_sql_exclusion_clause
        from modules.shared.db_path_manager import get_db_path_manager

        if scope not in ("all", "group"):
            raise HTTPException(status_code=400, detail="scope ä»…æ”¯æŒ all æˆ– group")
        if scope == "group" and not group_id:
            raise HTTPException(status_code=400, detail="scope=group æ—¶å¿…é¡»æä¾› group_id")

        manager = get_db_path_manager()
        groups = manager.list_all_groups()
        if scope == "group":
            groups = [g for g in groups if str(g.get("group_id")) == str(group_id)]

        exclude_clause, exclude_params = build_sql_exclusion_clause("stock_code", "stock_name")
        if not exclude_clause:
            return {
                "groups_processed": 0,
                "mentions_deleted": 0,
                "performances_deleted": 0,
                "details": [],
                "message": "æœªé…ç½®æ’é™¤è§„åˆ™ï¼Œæ— éœ€æ¸…ç†"
            }

        total_mentions_deleted = 0
        total_perf_deleted = 0
        details: List[Dict[str, Any]] = []

        for group in groups:
            gid = str(group.get("group_id"))
            db_path = group.get("topics_db")
            if not db_path or not os.path.exists(db_path):
                continue

            mentions_deleted = 0
            perf_deleted = 0
            conn = None
            try:
                import sqlite3
                conn = sqlite3.connect(db_path, timeout=30)
                cursor = conn.cursor()

                cursor.execute('''
                    SELECT 1 FROM sqlite_master
                    WHERE type = 'table' AND name = 'stock_mentions'
                ''')
                if cursor.fetchone() is None:
                    continue

                cursor.execute(
                    f"SELECT id FROM stock_mentions WHERE NOT (1=1 {exclude_clause})",
                    exclude_params
                )
                mention_ids = [row[0] for row in cursor.fetchall()]

                if mention_ids:
                    placeholders = ",".join(["?"] * len(mention_ids))
                    cursor.execute(
                        f"DELETE FROM mention_performance WHERE mention_id IN ({placeholders})",
                        mention_ids
                    )
                    perf_deleted = cursor.rowcount or 0

                    cursor.execute(
                        f"DELETE FROM stock_mentions WHERE id IN ({placeholders})",
                        mention_ids
                    )
                    mentions_deleted = cursor.rowcount or 0

                conn.commit()
            except Exception as e:
                if conn:
                    conn.rollback()
                details.append({
                    "group_id": gid,
                    "mentions_deleted": 0,
                    "performances_deleted": 0,
                    "error": str(e)
                })
                continue
            finally:
                if conn:
                    conn.close()

            total_mentions_deleted += mentions_deleted
            total_perf_deleted += perf_deleted
            details.append({
                "group_id": gid,
                "mentions_deleted": mentions_deleted,
                "performances_deleted": perf_deleted
            })

        try:
            from modules.analyzers.global_analyzer import get_global_analyzer
            get_global_analyzer().invalidate_cache()
        except Exception:
            pass

        return {
            "groups_processed": len(details),
            "mentions_deleted": total_mentions_deleted,
            "performances_deleted": total_perf_deleted,
            "details": details
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç†æ’é™¤è‚¡ç¥¨å¤±è´¥: {str(e)}")


# migrated to api/routers/global_tasks.py: @app.get("/api/global/scan-filter/config")
async def get_global_scan_filter_config():
    """è·å–éè‚¡ç¥¨ç¾¤æ’é™¤è§„åˆ™ï¼ˆæ‰‹åŠ¨ç™½é»‘åå•ï¼‰"""
    try:
        from modules.shared.group_scan_filter import get_filter_config, CONFIG_FILE
        data = get_filter_config()
        data["source_file"] = CONFIG_FILE
        return data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¯»å–æ‰«æè¿‡æ»¤é…ç½®å¤±è´¥: {str(e)}")


# migrated to api/routers/global_tasks.py: @app.put("/api/global/scan-filter/config")
async def update_global_scan_filter_config(request: ScanFilterConfigRequest):
    """æ›´æ–°éè‚¡ç¥¨ç¾¤æ’é™¤è§„åˆ™ï¼ˆæ‰‹åŠ¨ç™½é»‘åå•ï¼‰"""
    try:
        from modules.shared.group_scan_filter import save_filter_config
        data = save_filter_config(
            default_action=request.default_action,
            whitelist_group_ids=request.whitelist_group_ids,
            blacklist_group_ids=request.blacklist_group_ids
        )
        return {
            **data,
            "effective_counts": {
                "whitelist": len(data.get("whitelist_group_ids", [])),
                "blacklist": len(data.get("blacklist_group_ids", [])),
            }
        }
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ‰«æè¿‡æ»¤é…ç½®å¤±è´¥: {str(e)}")


# migrated to api/routers/global_tasks.py: @app.get("/api/global/scan-filter/preview")
async def preview_global_scan_filter(exclude_non_stock: bool = True):
    """é¢„è§ˆå½“å‰æ‰«æè¿‡æ»¤å‘½ä¸­ç»“æœ"""
    try:
        from modules.shared.db_path_manager import get_db_path_manager
        from modules.shared.group_scan_filter import decide_group

        manager = get_db_path_manager()
        groups = manager.list_all_groups()

        included_groups = []
        excluded_groups = []
        reason_counts: Dict[str, int] = {}

        for g in groups:
            gid = str(g.get("group_id"))
            gname = _get_group_name_for_scan_filter(gid, g.get("topics_db"))
            decision, reason = decide_group(gid)

            item = {
                "group_id": gid,
                "group_name": gname or gid,
                "decision": decision,
                "reason": reason,
            }
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
            if decision == "included":
                included_groups.append(item)
            else:
                excluded_groups.append(item)

        return {
            "total_groups": len(groups),
            "included_groups": included_groups,
            "excluded_groups": excluded_groups,
            "reason_counts": reason_counts,
            "compat_note": (
                "exclude_non_stock å‚æ•°å·²å…¼å®¹ä¿ç•™ï¼Œå½“å‰ç‰ˆæœ¬å§‹ç»ˆåº”ç”¨ç™½é»‘åå•è§„åˆ™"
                if exclude_non_stock is False else None
            ),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„è§ˆæ‰«æè¿‡æ»¤ç»“æœå¤±è´¥: {str(e)}")


# migrated to api/routers/global_tasks.py: @app.get("/api/global/scan-filter/cleanup-blacklist/preview")
async def preview_blacklist_cleanup():
    """é¢„è§ˆé»‘åå•ç¾¤ç»„å¯æ¸…ç†çš„åˆ†ææ•°æ®è§„æ¨¡ã€‚"""
    try:
        from modules.shared.db_path_manager import get_db_path_manager
        from modules.shared.group_scan_filter import get_filter_config
        import sqlite3

        cfg = get_filter_config()
        blacklist_ids = set(str(v).strip() for v in cfg.get("blacklist_group_ids", []) if str(v).strip())
        manager = get_db_path_manager()
        groups = manager.list_all_groups()

        details = []
        total_mentions = 0
        total_performance = 0

        for g in groups:
            gid = str(g.get("group_id", "")).strip()
            if not gid or gid not in blacklist_ids:
                continue

            db_path = g.get("topics_db")
            mentions_count = 0
            perf_count = 0
            if db_path and os.path.exists(db_path):
                conn = None
                try:
                    conn = sqlite3.connect(db_path, timeout=30)
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='stock_mentions'")
                    if (cursor.fetchone() or [0])[0]:
                        cursor.execute("SELECT COUNT(*) FROM stock_mentions")
                        mentions_count = int((cursor.fetchone() or [0])[0] or 0)
                        cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='mention_performance'")
                        if (cursor.fetchone() or [0])[0]:
                            cursor.execute("SELECT COUNT(*) FROM mention_performance")
                            perf_count = int((cursor.fetchone() or [0])[0] or 0)
                except Exception:
                    pass
                finally:
                    if conn:
                        conn.close()

            total_mentions += mentions_count
            total_performance += perf_count
            details.append({
                "group_id": gid,
                "group_name": _get_group_name_for_scan_filter(gid, db_path),
                "stock_mentions_count": mentions_count,
                "mention_performance_count": perf_count,
            })

        return {
            "blacklist_group_count": len(blacklist_ids),
            "matched_group_count": len(details),
            "total_stock_mentions": total_mentions,
            "total_mention_performance": total_performance,
            "groups": details,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„è§ˆé»‘åå•æ¸…ç†å¤±è´¥: {str(e)}")


async def cleanup_blacklist_data(background_tasks: BackgroundTasks):
    """æ¸…ç†é»‘åå•ç¾¤ç»„ä¸­çš„åˆ†ææ•°æ®ï¼ˆstock_mentions / mention_performanceï¼‰ã€‚"""
    global task_counter
    task_counter += 1
    task_id = f"global_cleanup_blacklist_{task_counter}"

    current_tasks[task_id] = {
        "task_id": task_id,
        "type": "global_cleanup_blacklist",
        "status": "running",
        "message": "æ­£åœ¨åˆå§‹åŒ–é»‘åå•æ•°æ®æ¸…ç†...",
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "result": None,
    }
    task_logs[task_id] = []
    task_stop_flags[task_id] = False

    def _cleanup_task(task_id: str):
        try:
            from modules.shared.db_path_manager import get_db_path_manager
            from modules.shared.group_scan_filter import get_filter_config
            import sqlite3

            update_task(task_id, "running", "å¼€å§‹æ¸…ç†é»‘åå•å†å²åˆ†ææ•°æ®...")
            cfg = get_filter_config()
            blacklist_ids = set(str(v).strip() for v in cfg.get("blacklist_group_ids", []) if str(v).strip())
            manager = get_db_path_manager()
            groups = manager.list_all_groups()
            target_groups = [g for g in groups if str(g.get("group_id", "")).strip() in blacklist_ids]

            add_task_log(task_id, f"ğŸ“‹ é»‘åå•ç¾¤ç»„æ€»æ•°: {len(blacklist_ids)}ï¼Œæœ¬åœ°åŒ¹é…: {len(target_groups)}")
            if not target_groups:
                update_task(task_id, "completed", "é»‘åå•æ¸…ç†å®Œæˆ: æ— åŒ¹é…æœ¬åœ°ç¾¤ç»„")
                return

            total_mentions_deleted = 0
            total_perf_deleted = 0
            processed = 0

            for i, g in enumerate(target_groups, 1):
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ æ¸…ç†ä»»åŠ¡å·²åœæ­¢")
                    break

                gid = str(g.get("group_id", "")).strip()
                db_path = g.get("topics_db")
                add_task_log(task_id, f"ğŸ‘‰ [{i}/{len(target_groups)}] æ¸…ç†ç¾¤ç»„ {gid}")

                if not db_path or not os.path.exists(db_path):
                    add_task_log(task_id, f"   âš ï¸ ç¾¤ç»„ {gid} æ— å¯ç”¨ topics_dbï¼Œè·³è¿‡")
                    continue

                conn = None
                try:
                    conn = sqlite3.connect(db_path, timeout=30)
                    cursor = conn.cursor()

                    cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='stock_mentions'")
                    has_mentions = bool((cursor.fetchone() or [0])[0])
                    cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='mention_performance'")
                    has_perf = bool((cursor.fetchone() or [0])[0])
                    if not has_mentions:
                        add_task_log(task_id, f"   â„¹ï¸ ç¾¤ç»„ {gid} æ—  stock_mentionsï¼Œè·³è¿‡")
                        continue

                    perf_deleted = 0
                    if has_perf:
                        cursor.execute(
                            "DELETE FROM mention_performance WHERE mention_id IN (SELECT id FROM stock_mentions)"
                        )
                        perf_deleted = cursor.rowcount or 0

                    cursor.execute("DELETE FROM stock_mentions")
                    mentions_deleted = cursor.rowcount or 0
                    conn.commit()

                    total_perf_deleted += perf_deleted
                    total_mentions_deleted += mentions_deleted
                    processed += 1
                    add_task_log(task_id, f"   âœ… å®Œæˆ: åˆ é™¤æåŠ {mentions_deleted}ï¼Œæ”¶ç›Š {perf_deleted}")
                except Exception as e:
                    if conn:
                        conn.rollback()
                    add_task_log(task_id, f"   âŒ æ¸…ç†å¤±è´¥: {e}")
                finally:
                    if conn:
                        conn.close()

            try:
                from modules.analyzers.global_analyzer import get_global_analyzer
                get_global_analyzer().invalidate_cache()
                add_task_log(task_id, "ğŸ”„ å…¨å±€ç»Ÿè®¡ç¼“å­˜å·²åˆ·æ–°")
            except Exception:
                pass

            if is_task_stopped(task_id):
                update_task(task_id, "cancelled", "é»‘åå•æ¸…ç†å·²åœæ­¢")
            else:
                update_task(
                    task_id,
                    "completed",
                    f"é»‘åå•æ¸…ç†å®Œæˆ: {processed}/{len(target_groups)} ä¸ªç¾¤ç»„ï¼Œåˆ é™¤æåŠ {total_mentions_deleted}ï¼Œæ”¶ç›Š {total_perf_deleted}",
                    {
                        "groups_processed": processed,
                        "groups_total": len(target_groups),
                        "mentions_deleted": total_mentions_deleted,
                        "performances_deleted": total_perf_deleted,
                    },
                )
        except Exception as e:
            add_task_log(task_id, f"âŒ é»‘åå•æ¸…ç†å¼‚å¸¸: {e}")
            update_task(task_id, "failed", f"é»‘åå•æ¸…ç†å¤±è´¥: {e}")

    background_tasks.add_task(_cleanup_task, task_id)
    return {"task_id": task_id, "message": "é»‘åå•æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨"}


STOCK_GROUP_HINT_KEYWORDS = (
    "è‚¡ç¥¨", "aè‚¡", "æ¸¯è‚¡", "ç¾è‚¡", "åŸºé‡‘", "æŠ•èµ„", "äº¤æ˜“", "å¤ç›˜", "é‡åŒ–", "è´¢ç»", "è¯åˆ¸", "ç ”æŠ¥", "æ‹©æ—¶"
)


def _get_group_name_for_scan_filter(group_id: str, topics_db_path: Optional[str]) -> str:
    """å°½é‡è·å–ç¾¤ç»„åç§°ï¼ˆæœ¬åœ°DB -> group_meta.jsonï¼‰"""
    import sqlite3
    from pathlib import Path

    if topics_db_path and os.path.exists(topics_db_path):
        try:
            conn = sqlite3.connect(topics_db_path, timeout=10)
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM groups WHERE group_id = ? LIMIT 1", (int(group_id),))
            row = cursor.fetchone()
            conn.close()
            if row and row[0]:
                return str(row[0])
        except Exception:
            pass

    try:
        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_data_dir(str(group_id))
        meta_path = Path(group_dir) / "group_meta.json"
        if meta_path.exists():
            with meta_path.open("r", encoding="utf-8") as f:
                meta = json.load(f)
            if meta.get("name"):
                return str(meta["name"])
    except Exception:
        pass

    return ""


def _group_has_stock_mentions_for_scan_filter(topics_db_path: Optional[str]) -> bool:
    """åˆ¤æ–­ç¾¤ç»„æœ¬åœ°åº“æ˜¯å¦å·²æœ‰è‚¡ç¥¨æåŠè®°å½•ã€‚"""
    import sqlite3

    if not topics_db_path or not os.path.exists(topics_db_path):
        return False

    try:
        conn = sqlite3.connect(topics_db_path, timeout=10)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM sqlite_master WHERE type = 'table' AND name = 'stock_mentions'")
        has_table = cursor.fetchone() is not None
        if not has_table:
            conn.close()
            return False

        cursor.execute("SELECT 1 FROM stock_mentions LIMIT 1")
        has_mentions = cursor.fetchone() is not None
        conn.close()
        return has_mentions
    except Exception:
        return False


def _is_stock_candidate_group_for_scan(group: Dict[str, Any]):
    """æ‰«æè¿‡æ»¤è§„åˆ™ï¼šå†å²æœ‰è‚¡ç¥¨æåŠï¼Œæˆ–ç¾¤ååŒ…å«è‚¡ç¥¨å…³é”®è¯ã€‚"""
    group_id = str(group.get("group_id", ""))
    topics_db_path = group.get("topics_db")
    group_name = _get_group_name_for_scan_filter(group_id, topics_db_path)
    normalized_name = group_name.lower()

    name_hit = any(keyword in normalized_name for keyword in STOCK_GROUP_HINT_KEYWORDS)
    mentions_hit = _group_has_stock_mentions_for_scan_filter(topics_db_path)

    if mentions_hit:
        return True, "å·²æœ‰è‚¡ç¥¨æåŠ"
    if name_hit:
        return True, "ç¾¤åå‘½ä¸­å…³é”®è¯"
    return False, "æ— æåŠä¸”ç¾¤åæœªå‘½ä¸­"


def scan_global(background_tasks: BackgroundTasks, force: bool = False, exclude_non_stock: bool = False):
    """å…¨å±€æ‰«ææ‰€æœ‰ç¾¤ç»„çš„è‚¡ç¥¨æ•°æ®ï¼ˆåå°ä»»åŠ¡ï¼‰"""
    global task_counter
    task_counter += 1
    task_id = f"global_scan_{task_counter}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": "global_scan",
        "status": "running",
        "message": "æ­£åœ¨åˆå§‹åŒ–å…¨å±€æ‰«æ...",
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "result": None
    }
    task_logs[task_id] = []
    task_stop_flags[task_id] = False

    def _global_scan_task(task_id: str):
        try:
            update_task(task_id, "running", "å‡†å¤‡å¼€å§‹å…¨å±€æ‰«æ...")
            add_task_log(task_id, "ğŸš€ å¼€å§‹å…¨å±€è‚¡ç¥¨æåŠæ‰«æ...")
            
            from modules.shared.db_path_manager import get_db_path_manager
            from modules.analyzers.global_pipeline import run_serial_incremental_pipeline
            
            manager = get_db_path_manager()
            groups = manager.list_all_groups()
            original_count = len(groups)
            add_task_log(task_id, f"ğŸ“‹ å…±å‘ç° {original_count} ä¸ªç¾¤ç»„")
            if force:
                add_task_log(task_id, "â„¹ï¸ å½“å‰å…¨å±€æ‰«æçš„ç¼–æ’æ¨¡å¼ä¸åŒºåˆ† forceï¼ŒæŒ‰å¢é‡é‡‡é›†æ‰§è¡Œ")
            if exclude_non_stock is False:
                add_task_log(task_id, "â„¹ï¸ å‚æ•° exclude_non_stock å·²å…¼å®¹ä¿ç•™ï¼Œå½“å‰ç‰ˆæœ¬å§‹ç»ˆå¼ºåˆ¶åº”ç”¨ç™½é»‘åå•è§„åˆ™")

            from api.services.group_filter_service import apply_group_scan_filter

            filtered = apply_group_scan_filter(groups)
            groups = filtered["included_groups"]
            excluded_groups = filtered["excluded_groups"]
            reason_counts = filtered["reason_counts"]
            default_action = filtered["default_action"]
            add_task_log(task_id, f"âš™ï¸ è¿‡æ»¤ç­–ç•¥: æœªé…ç½®ç¾¤ç»„é»˜è®¤{'çº³å…¥' if default_action == 'include' else 'æ’é™¤'}")
            add_task_log(task_id, f"ğŸ§¹ ç™½é»‘åå•è¿‡æ»¤åï¼šä¿ç•™ {len(groups)}/{original_count} ä¸ªç¾¤ç»„")
            if reason_counts:
                add_task_log(task_id, f"ğŸ“Œ å‘½ä¸­ç»Ÿè®¡: {reason_counts}")
            if excluded_groups:
                preview = "ï¼Œ".join(
                    f"{g.get('group_id')}({g.get('scan_filter_reason', 'unknown')})"
                    for g in excluded_groups[:20]
                )
                suffix = " ..." if len(excluded_groups) > 20 else ""
                add_task_log(task_id, f"ğŸš« å·²æ’é™¤: {preview}{suffix}")

            if not groups:
                add_task_log(task_id, "â„¹ï¸ è¿‡æ»¤åæ— å¯æ‰«æç¾¤ç»„ï¼Œä»»åŠ¡ç»“æŸ")
                update_task(task_id, "completed", "å…¨å±€æ‰«æå®Œæˆ: è¿‡æ»¤åæ— å¯æ‰«æç¾¤ç»„")
                return

            successes, failures = run_serial_incremental_pipeline(
                groups=groups,
                pages=2,
                per_page=20,
                calc_window_days=365,
                do_analysis=False,
                stop_check=lambda: is_task_stopped(task_id),
                log_callback=lambda msg: add_task_log(task_id, msg),
            )
            total_mentions = sum((item.get("extract") or {}).get("mentions_extracted", 0) for item in successes)

            if is_task_stopped(task_id):
                update_task(task_id, "cancelled", "å…¨å±€æ‰«æå·²åœæ­¢")
            else:
                add_task_log(task_id, "")
                add_task_log(task_id, "=" * 50)
                add_task_log(task_id, f"ğŸ‰ å…¨å±€æ‰«æå®Œæˆï¼å…±å¤„ç† {len(successes)}/{len(groups)} ä¸ªç¾¤ç»„")
                add_task_log(task_id, f"ğŸ“Š æœ¬æ¬¡ç´¯è®¡æå–æåŠ: {total_mentions} æ¬¡")
                if failures:
                    add_task_log(task_id, f"âš ï¸ å¤±è´¥ç¾¤ç»„: {len(failures)} ä¸ª")
                
                # è§¦å‘å…¨å±€åˆ†æå™¨ç¼“å­˜å¤±æ•ˆ
                try:
                    from modules.analyzers.global_analyzer import get_global_analyzer
                    get_global_analyzer().invalidate_cache()
                    add_task_log(task_id, "ğŸ”„ å…¨å±€ç»Ÿè®¡ç¼“å­˜å·²åˆ·æ–°")
                except:
                    pass
                
                update_task(task_id, "completed", f"å…¨å±€æ‰«æå®Œæˆ: {len(successes)} ä¸ªç¾¤ç»„, {total_mentions} æ¬¡æåŠ")

        except Exception as e:
            add_task_log(task_id, f"âŒ å…¨å±€æ‰«æå¼‚å¸¸: {e}")
            update_task(task_id, "failed", f"å…¨å±€æ‰«æå¤±è´¥: {e}")

    background_tasks.add_task(_global_scan_task, task_id)
    return {"task_id": task_id, "message": "å…¨å±€æ‰«æä»»åŠ¡å·²å¯åŠ¨"}


if __name__ == "__main__":
    import sys
    port = 8208  # é»˜è®¤ç«¯å£
    if len(sys.argv) > 2 and sys.argv[1] == "--port":
        try:
            port = int(sys.argv[2])
        except ValueError:
            port = 8208
    print(f"[startup] API version=1.0.0, port={port}")
    print("[startup] feature routes: /api/global/sector-topics, /api/scheduler/next-runs, /api/meta/features")
    uvicorn.run(app, host="0.0.0.0", port=port)
