#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‚¡ç¥¨èˆ†æƒ…åˆ†ææ¨¡å—
ä»çŸ¥è¯†æ˜Ÿçƒå¸–å­ä¸­æå–è‚¡ç¥¨åç§°ï¼Œç»“åˆAè‚¡è¡Œæƒ…æ•°æ®è¿›è¡Œäº‹ä»¶ç ”ç©¶åˆ†æ
"""

import re
import sqlite3
import time
import json
import os
import threading
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

import ahocorasick
import akshare as ak

from db_path_manager import get_db_path_manager
from logger_config import log_info, log_warning, log_error, log_debug


# ========== å¸¸é‡ ==========

# æ¿å—å…³é”®è¯æ˜ å°„
SECTOR_KEYWORDS = {
    "AIåº”ç”¨": ["aiåº”ç”¨", "å¤§æ¨¡å‹", "deepseek", "chatgpt", "gpt", "é€šä¹‰", "æ–‡å¿ƒ", "æ™ºè°±", "glm"],
    "AIç®—åŠ›": ["ç®—åŠ›", "å…‰æ¨¡å—", "cpo", "ä¸­é™…æ—­åˆ›", "æ–°æ˜“ç››", "å¤©å­šé€šä¿¡", "å…‰é€šä¿¡"],
    "å•†ä¸šèˆªå¤©": ["å•†ä¸šèˆªå¤©", "ç«ç®­", "å«æ˜Ÿ", "èˆªå¤©", "æ˜Ÿé“¾", "ä½è½¨"],
    "æœºå™¨äºº": ["æœºå™¨äºº", "äººå½¢æœºå™¨äºº", "å®‡æ ‘", "ç‰¹æ–¯æ‹‰æœºå™¨äºº", "optimus"],
    "åŠå¯¼ä½“": ["åŠå¯¼ä½“", "èŠ¯ç‰‡", "æ™¶åœ†", "å°æµ‹", "å…‰åˆ»", "å…ˆè¿›å°è£…", "å›½äº§èŠ¯ç‰‡"],
    "æ–°èƒ½æº": ["å…‰ä¼", "é”‚ç”µ", "å‚¨èƒ½", "æ–°èƒ½æº", "é£ç”µ", "æ°¢èƒ½"],
    "æ¶¨ä»·é“¾": ["æ¶¨ä»·", "æä»·", "æ¶¨ä»·å‡½", "æ¶¨ä»·é€»è¾‘"],
    "å†›å·¥": ["å†›å·¥", "å›½é˜²", "å¯¼å¼¹", "æ— äººæœº"],
    "åŒ»è¯": ["åŒ»è¯", "åˆ›æ–°è¯", "cxo", "ç”Ÿç‰©åŒ»è¯"],
    "æ¶ˆè´¹": ["æ¶ˆè´¹", "ç™½é…’", "é£Ÿé¥®", "è°ƒå‘³å“", "é¢„è°ƒé…’"],
    "åœ°äº§": ["åœ°äº§", "æˆ¿åœ°äº§", "äºŒæ‰‹æˆ¿", "æ–°æˆ¿", "ä¿ç§Ÿæˆ¿"],
}

# éœ€è¦è¿‡æ»¤çš„å¸¸è§è¯¯åŒ¹é…è¯
EXCLUDE_WORDS = frozenset([
    "ä¸­å›½", "ç¾å›½", "æ—¥æœ¬", "éŸ©å›½", "æ¬§æ´²", "å…¨çƒ", "é¦™æ¸¯",
    "ä¸Šæµ·", "åŒ—äº¬", "æ·±åœ³", "å¹¿å·", "æ­å·",
    "å…¬å¸", "é›†å›¢", "ç§‘æŠ€", "è‚¡ä»½", "ç”µå­", "ä¿¡æ¯", "é€šä¿¡",
    "é“¶è¡Œ", "è¯åˆ¸", "ä¿é™©",
    "å¸‚åœº", "è¡Œä¸š", "æ¿å—",
    "å¤§å®¶", "ä»Šå¤©", "æ˜å¤©", "æ˜¨å¤©", "ä»Šå¹´", "å»å¹´", "æ˜å¹´",
    "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰",
    "æ ¸å¿ƒ", "é¾™å¤´", "è¶‹åŠ¿",
])

# åŒ—äº¬æ—¶åŒº
BEIJING_TZ = timezone(timedelta(hours=8))


class StockAnalyzer:
    """è‚¡ç¥¨èˆ†æƒ…åˆ†æå¼•æ“"""
    # è¿›ç¨‹çº§å­—å…¸ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡ä»»åŠ¡é‡å¤æ„å»º
    _dict_lock = threading.RLock()
    _global_automaton = None
    _global_stock_dict: Dict[str, str] = {}
    _global_name_to_code: Dict[str, str] = {}
    _global_built_at: float = 0.0

    # æœ¬åœ°ç¼“å­˜æ—¶æ•ˆï¼ˆç§’ï¼‰ï¼Œé»˜è®¤12å°æ—¶
    DICT_CACHE_TTL_SECONDS = int(os.environ.get("STOCK_DICT_CACHE_TTL_SECONDS", "43200"))
    DICT_CACHE_FILE = "stock_dict_cache.json"

    def __init__(self, group_id: str, log_callback=None):
        self.group_id = group_id
        self.log_callback = log_callback
        self.db_path_manager = get_db_path_manager()

        # è¯é¢˜æ•°æ®åº“è·¯å¾„
        self.topics_db_path = self.db_path_manager.get_topics_db_path(group_id)

        # åˆå§‹åŒ–è‚¡ç¥¨åˆ†æç›¸å…³è¡¨
        self._init_stock_tables()

        # è‚¡ç¥¨å­—å…¸ (å»¶è¿ŸåŠ è½½)
        self._automaton = None
        self._stock_dict = {}  # code -> name
        self._name_to_code = {}  # name -> code
        self._dict_cache_path = Path(self.db_path_manager.base_dir) / self.DICT_CACHE_FILE

    def log(self, message: str):
        """ç»Ÿä¸€æ—¥å¿—"""
        if self.log_callback:
            self.log_callback(message)
        log_info(message)

    def _get_conn(self):
        """è·å–å¸¦ WAL æ¨¡å¼å’Œè¶…æ—¶çš„æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(self.topics_db_path, check_same_thread=False, timeout=30)
        conn.execute('PRAGMA journal_mode=WAL')
        conn.execute('PRAGMA busy_timeout=30000')
        return conn

    # ========== æ•°æ®åº“åˆå§‹åŒ– ==========

    def _init_stock_tables(self):
        """åœ¨è¯é¢˜æ•°æ®åº“ä¸­åˆ›å»ºè‚¡ç¥¨åˆ†æç›¸å…³è¡¨"""
        conn = self._get_conn()
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS stock_mentions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                topic_id INTEGER NOT NULL,
                stock_code TEXT NOT NULL,
                stock_name TEXT NOT NULL,
                mention_date TEXT NOT NULL,
                mention_time TEXT NOT NULL,
                context_snippet TEXT,
                sentiment TEXT DEFAULT 'neutral',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (topic_id) REFERENCES topics (topic_id)
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS stock_price_cache (
                stock_code TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                open REAL,
                close REAL,
                high REAL,
                low REAL,
                change_pct REAL,
                volume REAL,
                PRIMARY KEY (stock_code, trade_date)
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS mention_performance (
                mention_id INTEGER PRIMARY KEY,
                stock_code TEXT NOT NULL,
                mention_date TEXT NOT NULL,
                price_at_mention REAL,
                return_1d REAL,
                return_3d REAL,
                return_5d REAL,
                return_10d REAL,
                return_20d REAL,
                return_60d REAL,
                return_120d REAL,
                return_250d REAL,
                excess_return_1d REAL,
                excess_return_3d REAL,
                excess_return_5d REAL,
                excess_return_10d REAL,
                excess_return_20d REAL,
                excess_return_60d REAL,
                excess_return_120d REAL,
                excess_return_250d REAL,
                max_return REAL,
                max_drawdown REAL,
                freeze_level INTEGER DEFAULT 0,
                FOREIGN KEY (mention_id) REFERENCES stock_mentions(id)
            )
        ''')

        # å…¼å®¹æ—§è¡¨ï¼šæ·»åŠ æ–°åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        for col in ['return_60d', 'return_120d', 'return_250d',
                     'excess_return_60d', 'excess_return_120d', 'excess_return_250d',
                     'freeze_level']:
            try:
                cursor.execute(f'ALTER TABLE mention_performance ADD COLUMN {col} REAL')
            except Exception:
                pass  # åˆ—å·²å­˜åœ¨

        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sm_stock_code ON stock_mentions(stock_code)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sm_mention_date ON stock_mentions(mention_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_sm_topic_id ON stock_mentions(topic_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_mp_stock_code ON mention_performance(stock_code)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_mp_mention_date ON mention_performance(mention_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_spc_date ON stock_price_cache(trade_date)')

        conn.commit()
        conn.close()

    # ========== è‚¡ç¥¨å­—å…¸æ„å»º ==========

    def _build_stock_dictionary(self):
        """æ„å»º/å¤ç”¨ A è‚¡è‚¡ç¥¨å­—å…¸ï¼Œä¼˜å…ˆä½¿ç”¨è¿›ç¨‹ç¼“å­˜å’Œæœ¬åœ°ç¼“å­˜"""
        self.log("æ­£åœ¨å‡†å¤‡Aè‚¡è‚¡ç¥¨å­—å…¸")

        if self._automaton is not None:
            total = len(self._name_to_code)
            self.log("ä»AkShareè·å–æ¸…å•ï¼ˆå®ä¾‹ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡ï¼‰")
            self.log(f"å­—å…¸å¤„ç†è¿›åº¦ {total}/{total}")
            self.log(f"ç´¢å¼•æ„å»ºè¿›åº¦ {total}/{total}")
            self.log("è‚¡ç¥¨å­—å…¸å°±ç»ª")
            return

        # å…ˆå¤ç”¨è¿›ç¨‹çº§ç¼“å­˜
        if StockAnalyzer._global_automaton is not None:
            self._automaton = StockAnalyzer._global_automaton
            self._stock_dict = StockAnalyzer._global_stock_dict
            self._name_to_code = StockAnalyzer._global_name_to_code
            total = len(self._name_to_code)
            self.log("ä»AkShareè·å–æ¸…å•ï¼ˆè¿›ç¨‹ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡ï¼‰")
            self.log(f"å­—å…¸å¤„ç†è¿›åº¦ {total}/{total}")
            self.log(f"ç´¢å¼•æ„å»ºè¿›åº¦ {total}/{total}")
            self.log("è‚¡ç¥¨å­—å…¸å°±ç»ª")
            return

        with StockAnalyzer._dict_lock:
            # åŒé‡æ£€æŸ¥ï¼Œé¿å…å¹¶å‘é‡å¤æ„å»º
            if StockAnalyzer._global_automaton is not None:
                self._automaton = StockAnalyzer._global_automaton
                self._stock_dict = StockAnalyzer._global_stock_dict
                self._name_to_code = StockAnalyzer._global_name_to_code
                total = len(self._name_to_code)
                self.log("ä»AkShareè·å–æ¸…å•ï¼ˆè¿›ç¨‹ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡ï¼‰")
                self.log(f"å­—å…¸å¤„ç†è¿›åº¦ {total}/{total}")
                self.log(f"ç´¢å¼•æ„å»ºè¿›åº¦ {total}/{total}")
                self.log("è‚¡ç¥¨å­—å…¸å°±ç»ª")
                return

            self.log("ä»AkShareè·å–æ¸…å•")
            stock_dict, name_to_code = self._load_stock_dictionary_from_cache()

            if not stock_dict:
                stock_dict, name_to_code = self._fetch_stock_dictionary_from_akshare()
                self._save_stock_dictionary_cache(stock_dict, name_to_code)
            else:
                total = len(name_to_code)
                self.log(f"å­—å…¸å¤„ç†è¿›åº¦ {total}/{total}")

            self._load_and_apply_user_aliases(name_to_code, stock_dict)
            self.log("æ­£åœ¨æ„å»ºè‚¡ç¥¨åŒ¹é…ç´¢å¼•")
            automaton = ahocorasick.Automaton()
            total = len(name_to_code)
            for idx, (name, code) in enumerate(name_to_code.items(), 1):
                if len(name) >= 2:
                    automaton.add_word(name, (code, name))
                if idx % 1000 == 0 or idx == total:
                    self.log(f"ç´¢å¼•æ„å»ºè¿›åº¦ {idx}/{total}")

            automaton.make_automaton()

            # å›å†™è¿›ç¨‹çº§ç¼“å­˜
            StockAnalyzer._global_automaton = automaton
            StockAnalyzer._global_stock_dict = stock_dict
            StockAnalyzer._global_name_to_code = name_to_code
            StockAnalyzer._global_built_at = time.time()

            self._automaton = automaton
            self._stock_dict = stock_dict
            self._name_to_code = name_to_code
            self.log("è‚¡ç¥¨å­—å…¸å°±ç»ª")

    def _load_and_apply_user_aliases(self, name_to_code: Dict[str, str], stock_dict: Dict[str, str]):
        """
        åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰åˆ«åå¹¶åº”ç”¨åˆ°å­—å…¸ä¸­
        stock_aliases.json æ ¼å¼: {"åˆ«å": "æ ‡å‡†è‚¡ç¥¨åç§°"}
        """
        alias_file = Path("stock_aliases.json")
        if not alias_file.exists():
            return

        try:
            with open(alias_file, "r", encoding="utf-8") as f:
                aliases = json.load(f)
            
            count = 0
            # å»ºç«‹åå‘æŸ¥æ‰¾è¡¨: Standard Name -> Code (stock_dict is Code -> Name)
            std_name_to_code = {v: k for k, v in stock_dict.items()}

            for alias, std_name in aliases.items():
                alias = alias.strip()
                std_name = std_name.strip()
                if not alias or not std_name:
                    continue
                
                # æŸ¥æ‰¾æ ‡å‡†åç§°å¯¹åº”çš„ä»£ç 
                code = std_name_to_code.get(std_name)
                
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯• std_name æ˜¯å¦æœ¬èº«å°±æ˜¯ä»£ç 
                if not code and std_name in stock_dict:
                     code = std_name
                
                if code:
                    # å°†åˆ«åæ˜ å°„åˆ°è¯¥ä»£ç 
                    name_to_code[alias] = code
                    count += 1
                else:
                    msg = f"åˆ«åé…ç½®é”™è¯¯: æ‰¾ä¸åˆ°è‚¡ç¥¨ '{std_name}' (åˆ«å: {alias})"
                    self.log(msg)
                    log_warning(msg)

            if count > 0:
                self.log(f"å·²åŠ è½½ {count} ä¸ªç”¨æˆ·è‡ªå®šä¹‰è‚¡ç¥¨åˆ«å")

        except Exception as e:
            msg = f"åŠ è½½è‚¡ç¥¨åˆ«åå¤±è´¥: {e}"
            self.log(msg)
            log_warning(msg)

    def _load_stock_dictionary_from_cache(self) -> Tuple[Dict[str, str], Dict[str, str]]:
        """å°è¯•ä»æœ¬åœ°ç¼“å­˜è¯»å–è‚¡ç¥¨å­—å…¸"""
        try:
            if not self._dict_cache_path.exists():
                return {}, {}

            with open(self._dict_cache_path, "r", encoding="utf-8") as f:
                payload = json.load(f)

            built_at = float(payload.get("built_at", 0))
            age = time.time() - built_at
            if age > self.DICT_CACHE_TTL_SECONDS:
                self.log("â™»ï¸ æœ¬åœ°è‚¡ç¥¨å­—å…¸ç¼“å­˜å·²è¿‡æœŸï¼Œå‡†å¤‡åˆ·æ–°")
                return {}, {}

            stock_dict = payload.get("stock_dict", {})
            name_to_code = payload.get("name_to_code", {})
            if isinstance(stock_dict, dict) and isinstance(name_to_code, dict) and stock_dict and name_to_code:
                self.log(f"âš¡ å·²åŠ è½½æœ¬åœ°è‚¡ç¥¨å­—å…¸ç¼“å­˜ï¼ˆ{len(name_to_code)}åªï¼‰")
                return stock_dict, name_to_code
            return {}, {}
        except Exception as e:
            log_warning(f"è¯»å–è‚¡ç¥¨å­—å…¸ç¼“å­˜å¤±è´¥: {e}")
            return {}, {}

    def _save_stock_dictionary_cache(self, stock_dict: Dict[str, str], name_to_code: Dict[str, str]):
        """ä¿å­˜è‚¡ç¥¨å­—å…¸åˆ°æœ¬åœ°ç¼“å­˜"""
        try:
            self._dict_cache_path.parent.mkdir(parents=True, exist_ok=True)
            payload = {
                "built_at": time.time(),
                "stock_dict": stock_dict,
                "name_to_code": name_to_code,
            }
            with open(self._dict_cache_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False)
        except Exception as e:
            log_warning(f"å†™å…¥è‚¡ç¥¨å­—å…¸ç¼“å­˜å¤±è´¥: {e}")

    def _fetch_stock_dictionary_from_akshare(self) -> Tuple[Dict[str, str], Dict[str, str]]:
        """ä» AkShare è·å–å…¨é‡è‚¡ç¥¨å­—å…¸ï¼Œå¹¶è¾“å‡ºæ„å»ºè¿›åº¦"""
        self.log("ä»AkShareè·å–æ¸…å•")
        try:
            holder: Dict[str, Any] = {"df": None, "err": None}

            def _fetch():
                try:
                    holder["df"] = ak.stock_zh_a_spot_em()
                except Exception as e:
                    holder["err"] = e

            t = threading.Thread(target=_fetch, daemon=True)
            t.start()
            waited = 0
            while t.is_alive():
                t.join(timeout=5)
                waited += 5
                if t.is_alive():
                    self.log(f"ä»AkShareè·å–æ¸…å•ä¸­...å·²ç­‰å¾… {waited} ç§’")

            if holder["err"] is not None:
                raise holder["err"]

            df = holder["df"]
            total_rows = len(df)
            self.log(f"å­—å…¸å¤„ç†è¿›åº¦ 0/{total_rows}")

            stock_dict: Dict[str, str] = {}
            name_to_code: Dict[str, str] = {}

            for idx, (_, row) in enumerate(df.iterrows(), 1):
                code = str(row['ä»£ç '])
                name = str(row['åç§°']).strip()

                if not name or len(name) < 2:
                    continue
                if name in EXCLUDE_WORDS:
                    continue

                if code.startswith('6'):
                    full_code = f"{code}.SH"
                elif code.startswith(('0', '3')):
                    full_code = f"{code}.SZ"
                elif code.startswith(('4', '8')):
                    full_code = f"{code}.BJ"
                else:
                    full_code = code

                stock_dict[full_code] = name
                name_to_code[name] = full_code

                if idx % 1000 == 0 or idx == total_rows:
                    self.log(f"å­—å…¸å¤„ç†è¿›åº¦ {idx}/{total_rows}")

            return stock_dict, name_to_code
        except Exception as e:
            log_error(f"æ„å»ºè‚¡ç¥¨å­—å…¸å¤±è´¥: {e}")
            raise

    def extract_stocks(self, text: str) -> List[Dict[str, Any]]:
        """
        ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰è‚¡ç¥¨æåŠ
        è¿”å›: [{code, name, position, context}]
        """
        if self._automaton is None:
            self._build_stock_dictionary()

        if not text or not self._automaton:
            return []

        # æ¸…ç†æ–‡æœ¬ä¸­çš„ XML/HTML æ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', text)

        results = []
        seen_codes = set()

        for end_pos, (code, name) in self._automaton.iter(clean_text):
            if code in seen_codes:
                continue

            start_pos = end_pos - len(name) + 1

            # æå–ä¸Šä¸‹æ–‡ç‰‡æ®µ (å‰å50å­—ç¬¦)
            ctx_start = max(0, start_pos - 50)
            ctx_end = min(len(clean_text), end_pos + 51)
            context = clean_text[ctx_start:ctx_end].strip()

            results.append({
                'code': code,
                'name': name,
                'position': start_pos,
                'context': context
            })
            seen_codes.add(code)

        return results

    # ========== è¡Œæƒ…æ•°æ® ==========

    def fetch_price_range(self, stock_code: str, start_date: str, end_date: str) -> List[Dict]:
        """
        è·å–è‚¡ç¥¨åŒºé—´è¡Œæƒ…ï¼ŒæŒ‰å¤©çº§ç²’åº¦ç¼“å­˜
        - T-2 ä¹‹å‰å†å²æ•°æ®è§†ä¸ºç¨³å®šï¼Œç›´æ¥å¤ç”¨ç¼“å­˜
        - T-1 / T æ•°æ®æ¯æ¬¡åˆ·æ–°ï¼ˆç›˜åä¿®æ­£ + ç›˜ä¸­å˜åŠ¨ï¼‰
        - åªå¯¹ç¼ºå¤±æ—¥æœŸè°ƒç”¨ AkShare
        """
        pure_code = stock_code.split('.')[0]
        today_dt = datetime.now().date()
        refresh_from = (today_dt - timedelta(days=1)).strftime('%Y-%m-%d')  # T-1 èµ·åˆ·æ–°

        # é˜¶æ®µ1ï¼šæŸ¥è¯¢ç¼“å­˜ï¼ˆçŸ­è¿æ¥ï¼Œå¿«é€Ÿé‡Šæ”¾ï¼‰
        conn = self._get_conn()
        cursor = conn.cursor()

        cursor.execute('''
            SELECT trade_date, open, close, high, low, change_pct, volume
            FROM stock_price_cache
            WHERE stock_code = ? AND trade_date >= ? AND trade_date <= ?
            ORDER BY trade_date
        ''', (stock_code, start_date, end_date))
        cached_rows = cursor.fetchall()
        cached_dates = {r[0] for r in cached_rows}

        # åˆ é™¤ T-1 / T ç¼“å­˜ï¼ˆéœ€å®æ—¶åˆ·æ–°ï¼‰ï¼ŒT-2 ä¹‹å‰ä¿ç•™
        volatile_dates = [d for d in cached_dates if d >= refresh_from]
        if volatile_dates:
            cursor.execute('''
                DELETE FROM stock_price_cache
                WHERE stock_code = ? AND trade_date >= ? AND trade_date <= ?
            ''', (stock_code, refresh_from, end_date))
            conn.commit()
            cached_dates = {d for d in cached_dates if d < refresh_from}
            cached_rows = [r for r in cached_rows if r[0] < refresh_from]

        conn.close()  # â˜… é‡Šæ”¾è¿æ¥åå†åšç½‘ç»œè¯·æ±‚

        # æ„å»ºç¼“å­˜ç»“æœ
        results_map = {}
        for r in cached_rows:
            results_map[r[0]] = {
                'trade_date': r[0], 'open': r[1], 'close': r[2],
                'high': r[3], 'low': r[4], 'change_pct': r[5], 'volume': r[6]
            }

        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä» AkShare æ‹‰å–
        need_fetch = len(cached_dates) == 0
        if not need_fetch:
            # åŒºé—´è§¦åŠ T-1 / T æ—¶å¼ºåˆ¶åˆ·æ–°
            if end_date >= refresh_from:
                need_fetch = True
            else:
                # å¯¹ç¨³å®šå†å²åŒºé—´åšè½»é‡å®Œæ•´æ€§æ ¡éªŒï¼šä»…å½“é¦–æ¡æ•°æ®æ˜æ˜¾æ™šäº start_date æ—¶å›è¡¥
                try:
                    first_cached = datetime.strptime(cached_rows[0][0], '%Y-%m-%d').date() if cached_rows else None
                    start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
                    if first_cached and (first_cached - start_dt).days > 3:
                        need_fetch = True
                except Exception:
                    need_fetch = True

        # é˜¶æ®µ2ï¼šç½‘ç»œè¯·æ±‚ï¼ˆä¸æŒæœ‰æ•°æ®åº“è¿æ¥ï¼‰
        new_records = []
        if need_fetch:
            try:
                df = ak.stock_zh_a_hist(
                    symbol=pure_code,
                    period="daily",
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', ''),
                    adjust="qfq"
                )

                if df is not None and not df.empty:
                    for _, row in df.iterrows():
                        trade_date = str(row['æ—¥æœŸ'])[:10]
                        # å†å²ç¨³å®šæ—¥å·²ç¼“å­˜åˆ™è·³è¿‡ï¼›T-1/T ç”±ä¸Šæ–¹åˆ é™¤åä¼šé‡å†™
                        if trade_date in cached_dates and trade_date < refresh_from:
                            continue

                        record = {
                            'trade_date': trade_date,
                            'open': float(row['å¼€ç›˜']),
                            'close': float(row['æ”¶ç›˜']),
                            'high': float(row['æœ€é«˜']),
                            'low': float(row['æœ€ä½']),
                            'change_pct': float(row['æ¶¨è·Œå¹…']),
                            'volume': float(row['æˆäº¤é‡']),
                        }
                        results_map[trade_date] = record
                        new_records.append((stock_code, record))

            except Exception as e:
                log_warning(f"è·å– {stock_code} è¡Œæƒ…å¤±è´¥: {e}")

        # é˜¶æ®µ3ï¼šæ‰¹é‡å†™å…¥ç¼“å­˜ï¼ˆé‡æ–°æ‰“å¼€çŸ­è¿æ¥ï¼‰
        if new_records:
            conn = self._get_conn()
            cursor = conn.cursor()
            for sc, rec in new_records:
                cursor.execute('''
                    INSERT OR REPLACE INTO stock_price_cache
                    (stock_code, trade_date, open, close, high, low, change_pct, volume)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    sc, rec['trade_date'],
                    rec['open'], rec['close'], rec['high'], rec['low'],
                    rec['change_pct'], rec['volume']
                ))
            conn.commit()
            conn.close()

        # æŒ‰æ—¥æœŸæ’åºè¿”å›
        return [results_map[d] for d in sorted(results_map.keys())]

    def _fetch_index_price(self, start_date: str, end_date: str) -> Dict[str, float]:
        """è·å–æ²ªæ·±300æŒ‡æ•°è¡Œæƒ…ï¼ˆç”¨äºè®¡ç®—è¶…é¢æ”¶ç›Šï¼‰"""
        index_code = "000300.SH"  # æ²ªæ·±300
        today_dt = datetime.now().date()
        refresh_from = (today_dt - timedelta(days=1)).strftime('%Y-%m-%d')  # T-1 èµ·åˆ·æ–°

        # é˜¶æ®µ1ï¼šæŸ¥ç¼“å­˜ï¼ˆçŸ­è¿æ¥ï¼‰
        conn = self._get_conn()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT trade_date, close FROM stock_price_cache
            WHERE stock_code = ? AND trade_date >= ? AND trade_date <= ?
            ORDER BY trade_date
        ''', (index_code, start_date, end_date))
        cached = cursor.fetchall()
        cached_map = {r[0]: r[1] for r in cached}

        # åˆ é™¤ T-1 / T æŒ‡æ•°ç¼“å­˜ï¼Œé¿å…å¤æƒ/æ”¶ç›˜åä¿®æ­£ä¸ä¸€è‡´
        if any(d >= refresh_from for d in cached_map.keys()):
            cursor.execute('''
                DELETE FROM stock_price_cache
                WHERE stock_code = ? AND trade_date >= ? AND trade_date <= ?
            ''', (index_code, refresh_from, end_date))
            conn.commit()
            cached_map = {d: v for d, v in cached_map.items() if d < refresh_from}

        conn.close()  # â˜… é‡Šæ”¾è¿æ¥

        need_fetch = len(cached_map) == 0 or end_date >= refresh_from
        if not need_fetch:
            try:
                first_cached = min(datetime.strptime(d, '%Y-%m-%d').date() for d in cached_map.keys())
                start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
                if (first_cached - start_dt).days > 3:
                    need_fetch = True
            except Exception:
                need_fetch = True

        if not need_fetch:
            return cached_map

        # é˜¶æ®µ2ï¼šç½‘ç»œè¯·æ±‚ï¼ˆä¸æŒæœ‰æ•°æ®åº“è¿æ¥ï¼‰
        try:
            df = ak.stock_zh_index_daily(symbol="sh000300")
            if df is None or df.empty:
                return cached_map

            result = {}
            cache_rows = []
            for _, row in df.iterrows():
                trade_date = str(row['date'])[:10]
                if trade_date < start_date or trade_date > end_date:
                    continue
                # ç¨³å®šå†å²æ—¥å‘½ä¸­ç¼“å­˜ç›´æ¥å¤ç”¨ï¼Œé¿å…é‡å¤å†™
                if trade_date in cached_map and trade_date < refresh_from:
                    result[trade_date] = cached_map[trade_date]
                    continue
                close_val = float(row['close'])
                result[trade_date] = close_val
                cache_rows.append((
                    index_code, trade_date, float(row['open']), close_val,
                    float(row['high']), float(row['low']), 0, float(row['volume'])
                ))

            # é˜¶æ®µ3ï¼šæ‰¹é‡å†™å…¥ç¼“å­˜
            if cache_rows:
                conn = self._get_conn()
                cursor = conn.cursor()
                cursor.executemany('''
                    INSERT OR REPLACE INTO stock_price_cache
                    (stock_code, trade_date, open, close, high, low, change_pct, volume)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', cache_rows)
                conn.commit()
                conn.close()

            # åˆå¹¶ä¿ç•™çš„ç¨³å®šå†å²ç¼“å­˜
            result.update({d: v for d, v in cached_map.items() if d not in result})
            return result

        except Exception as e:
            log_warning(f"è·å–æ²ªæ·±300æŒ‡æ•°å¤±è´¥: {e}")
            return cached_map

    # ========== äº‹ä»¶è¡¨ç°è®¡ç®— ==========

    def _calc_mention_performance(self, mention_id: int, stock_code: str, mention_date: str):
        """
        è®¡ç®—ä¸€æ¬¡æåŠäº‹ä»¶çš„åç»­è¡¨ç°
        T+1, T+3, T+5, T+10, T+20, T+60, T+120, T+250 æ”¶ç›Šç‡ & è¶…é¢æ”¶ç›Šç‡
        æ”¯æŒæ¸è¿›å¼å†»ç»“ï¼šå·²å†»ç»“çš„å­—æ®µä¸å†é‡æ–°æ‹‰å–è¡Œæƒ…
        """
        ALL_PERIODS = [1, 3, 5, 10, 20, 60, 120, 250]

        # æ£€æŸ¥å½“å‰ freeze_levelï¼Œå†³å®šéœ€è¦è®¡ç®—å“ªäº›å‘¨æœŸ
        conn = self._get_conn()
        cursor = conn.cursor()
        cursor.execute('SELECT freeze_level FROM mention_performance WHERE mention_id = ?', (mention_id,))
        row = cursor.fetchone()
        current_freeze = row[0] if row and row[0] else 0
        conn.close()

        # æ ¹æ® freeze_level ç¡®å®šéœ€è¦è®¡ç®—çš„å‘¨æœŸ
        # 0: æ‰€æœ‰éƒ½éœ€è¦, 1: T+60/120/250, 2: T+120/250, 3: å…¨éƒ¨å†»ç»“
        if current_freeze >= 3:
            return  # å…¨éƒ¨å†»ç»“ï¼Œè·³è¿‡

        freeze_thresholds = {1: 20, 2: 60, 3: 120}
        periods_to_calc = [d for d in ALL_PERIODS if d > freeze_thresholds.get(current_freeze, 0)]
        if current_freeze == 0:
            periods_to_calc = ALL_PERIODS

        # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼šæåŠæ—¥å‰5å¤© ~ åè¶³å¤Ÿå¤©æ•°
        dt = datetime.strptime(mention_date, '%Y-%m-%d')
        max_period = max(periods_to_calc)
        start = (dt - timedelta(days=10)).strftime('%Y-%m-%d')
        end = (dt + timedelta(days=int(max_period * 1.5) + 10)).strftime('%Y-%m-%d')

        prices = self.fetch_price_range(stock_code, start, end)
        if not prices:
            return

        # æ‰¾åˆ°æåŠæ—¥æˆ–ä¹‹åæœ€è¿‘çš„äº¤æ˜“æ—¥ä½œä¸ºåŸºå‡†
        base_price = None
        base_idx = -1
        for i, p in enumerate(prices):
            if p['trade_date'] >= mention_date:
                base_price = p['close']
                base_idx = i
                break

        if base_price is None or base_price == 0:
            return

        # è·å–æ²ªæ·±300 å¯¹åº”æœŸé—´æ•°æ®
        index_prices = self._fetch_index_price(start, end)

        # æ‰¾åˆ°æ²ªæ·±300 åŸºå‡†ä»·
        index_base = None
        for p in prices:
            if p['trade_date'] >= mention_date and p['trade_date'] in index_prices:
                index_base = index_prices[p['trade_date']]
                break

        # è®¡ç®—å„æœŸé™æ”¶ç›Šç‡
        returns = {}
        excess_returns = {}
        for days in periods_to_calc:
            target_idx = base_idx + days
            if target_idx < len(prices):
                target_price = prices[target_idx]['close']
                ret = (target_price - base_price) / base_price * 100
                returns[days] = round(ret, 2)

                # è¶…é¢æ”¶ç›Š
                target_date = prices[target_idx]['trade_date']
                if index_base and target_date in index_prices and index_base > 0:
                    index_ret = (index_prices[target_date] - index_base) / index_base * 100
                    excess_returns[days] = round(ret - index_ret, 2)
                else:
                    excess_returns[days] = None
            else:
                returns[days] = None
                excess_returns[days] = None

        # è®¡ç®—æœŸé—´æœ€å¤§æ¶¨å¹…å’Œæœ€å¤§å›æ’¤ï¼ˆä½¿ç”¨æœ€é•¿å¯ç”¨å‘¨æœŸï¼Œæœ€å¤š250ä¸ªäº¤æ˜“æ—¥ï¼‰
        max_return = 0
        max_drawdown = 0
        max_track = min(base_idx + max_period + 1, len(prices))
        for i in range(base_idx + 1, max_track):
            ret = (prices[i]['high'] - base_price) / base_price * 100
            max_return = max(max_return, ret)
            dd = (prices[i]['low'] - base_price) / base_price * 100
            max_drawdown = min(max_drawdown, dd)

        # ç¡®å®šæ–°çš„ freeze_level
        today = datetime.now().strftime('%Y-%m-%d')
        trading_days_elapsed = base_idx  # ç²—ç•¥ä¼°è®¡
        # æ›´å‡†ç¡®ï¼šè®¡ç®—æåŠæ—¥åˆ°ä»Šå¤©ä¹‹é—´çš„äº¤æ˜“æ—¥æ•°
        today_idx = -1
        for i, p in enumerate(prices):
            if p['trade_date'] >= today:
                today_idx = i
                break
        if today_idx < 0:
            today_idx = len(prices)
        trading_days_elapsed = today_idx - base_idx

        new_freeze = current_freeze
        if trading_days_elapsed > 260:
            new_freeze = 3
        elif trading_days_elapsed > 130:
            new_freeze = max(current_freeze, 2)
        elif trading_days_elapsed > 70:
            new_freeze = max(current_freeze, 1)
        elif trading_days_elapsed > 25:
            new_freeze = max(current_freeze, 1)

        # å†™å…¥æ•°æ®åº“ï¼ˆä½¿ç”¨ UPSERT æ¨¡å¼ï¼‰
        conn = self._get_conn()
        cursor = conn.cursor()

        if row:
            # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•ï¼ˆåªæ›´æ–°æœªå†»ç»“å­—æ®µï¼‰
            updates = []
            params = []
            for days in periods_to_calc:
                if returns.get(days) is not None:
                    updates.append(f'return_{days}d = ?')
                    params.append(returns[days])
                    updates.append(f'excess_return_{days}d = ?')
                    params.append(excess_returns.get(days))
            updates.append('max_return = ?')
            params.append(round(max_return, 2))
            updates.append('max_drawdown = ?')
            params.append(round(max_drawdown, 2))
            updates.append('freeze_level = ?')
            params.append(new_freeze)
            params.append(mention_id)

            if updates:
                cursor.execute(f'''
                    UPDATE mention_performance SET {', '.join(updates)}
                    WHERE mention_id = ?
                ''', params)
        else:
            # æ–°æ’å…¥
            cursor.execute('''
                INSERT OR REPLACE INTO mention_performance
                (mention_id, stock_code, mention_date, price_at_mention,
                 return_1d, return_3d, return_5d, return_10d, return_20d,
                 return_60d, return_120d, return_250d,
                 excess_return_1d, excess_return_3d, excess_return_5d,
                 excess_return_10d, excess_return_20d,
                 excess_return_60d, excess_return_120d, excess_return_250d,
                 max_return, max_drawdown, freeze_level)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                mention_id, stock_code, mention_date, round(base_price, 2),
                returns.get(1), returns.get(3), returns.get(5),
                returns.get(10), returns.get(20),
                returns.get(60), returns.get(120), returns.get(250),
                excess_returns.get(1), excess_returns.get(3), excess_returns.get(5),
                excess_returns.get(10), excess_returns.get(20),
                excess_returns.get(60), excess_returns.get(120), excess_returns.get(250),
                round(max_return, 2), round(max_drawdown, 2), new_freeze
            ))
        conn.commit()
        conn.close()

    # ========== å…¨é‡æ‰«æ ==========

    def scan_group(self, group_id: str = None, force: bool = False) -> Dict[str, Any]:
        """
        æ‰«æç¾¤ç»„å…¨éƒ¨å¸–å­ï¼Œæå–è‚¡ç¥¨æåŠå¹¶è®¡ç®—åç»­è¡¨ç°

        Args:
            group_id: ç¾¤ç»„IDï¼ˆé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„group_idï¼‰
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°æ‰«æï¼ˆæ¸…é™¤æ—§æ•°æ®ï¼‰

        Returns:
            æ‰«æç»“æœç»Ÿè®¡
        """
        gid = group_id or self.group_id
        self._build_stock_dictionary()

        conn = self._get_conn()
        cursor = conn.cursor()

        if force:
            cursor.execute('DELETE FROM mention_performance')
            cursor.execute('DELETE FROM stock_mentions')
            conn.commit()
            self.log("ğŸ—‘ï¸ å·²æ¸…é™¤æ—§çš„è‚¡ç¥¨åˆ†ææ•°æ®")

        # è·å–å¾…å¤„ç†å¸–å­ï¼ˆé force æ¨¡å¼ä¸‹ä»…å¤„ç†æœªæå–è¿‡çš„ topicï¼‰
        cursor.execute('''
            SELECT t.topic_id, tk.text, t.create_time
            FROM topics t
            JOIN talks tk ON t.topic_id = tk.topic_id
            WHERE tk.text IS NOT NULL AND tk.text != ''
              AND (
                ? = 1
                OR NOT EXISTS (
                    SELECT 1 FROM stock_mentions sm WHERE sm.topic_id = t.topic_id
                )
              )
            ORDER BY t.create_time
        ''', (1 if force else 0,))
        topics = cursor.fetchall()

        total_topics = len(topics)
        total_mentions = 0
        stocks_found = set()

        self.log(f"ğŸ” å¼€å§‹æ‰«æ {total_topics} æ¡å¸–å­...")

        for i, (topic_id, text, create_time) in enumerate(topics):
            stocks = self.extract_stocks(text)
            if not stocks:
                continue

            # è§£ææ—¥æœŸ
            mention_date = create_time[:10] if create_time else ''
            if not mention_date:
                continue

            for stock in stocks:
                cursor.execute('''
                    INSERT INTO stock_mentions
                    (topic_id, stock_code, stock_name, mention_date, mention_time, context_snippet)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    topic_id, stock['code'], stock['name'],
                    mention_date, create_time or '', stock['context']
                ))
                total_mentions += 1
                stocks_found.add(stock['code'])

            if (i + 1) % 20 == 0:
                conn.commit()
                self.log(f"ğŸ“Š å·²æ‰«æ {i+1}/{total_topics} æ¡å¸–å­ï¼Œç´¯è®¡æå– {total_mentions} æ¬¡è‚¡ç¥¨æåŠ")

        conn.commit()
        self.log(f"âœ… æ‰«æå®Œæˆï¼š{total_topics} æ¡å¸–å­ï¼Œæå– {total_mentions} æ¬¡æåŠï¼Œæ¶‰åŠ {len(stocks_found)} åªè‚¡ç¥¨")

        # é˜¶æ®µäºŒï¼šè®¡ç®—æ¯æ¬¡æåŠçš„åç»­è¡¨ç°
        self.log("ğŸ“ˆ å¼€å§‹è®¡ç®—æåŠåè¡¨ç°...")
        cursor.execute('''
            SELECT sm.id, sm.stock_code, sm.mention_date
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE mp.mention_id IS NULL
        ''')
        pending = cursor.fetchall()
        conn.close()

        total_pending = len(pending)
        self.log(f"ğŸ“Œ å¾…è®¡ç®—æåŠè¡¨ç°: {total_pending} æ¡")
        if total_pending == 0:
            self.log("âœ… æ— éœ€å¢é‡è®¡ç®—ï¼Œä»»åŠ¡ç»“æŸ")
            return {
                'topics_scanned': total_topics,
                'mentions_extracted': total_mentions,
                'unique_stocks': len(stocks_found),
                'performance_calculated': 0
            }

        for j, (mention_id, stock_code, mention_date) in enumerate(pending):
            try:
                self._calc_mention_performance(mention_id, stock_code, mention_date)
            except Exception as e:
                log_warning(f"è®¡ç®— {stock_code} è¡¨ç°å¤±è´¥: {e}")

            if (j + 1) % 20 == 0:
                self.log(f"ğŸ“ˆ å·²è®¡ç®— {j+1}/{total_pending} æ¡æåŠçš„åç»­è¡¨ç°")

            # æ§åˆ¶ API è¯·æ±‚é¢‘ç‡
            time.sleep(0.3)

        self.log(f"âœ… å…¨éƒ¨å®Œæˆï¼å…±å¤„ç† {total_pending} æ¡æåŠè¡¨ç°è®¡ç®—")

        return {
            'topics_scanned': total_topics,
            'mentions_extracted': total_mentions,
            'unique_stocks': len(stocks_found),
            'performance_calculated': total_pending
        }

    # ========== åˆ†ç¦»å¼æ–¹æ³•ï¼ˆè°ƒåº¦å™¨ä¸“ç”¨ï¼‰==========

    def extract_only(self, group_id: str = None) -> Dict[str, Any]:
        """
        ä»…æå–è‚¡ç¥¨åç§°ï¼Œä¸è®¡ç®—æ”¶ç›Šè¡¨ç°ï¼ˆçº¯æœ¬åœ°æ“ä½œï¼Œç§’çº§å®Œæˆï¼‰
        ä¾›è°ƒåº¦å™¨é«˜é¢‘å¾ªç¯ä½¿ç”¨
        """
        gid = group_id or self.group_id
        self._build_stock_dictionary()

        conn = self._get_conn()
        cursor = conn.cursor()

        # è·å–å·²æ‰«æçš„ topic_id é›†åˆ
        cursor.execute('SELECT DISTINCT topic_id FROM stock_mentions')
        scanned_ids = {r[0] for r in cursor.fetchall()}

        # è·å–å…¨éƒ¨å¸–å­
        cursor.execute('''
            SELECT t.topic_id, tk.text, t.create_time
            FROM topics t
            JOIN talks tk ON t.topic_id = tk.topic_id
            WHERE tk.text IS NOT NULL AND tk.text != ''
            ORDER BY t.create_time
        ''')
        topics = cursor.fetchall()

        total_topics = len(topics)
        total_mentions = 0
        stocks_found = set()
        new_topics = 0

        for i, (topic_id, text, create_time) in enumerate(topics):
            if topic_id in scanned_ids:
                continue

            new_topics += 1
            stocks = self.extract_stocks(text)
            if not stocks:
                continue

            mention_date = create_time[:10] if create_time else ''
            if not mention_date:
                continue

            for stock in stocks:
                cursor.execute('''
                    INSERT INTO stock_mentions
                    (topic_id, stock_code, stock_name, mention_date, mention_time, context_snippet)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    topic_id, stock['code'], stock['name'],
                    mention_date, create_time or '', stock['context']
                ))
                total_mentions += 1
                stocks_found.add(stock['code'])

        conn.commit()
        conn.close()

        if new_topics > 0:
            self.log(f"ğŸ“ æå–å®Œæˆï¼š{new_topics} æ¡æ–°å¸–å­ï¼Œ{total_mentions} æ¬¡æåŠï¼Œ{len(stocks_found)} åªè‚¡ç¥¨")

        return {
            'new_topics': new_topics,
            'mentions_extracted': total_mentions,
            'unique_stocks': len(stocks_found)
        }

    def calc_pending_performance(self, calc_window_days: int = 365, progress_callback=None) -> Dict[str, Any]:
        """
        è®¡ç®—å¾…å¤„ç†çš„æ”¶ç›Šè¡¨ç°ï¼ˆéœ€è¦ç½‘ç»œï¼Œä¾›å®šæ—¶ä»»åŠ¡ä½¿ç”¨ï¼‰
        åŒ…æ‹¬ï¼šæœªè®¡ç®—çš„æ–°æåŠ + æœªå®Œå…¨å†»ç»“çš„æ—§æåŠ

        Args:
            calc_window_days: æ´»è·ƒè®¡ç®—çª—å£å¤©æ•°ï¼ˆé»˜è®¤365å¤©ï¼Œè¦†ç›–T+250ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œfunc(current, total, msg)
        """
        self._build_stock_dictionary()
        since_date = (datetime.now() - timedelta(days=calc_window_days)).strftime('%Y-%m-%d')

        conn = self._get_conn()
        cursor = conn.cursor()

        # æŸ¥è¯¢1ï¼šæœªè®¡ç®—æ”¶ç›Šçš„æ–°æåŠ
        cursor.execute('''
            SELECT sm.id, sm.stock_code, sm.mention_date
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE mp.mention_id IS NULL
            AND sm.mention_date >= ?
        ''', (since_date,))
        new_pending = cursor.fetchall()

        # æŸ¥è¯¢2ï¼šå·²æœ‰è®°å½•ä½†æœªå®Œå…¨å†»ç»“çš„æåŠï¼ˆéœ€è¦æ›´æ–°é•¿å‘¨æœŸæ•°æ®ï¼‰
        cursor.execute('''
            SELECT sm.id, sm.stock_code, sm.mention_date
            FROM stock_mentions sm
            JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE (mp.freeze_level IS NULL OR mp.freeze_level < 3)
            AND sm.mention_date >= ?
        ''', (since_date,))
        update_pending = cursor.fetchall()

        conn.close()

        total_new = len(new_pending)
        total_update = len(update_pending)
        all_pending = new_pending + update_pending

        self.log(f"ğŸ“ˆ æ”¶ç›Šè®¡ç®—ï¼š{total_new} æ¡æ–°æåŠ + {total_update} æ¡å¾…æ›´æ–°")

        processed = 0
        errors = 0
        total = len(all_pending)
        
        for i, (mention_id, stock_code, mention_date) in enumerate(all_pending, 1):
            status_msg = ""
            try:
                self._calc_mention_performance(mention_id, stock_code, mention_date)
                processed += 1
                status_msg = f"å·²ä¿å­˜ {stock_code} ({mention_date})"
            except Exception as e:
                log_warning(f"è®¡ç®— {stock_code} è¡¨ç°å¤±è´¥: {e}")
                errors += 1
                status_msg = f"å¤±è´¥ {stock_code}: {e}"

            if progress_callback:
                # The callback handles the 10s interval logic
                progress_callback(i, total, status_msg)
            
            # Internal log - keep it periodic
            if i % 20 == 0 or i == total:
                self.log(f"ğŸ“ˆ æ”¶ç›Šè®¡ç®—ä¸­: {i}/{total} (é”™è¯¯: {errors})")

            time.sleep(0.3)

        self.log(f"âœ… æ”¶ç›Šè®¡ç®—å®Œæˆï¼šå¤„ç† {processed} æ¡ï¼Œå¤±è´¥ {errors} æ¡")

        return {
            'new_calculated': total_new,
            'updated': total_update,
            'processed': processed,
            'errors': errors
        }

    # ========== æŸ¥è¯¢æ¥å£ ==========

    def get_topic_mentions(self, page: int = 1, per_page: int = 20) -> Dict[str, Any]:
        """
        è·å–æŒ‰è¯é¢˜åˆ†ç»„çš„è‚¡ç¥¨æåŠåˆ—è¡¨
        """
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        # 1. åˆ†é¡µè·å–å«æœ‰è‚¡ç¥¨æåŠçš„ topic_id (æŒ‰æœ€è¿‘æåŠæ—¶é—´æ’åº)
        offset = (page - 1) * per_page
        cursor.execute('''
            SELECT topic_id, MAX(mention_time) as latest_mention
            FROM stock_mentions
            GROUP BY topic_id
            ORDER BY latest_mention DESC
            LIMIT ? OFFSET ?
        ''', (per_page, offset))
        
        rows = cursor.fetchall()
        topic_ids = [row[0] for row in rows]

        if not topic_ids:
            conn.close()
            # Try to get total count anyway to be correct
            conn2 = self._get_conn()
            cursor2 = conn2.cursor()
            cursor2.execute('SELECT COUNT(DISTINCT topic_id) FROM stock_mentions')
            total = cursor2.fetchone()[0]
            conn2.close()
            
            return {
                'total': total,
                'page': page,
                'per_page': per_page,
                'items': []
            }

        # 2. è·å–æ€»æ•°
        cursor.execute('SELECT COUNT(DISTINCT topic_id) FROM stock_mentions')
        total = cursor.fetchone()[0]

        # 3. æ‰¹é‡è·å–è¯é¢˜å†…å®¹
        placeholders = ','.join('?' * len(topic_ids))
        cursor.execute(f'''
            SELECT t.topic_id, t.create_time, tk.text
            FROM topics t
            JOIN talks tk ON t.topic_id = tk.topic_id
            WHERE t.topic_id IN ({placeholders})
        ''', topic_ids)
        topics_map = {row['topic_id']: dict(row) for row in cursor.fetchall()}

        # 4. æ‰¹é‡è·å–è¿™äº›è¯é¢˜ä¸‹çš„è‚¡ç¥¨æåŠå’Œè¡¨ç°
        cursor.execute(f'''
            SELECT sm.topic_id, sm.stock_code, sm.stock_name,
                   mp.return_1d, mp.return_3d, mp.return_5d, mp.return_10d, mp.return_20d,
                   mp.max_return
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE sm.topic_id IN ({placeholders})
        ''', topic_ids)
        
        mentions_by_topic = {}
        for row in cursor.fetchall():
            tid = row['topic_id']
            if tid not in mentions_by_topic:
                mentions_by_topic[tid] = []
            
            # Convert row to dict and handle None values for cleaner frontend JSON
            item = dict(row)
            mentions_by_topic[tid].append(item)

        # 5. ç»„è£…ç»“æœ
        items = []
        for tid in topic_ids:
            # Note: It's possible a topic is in stock_mentions but missing from topics/talks if data inconsistency exists
            # We skip if topic content not found
            if tid not in topics_map:
                continue
                
            topic = topics_map[tid]
            topic['mentions'] = mentions_by_topic.get(tid, [])
            items.append(topic)

        conn.close()
        return {
            'total': total,
            'page': page,
            'per_page': per_page,
            'items': items
        }

    def get_mentions(self, stock_code: str = None, page: int = 1, per_page: int = 50,
                     sort_by: str = 'mention_date', order: str = 'desc') -> Dict[str, Any]:
        """
        è·å–è‚¡ç¥¨æåŠåˆ—è¡¨
        sort_by: mention_date / return_5d / excess_return_5d / max_return
        """
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        where_clause = "WHERE 1=1"
        params = []
        # æ¨¡ç³Šæœç´¢é€»è¾‘
        if stock_code:
            # é¢„å¤„ç†ï¼šå¦‚æœæ˜¯çº¯æ•°å­—+åç¼€ï¼ˆå¦‚ 300308.SZï¼‰ï¼Œå»æ‰åç¼€
            clean_code = stock_code.strip()
            if '.' in clean_code:
                parts = clean_code.split('.')
                # å¦‚æœå‰ç¼€æ˜¯æ•°å­—ï¼Œä¸”åç¼€æ˜¯SZ/SH/BJç­‰ï¼Œåˆ™åªå–å‰ç¼€
                if parts[0].isdigit() and parts[1].upper() in ['SZ', 'SH', 'BJ', 'SS']:
                    clean_code = parts[0]
            
            # æ”¯æŒ ä»£ç å…è®¸å‰ç¼€åŒ¹é…/åŒ…å«åŒ¹é…ï¼Œåç§°å…è®¸æ¨¡ç³ŠåŒ¹é…
            # ç”¨æˆ·éœ€æ±‚ï¼š300308.SZ ç­‰åŒäº 300308 (å‰ç¼€åŒ¹é…) -> å…¶å®æ˜¯æ¸…æ´—åçš„ç²¾ç¡®æˆ–å‰ç¼€
            # è¿™é‡Œä½¿ç”¨ OR é€»è¾‘ï¼šä»£ç åŒ…å« OR åç§°åŒ…å«
            where_clause += " AND (sm.stock_code LIKE ? OR sm.stock_name LIKE ?)"
            search_term = f"%{clean_code}%"
            params.append(search_term)
            params.append(search_term)

        # å…è®¸çš„æ’åºå­—æ®µ
        valid_sorts = {
            'mention_date': 'sm.mention_date',
            'return_1d': 'mp.return_1d', 'return_3d': 'mp.return_3d',
            'return_5d': 'mp.return_5d', 'return_10d': 'mp.return_10d',
            'return_20d': 'mp.return_20d',
            'excess_return_5d': 'mp.excess_return_5d',
            'excess_return_10d': 'mp.excess_return_10d',
            'max_return': 'mp.max_return',
        }
        sort_col = valid_sorts.get(sort_by, 'sm.mention_date')
        order_dir = 'DESC' if order.lower() == 'desc' else 'ASC'

        # æ€»æ•°
        cursor.execute(f'''
            SELECT COUNT(*) FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            {where_clause}
        ''', params)
        total = cursor.fetchone()[0]

        # åˆ†é¡µæŸ¥è¯¢
        offset = (page - 1) * per_page
        cursor.execute(f'''
            SELECT sm.id, sm.topic_id, sm.stock_code, sm.stock_name,
                   sm.mention_date, sm.mention_time, sm.context_snippet, sm.sentiment,
                   mp.price_at_mention,
                   mp.return_1d, mp.return_3d, mp.return_5d, mp.return_10d, mp.return_20d,
                   mp.excess_return_1d, mp.excess_return_3d, mp.excess_return_5d,
                   mp.excess_return_10d, mp.excess_return_20d,
                   mp.max_return, mp.max_drawdown
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            {where_clause}
            ORDER BY {sort_col} {order_dir}
            LIMIT ? OFFSET ?
        ''', params + [per_page, offset])

        items = [dict(row) for row in cursor.fetchall()]
        conn.close()

        return {
            'total': total,
            'page': page,
            'per_page': per_page,
            'items': items
        }

    def get_stock_events(self, stock_code: str) -> Dict[str, Any]:
        """è·å–æŸåªè‚¡ç¥¨çš„å…¨éƒ¨æåŠäº‹ä»¶ + æ¯æ¬¡è¡¨ç°"""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute('''
            SELECT sm.context_snippet as context, sm.*, mp.price_at_mention,
                   mp.return_1d, mp.return_3d, mp.return_5d, mp.return_10d, mp.return_20d,
                   mp.excess_return_5d, mp.excess_return_10d,
                   mp.max_return, mp.max_drawdown
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE sm.stock_code = ?
            ORDER BY sm.mention_time DESC
        ''', (stock_code,))

        events = [dict(row) for row in cursor.fetchall()]

        # ç»Ÿè®¡
        valid_returns = [e['return_5d'] for e in events if e.get('return_5d') is not None]
        win_count = sum(1 for r in valid_returns if r > 0)

        stock_name = events[0]['stock_name'] if events else ''

        conn.close()

        return {
            'stock_code': stock_code,
            'stock_name': stock_name,
            'total_mentions': len(events),
            'win_rate_5d': round(win_count / len(valid_returns) * 100, 1) if valid_returns else None,
            'avg_return_5d': round(sum(valid_returns) / len(valid_returns), 2) if valid_returns else None,
            'events': events
        }

    def get_stock_price_with_mentions(self, stock_code: str, days: int = 90) -> Dict[str, Any]:
        """è·å–è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿ + æåŠæ ‡æ³¨ç‚¹"""
        end_date = datetime.now(BEIJING_TZ).strftime('%Y-%m-%d')
        start_date = (datetime.now(BEIJING_TZ) - timedelta(days=days)).strftime('%Y-%m-%d')

        prices = self.fetch_price_range(stock_code, start_date, end_date)

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute('''
            SELECT sm.mention_date, sm.context_snippet, sm.topic_id,
                   mp.return_5d, mp.max_return
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE sm.stock_code = ? AND sm.mention_date >= ?
            ORDER BY sm.mention_date
        ''', (stock_code, start_date))

        mentions = [dict(row) for row in cursor.fetchall()]
        stock_name = self._stock_dict.get(stock_code, stock_code)
        conn.close()

        return {
            'stock_code': stock_code,
            'stock_name': stock_name,
            'prices': prices,
            'mentions': mentions
        }

    def get_win_rate_ranking(self, min_mentions: int = 2, return_period: str = 'return_5d',
                             limit: int = 50) -> List[Dict]:
        """
        èƒœç‡æ’è¡Œæ¦œï¼šæŒ‰æåŠåNæ—¥æ­£æ”¶ç›Šç‡æ’åº

        Args:
            min_mentions: æœ€å°‘è¢«æåŠæ¬¡æ•°ï¼ˆè¿‡æ»¤å™ªéŸ³ï¼‰
            return_period: ä½¿ç”¨å“ªä¸ªæ”¶ç›Šç‡å‘¨æœŸ
            limit: è¿”å›æ•°é‡
        """
        valid_periods = ['return_1d', 'return_3d', 'return_5d', 'return_10d', 'return_20d']
        if return_period not in valid_periods:
            return_period = 'return_5d'

        conn = self._get_conn()
        cursor = conn.cursor()

        cursor.execute(f'''
            SELECT
                sm.stock_code,
                sm.stock_name,
                COUNT(*) as total_mentions,
                SUM(CASE WHEN mp.{return_period} > 0 THEN 1 ELSE 0 END) as win_count,
                ROUND(AVG(mp.{return_period}), 2) as avg_return,
                ROUND(MAX(mp.max_return), 2) as best_max_return,
                ROUND(AVG(mp.max_return), 2) as avg_max_return,
                ROUND(MIN(mp.max_drawdown), 2) as worst_drawdown,
                MAX(sm.mention_date) as latest_mention
            FROM stock_mentions sm
            JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE mp.{return_period} IS NOT NULL
            GROUP BY sm.stock_code
            HAVING COUNT(*) >= ?
            ORDER BY
                CAST(SUM(CASE WHEN mp.{return_period} > 0 THEN 1 ELSE 0 END) AS REAL) / COUNT(*) DESC,
                AVG(mp.{return_period}) DESC
            LIMIT ?
        ''', (min_mentions, limit))

        results = []
        for row in cursor.fetchall():
            total = row[2]
            wins = row[3]
            results.append({
                'stock_code': row[0],
                'stock_name': row[1],
                'total_mentions': total,
                'win_count': wins,
                'win_rate': round(wins / total * 100, 1) if total > 0 else 0,
                'avg_return': row[4],
                'best_max_return': row[5],
                'avg_max_return': row[6],
                'worst_drawdown': row[7],
                'latest_mention': row[8]
            })

        conn.close()
        return results

    def get_sector_heatmap(self) -> List[Dict]:
        """æ¿å—çƒ­åº¦åˆ†æ"""
        conn = self._get_conn()
        cursor = conn.cursor()

        # è·å–æ‰€æœ‰å¸–å­æ–‡æœ¬ï¼ˆå¸¦æ—¶é—´ï¼‰
        cursor.execute('''
            SELECT tk.text, t.create_time
            FROM topics t
            JOIN talks tk ON t.topic_id = tk.topic_id
            WHERE tk.text IS NOT NULL AND tk.text != ''
        ''')
        topics = cursor.fetchall()
        conn.close()

        # æŒ‰æ¿å—ç»Ÿè®¡
        sector_stats = {}
        for sector, keywords in SECTOR_KEYWORDS.items():
            mentions_by_date = {}
            total = 0
            for text, create_time in topics:
                text_lower = text.lower()
                if any(kw in text_lower for kw in keywords):
                    date = create_time[:10] if create_time else ''
                    if date:
                        mentions_by_date[date] = mentions_by_date.get(date, 0) + 1
                        total += 1

            if total > 0:
                sector_stats[sector] = {
                    'sector': sector,
                    'total_mentions': total,
                    'daily_mentions': dict(sorted(mentions_by_date.items())),
                    'peak_date': max(mentions_by_date, key=mentions_by_date.get) if mentions_by_date else None,
                    'peak_count': max(mentions_by_date.values()) if mentions_by_date else 0
                }

        return sorted(sector_stats.values(), key=lambda x: x['total_mentions'], reverse=True)

    def get_signals(self, lookback_days: int = 7, min_mentions: int = 2) -> List[Dict]:
        """
        ä¿¡å·é›·è¾¾ï¼šè¿‘æœŸé«˜é¢‘æåŠ + å†å²èƒœç‡é«˜çš„è‚¡ç¥¨

        æ¡ä»¶ï¼š
        - è¿‘ lookback_days å¤©å†…è¢«æåŠ >= min_mentions æ¬¡
        - å†å²æåŠå5æ—¥èƒœç‡ >= 50%
        """
        cutoff_date = (datetime.now(BEIJING_TZ) - timedelta(days=lookback_days)).strftime('%Y-%m-%d')

        conn = self._get_conn()
        cursor = conn.cursor()

        cursor.execute('''
            SELECT
                sm.stock_code,
                sm.stock_name,
                COUNT(*) as recent_mentions,
                (SELECT COUNT(*) FROM stock_mentions sm2
                 JOIN mention_performance mp2 ON sm2.id = mp2.mention_id
                 WHERE sm2.stock_code = sm.stock_code AND mp2.return_5d > 0
                ) as historical_wins,
                (SELECT COUNT(*) FROM stock_mentions sm3
                 JOIN mention_performance mp3 ON sm3.id = mp3.mention_id
                 WHERE sm3.stock_code = sm.stock_code AND mp3.return_5d IS NOT NULL
                ) as historical_total,
                (SELECT ROUND(AVG(mp4.return_5d), 2)
                 FROM stock_mentions sm4
                 JOIN mention_performance mp4 ON sm4.id = mp4.mention_id
                 WHERE sm4.stock_code = sm.stock_code
                ) as historical_avg_return,
                MAX(sm.mention_date) as latest_mention,
                GROUP_CONCAT(sm.context_snippet, ' | ') as recent_contexts
            FROM stock_mentions sm
            WHERE sm.mention_date >= ?
            GROUP BY sm.stock_code
            HAVING COUNT(*) >= ?
            ORDER BY COUNT(*) DESC
        ''', (cutoff_date, min_mentions))

        signals = []
        for row in cursor.fetchall():
            hist_total = row[4]
            hist_wins = row[3]
            win_rate = round(hist_wins / hist_total * 100, 1) if hist_total > 0 else None

            signals.append({
                'stock_code': row[0],
                'stock_name': row[1],
                'recent_mentions': row[2],
                'historical_win_rate': win_rate,
                'historical_avg_return': row[5],
                'latest_mention': row[6],
                'recent_contexts': row[7][:500] if row[7] else ''
            })

        conn.close()

        # æŒ‰ recent_mentions å’Œ win_rate ç»¼åˆæ’åº
        signals.sort(key=lambda x: (
            x['recent_mentions'] * 2 + (x['historical_win_rate'] or 0) / 10
        ), reverse=True)

        return signals

    def get_summary_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ¦‚è§ˆç»Ÿè®¡"""
        conn = self._get_conn()
        cursor = conn.cursor()

        stats = {}

        cursor.execute('SELECT COUNT(*) FROM stock_mentions')
        stats['total_mentions'] = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(DISTINCT stock_code) FROM stock_mentions')
        stats['unique_stocks'] = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(DISTINCT topic_id) FROM stock_mentions')
        stats['topics_with_stocks'] = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM mention_performance')
        stats['performance_calculated'] = cursor.fetchone()[0]

        # æ•´ä½“èƒœç‡
        cursor.execute('''
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN return_5d > 0 THEN 1 ELSE 0 END) as wins
            FROM mention_performance
            WHERE return_5d IS NOT NULL
        ''')
        row = cursor.fetchone()
        if row and row[0] > 0:
            stats['overall_win_rate_5d'] = round(row[1] / row[0] * 100, 1)
            stats['total_with_returns'] = row[0]
        else:
            stats['overall_win_rate_5d'] = None

        # æœ€è¢«æåŠçš„è‚¡ç¥¨ Top 10
        cursor.execute('''
            SELECT stock_code, stock_name, COUNT(*) as cnt
            FROM stock_mentions
            GROUP BY stock_code
            ORDER BY cnt DESC
            LIMIT 10
        ''')
        stats['top_mentioned'] = [
            {'stock_code': r[0], 'stock_name': r[1], 'count': r[2]}
            for r in cursor.fetchall()
        ]

        conn.close()
        return stats
