import random
import time
from typing import Any, Callable, Dict, List

from db_path_manager import get_db_path_manager
from zsxq_file_downloader import ZSXQFileDownloader


class GlobalFileTaskService:
    """å…¨åŒºæ–‡ä»¶æ”¶é›†/ä¸‹è½½æœåŠ¡ï¼ˆä» main.py æ‹†å‡ºä¸šåŠ¡æµç¨‹ï¼‰ã€‚"""

    @staticmethod
    def _apply_group_scan_filter_for_tasks(groups: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç»Ÿä¸€åº”ç”¨ç™½é»‘åå•è¿‡æ»¤ï¼Œä¾›å…¨åŒºä»»åŠ¡å¤ç”¨ã€‚"""
        from group_scan_filter import filter_groups

        filtered = filter_groups(groups)
        cfg = filtered.get("config", {}) or {}
        return {
            "all_groups": groups,
            "included_groups": filtered.get("included_groups", []) or [],
            "excluded_groups": filtered.get("excluded_groups", []) or [],
            "reason_counts": filtered.get("reason_counts", {}) or {},
            "default_action": str(cfg.get("default_action", "include")),
        }

    @staticmethod
    def _log_group_filter_summary(
        task_id: str,
        add_task_log: Callable[[str, str], None],
        all_groups: List[Dict[str, Any]],
        groups: List[Dict[str, Any]],
        excluded_groups: List[Dict[str, Any]],
        reason_counts: Dict[str, Any],
        default_action: str,
    ) -> None:
        add_task_log(task_id, f"ğŸ“‹ å…±å‘ç° {len(all_groups)} ä¸ªç¾¤ç»„")
        add_task_log(task_id, f"âš™ï¸ è¿‡æ»¤ç­–ç•¥: æœªé…ç½®ç¾¤ç»„é»˜è®¤{'çº³å…¥' if default_action == 'include' else 'æ’é™¤'}")
        add_task_log(task_id, f"ğŸ§¹ è¿‡æ»¤åçº³å…¥ {len(groups)}/{len(all_groups)} ä¸ªç¾¤ç»„")
        if reason_counts:
            add_task_log(task_id, f"ğŸ“Œ å‘½ä¸­ç»Ÿè®¡: {reason_counts}")
        if excluded_groups:
            preview = "ï¼Œ".join(
                f"{g.get('group_id')}({g.get('scan_filter_reason', 'unknown')})"
                for g in excluded_groups[:20]
            )
            suffix = " ..." if len(excluded_groups) > 20 else ""
            add_task_log(task_id, f"ğŸš« å·²æ’é™¤: {preview}{suffix}")

    def run_collect(
        self,
        task_id: str,
        add_task_log: Callable[[str, str], None],
        update_task: Callable[..., Any],
        is_task_stopped: Callable[[str], bool],
        get_cookie_for_group: Callable[[str], str],
        file_downloader_instances: Dict[str, Any],
    ) -> None:
        """æ‰§è¡Œå…¨åŒºæ–‡ä»¶åˆ—è¡¨æ”¶é›†æµç¨‹ã€‚"""
        try:
            update_task(task_id, "running", "å‡†å¤‡å¼€å§‹å…¨åŒºæ–‡ä»¶æ”¶é›†...")
            add_task_log(task_id, "ğŸš€ å¼€å§‹å…¨åŒºæ–‡ä»¶åˆ—è¡¨æ”¶é›†")

            manager = get_db_path_manager()
            all_groups = manager.list_all_groups()
            filtered = self._apply_group_scan_filter_for_tasks(all_groups)
            groups = filtered["included_groups"]
            excluded_groups = filtered["excluded_groups"]
            reason_counts = filtered["reason_counts"]
            default_action = filtered["default_action"]
            self._log_group_filter_summary(
                task_id,
                add_task_log,
                all_groups,
                groups,
                excluded_groups,
                reason_counts,
                default_action,
            )

            if not groups:
                update_task(task_id, "completed", "å…¨åŒºæ”¶é›†å®Œæˆ: è¿‡æ»¤åæ— å¯æ‰«æç¾¤ç»„")
                return

            processed_groups = 0
            for i, group in enumerate(groups, 1):
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                    break

                group_id = str(group["group_id"])
                add_task_log(task_id, "")
                add_task_log(task_id, f"ğŸ‘‰ [{i}/{len(groups)}] æ­£åœ¨æ”¶é›†ç¾¤ç»„ {group_id} çš„æ–‡ä»¶åˆ—è¡¨...")

                try:
                    cookie = get_cookie_for_group(group_id)
                    db_path = manager.get_files_db_path(group_id)

                    downloader = ZSXQFileDownloader(cookie, group_id, db_path)
                    downloader.log_callback = lambda msg: add_task_log(task_id, f"   {msg}")
                    downloader.stop_check_func = lambda: is_task_stopped(task_id)

                    file_downloader_instances[task_id] = downloader
                    res = downloader.collect_incremental_files()

                    add_task_log(
                        task_id,
                        f"   âœ… ç¾¤ç»„ {group_id} æ–‡ä»¶æ”¶é›†å®Œæˆ! æ–°å¢å¾…ä¸‹è½½: {res.get('new_files', 0) if isinstance(res, dict) else res}",
                    )
                    processed_groups += 1
                except Exception as ge:
                    add_task_log(task_id, f"   âŒ ç¾¤ç»„ {group_id} æ”¶é›†å¼‚å¸¸: {ge}")
                finally:
                    if task_id in file_downloader_instances:
                        del file_downloader_instances[task_id]

                if i < len(groups) and not is_task_stopped(task_id):
                    sleep_time = random.uniform(1.0, 3.0)
                    add_task_log(task_id, f"â³ ç­‰å¾… {sleep_time:.1f} ç§’...")
                    time.sleep(sleep_time)

            if is_task_stopped(task_id):
                update_task(task_id, "cancelled", "å…¨åŒºæ”¶é›†å·²åœæ­¢")
            else:
                add_task_log(task_id, "")
                add_task_log(task_id, "=" * 50)
                add_task_log(task_id, f"ğŸ‰ å…¨åŒºæ–‡ä»¶åˆ—è¡¨æ”¶é›†å®Œæˆï¼å…±å¤„ç† {processed_groups}/{len(groups)} ä¸ªç¾¤ç»„")
                update_task(task_id, "completed", f"å…¨åŒºæ”¶é›†å®Œæˆ: {processed_groups} ä¸ªç¾¤ç»„")
        except Exception as e:
            add_task_log(task_id, f"âŒ å…¨åŒºæ”¶é›†å¼‚å¸¸: {e}")
            update_task(task_id, "failed", f"å…¨åŒºæ”¶é›†å¤±è´¥: {e}")

    def run_download(
        self,
        task_id: str,
        request: Any,
        add_task_log: Callable[[str, str], None],
        update_task: Callable[..., Any],
        is_task_stopped: Callable[[str], bool],
        get_cookie_for_group: Callable[[str], str],
        file_downloader_instances: Dict[str, Any],
    ) -> None:
        """æ‰§è¡Œå…¨åŒºæ–‡ä»¶ä¸‹è½½æµç¨‹ã€‚"""
        try:
            update_task(task_id, "running", "å‡†å¤‡å¼€å§‹å…¨åŒºä¸‹è½½...")
            add_task_log(task_id, "ğŸš€ å¼€å§‹å…¨åŒºæ–‡ä»¶ä¸‹è½½")

            manager = get_db_path_manager()
            all_groups = manager.list_all_groups()
            filtered = self._apply_group_scan_filter_for_tasks(all_groups)
            groups = filtered["included_groups"]
            excluded_groups = filtered["excluded_groups"]
            reason_counts = filtered["reason_counts"]
            default_action = filtered["default_action"]
            self._log_group_filter_summary(
                task_id,
                add_task_log,
                all_groups,
                groups,
                excluded_groups,
                reason_counts,
                default_action,
            )

            if not groups:
                update_task(task_id, "completed", "å…¨åŒºä¸‹è½½å®Œæˆ: è¿‡æ»¤åæ— å¯æ‰«æç¾¤ç»„")
                return

            processed_groups = 0
            for i, group in enumerate(groups, 1):
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                    break

                group_id = str(group["group_id"])
                add_task_log(task_id, "")
                add_task_log(task_id, f"ğŸ‘‰ [{i}/{len(groups)}] æ­£åœ¨ä¸‹è½½ç¾¤ç»„ {group_id} çš„æ–‡ä»¶...")

                try:
                    cookie = get_cookie_for_group(group_id)
                    db_path = manager.get_files_db_path(group_id)

                    downloader = ZSXQFileDownloader(
                        cookie=cookie,
                        group_id=group_id,
                        db_path=db_path,
                        download_interval=request.download_interval,
                        long_sleep_interval=request.long_sleep_interval,
                        files_per_batch=request.files_per_batch,
                        download_interval_min=request.download_interval_min,
                        download_interval_max=request.download_interval_max,
                        long_sleep_interval_min=request.long_sleep_interval_min,
                        long_sleep_interval_max=request.long_sleep_interval_max,
                    )
                    downloader.log_callback = lambda msg: add_task_log(task_id, f"   {msg}")
                    downloader.stop_check_func = lambda: is_task_stopped(task_id)

                    file_downloader_instances[task_id] = downloader
                    res = downloader.download_files(request.max_files, sort_by=request.sort_by)

                    dl_success = res.get("downloaded", 0) if isinstance(res, dict) else res
                    add_task_log(task_id, f"   âœ… ç¾¤ç»„ {group_id} ä¸‹è½½å®Œæˆ! æˆåŠŸ: {dl_success}")
                    processed_groups += 1
                except Exception as ge:
                    add_task_log(task_id, f"   âŒ ç¾¤ç»„ {group_id} ä¸‹è½½å¼‚å¸¸: {ge}")
                finally:
                    if task_id in file_downloader_instances:
                        del file_downloader_instances[task_id]

            if is_task_stopped(task_id):
                update_task(task_id, "cancelled", "å…¨åŒºä¸‹è½½å·²åœæ­¢")
            else:
                add_task_log(task_id, "")
                add_task_log(task_id, "=" * 50)
                add_task_log(task_id, f"ğŸ‰ å…¨åŒºæ–‡ä»¶ä¸‹è½½å®Œæˆï¼å…±å¤„ç† {processed_groups}/{len(groups)} ä¸ªç¾¤ç»„")
                update_task(task_id, "completed", f"å…¨åŒºä¸‹è½½å®Œæˆ: {processed_groups} ä¸ªç¾¤ç»„")
        except Exception as e:
            add_task_log(task_id, f"âŒ å…¨åŒºä¸‹è½½å¼‚å¸¸: {e}")
            update_task(task_id, "failed", f"å…¨åŒºä¸‹è½½å¤±è´¥: {e}")

