import random
import time
from datetime import datetime, timedelta, timezone
from typing import Any, Callable, Dict, List

from db_path_manager import get_db_path_manager
from zsxq_interactive_crawler import ZSXQInteractiveCrawler


class GlobalCrawlService:
    """å…¨åŒºè¯é¢˜é‡‡é›†æœåŠ¡ï¼ˆä» main.py æ‹†å‡ºä¸šåŠ¡æµç¨‹ï¼‰ã€‚"""

    @staticmethod
    def _apply_group_scan_filter_for_tasks(groups: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç»Ÿä¸€åº”ç”¨ç™½é»‘åå•è¿‡æ»¤ï¼Œä¾›å…¨åŒºä»»åŠ¡å¤ç”¨ã€‚"""
        from group_scan_filter import filter_groups

        filtered = filter_groups(groups)
        cfg = filtered.get("config", {}) or {}
        return {
            "all_groups": groups,
            "included_groups": filtered.get("included_groups", []) or [],
            "excluded_groups": filtered.get("excluded_groups", []) or [],
            "reason_counts": filtered.get("reason_counts", {}) or {},
            "default_action": str(cfg.get("default_action", "include")),
        }

    @staticmethod
    def _log_group_filter_summary(
        task_id: str,
        add_task_log: Callable[[str, str], None],
        all_groups: List[Dict[str, Any]],
        groups: List[Dict[str, Any]],
        excluded_groups: List[Dict[str, Any]],
        reason_counts: Dict[str, Any],
        default_action: str,
    ) -> None:
        add_task_log(task_id, f"ğŸ“‹ å…±å‘ç° {len(all_groups)} ä¸ªç¾¤ç»„")
        add_task_log(task_id, f"âš™ï¸ è¿‡æ»¤ç­–ç•¥: æœªé…ç½®ç¾¤ç»„é»˜è®¤{'çº³å…¥' if default_action == 'include' else 'æ’é™¤'}")
        add_task_log(task_id, f"ğŸ§¹ è¿‡æ»¤åçº³å…¥ {len(groups)}/{len(all_groups)} ä¸ªç¾¤ç»„")
        if reason_counts:
            add_task_log(task_id, f"ğŸ“Œ å‘½ä¸­ç»Ÿè®¡: {reason_counts}")
        if excluded_groups:
            preview = "ï¼Œ".join(
                f"{g.get('group_id')}({g.get('scan_filter_reason', 'unknown')})"
                for g in excluded_groups[:20]
            )
            suffix = " ..." if len(excluded_groups) > 20 else ""
            add_task_log(task_id, f"ğŸš« å·²æ’é™¤: {preview}{suffix}")

    def run(
        self,
        task_id: str,
        request: Any,
        add_task_log: Callable[[str, str], None],
        update_task: Callable[..., Any],
        is_task_stopped: Callable[[str], bool],
        get_cookie_for_group: Callable[[str], str],
    ) -> None:
        """æ‰§è¡Œå…¨åŒºè¯é¢˜é‡‡é›†ä¸»æµç¨‹ã€‚"""
        try:
            update_task(task_id, "running", "å‡†å¤‡å¼€å§‹å…¨åŒºé‡‡é›†...")
            add_task_log(task_id, f"ğŸš€ å¼€å§‹å…¨åŒºè¯é¢˜é‡‡é›† [æ¨¡å¼: {request.mode}]")

            manager = get_db_path_manager()
            all_groups = manager.list_all_groups()
            filtered = self._apply_group_scan_filter_for_tasks(all_groups)
            groups = filtered["included_groups"]
            excluded_groups = filtered["excluded_groups"]
            reason_counts = filtered["reason_counts"]
            default_action = filtered["default_action"]
            self._log_group_filter_summary(
                task_id,
                add_task_log,
                all_groups,
                groups,
                excluded_groups,
                reason_counts,
                default_action,
            )

            if not groups:
                update_task(task_id, "completed", "å…¨åŒºé‡‡é›†å®Œæˆ: è¿‡æ»¤åæ— å¯æ‰«æç¾¤ç»„")
                return

            processed_groups = 0
            for i, group in enumerate(groups, 1):
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                    break

                group_id = str(group["group_id"])
                add_task_log(task_id, "")
                add_task_log(task_id, f"ğŸ‘‰ [{i}/{len(groups)}] æ­£åœ¨é‡‡é›†ç¾¤ç»„ {group_id}...")

                try:
                    cookie = get_cookie_for_group(group_id)
                    db_path = manager.get_topics_db_path(group_id)

                    def log_callback(msg: str):
                        add_task_log(task_id, f"   {msg}")

                    crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
                    crawler.stop_check_func = lambda: is_task_stopped(task_id)

                    crawler.set_custom_intervals(
                        crawl_interval_min=request.crawl_interval_min,
                        crawl_interval_max=request.crawl_interval_max,
                        long_sleep_interval_min=request.long_sleep_interval_min,
                        long_sleep_interval_max=request.long_sleep_interval_max,
                        pages_per_batch=request.pages_per_batch,
                    )

                    if request.mode == "latest":
                        res = crawler.crawl_latest_until_complete()
                    elif request.mode == "incremental":
                        res = crawler.crawl_incremental(pages=request.pages or 100, per_page=request.per_page or 20)
                    elif request.mode == "range":
                        if request.last_days is not None:
                            add_task_log(task_id, f"ğŸ§­ range æ¡ä»¶: æœ€è¿‘å¤©æ•° = {request.last_days}")
                        else:
                            add_task_log(
                                task_id,
                                f"ğŸ§­ range æ¡ä»¶: å¼€å§‹={request.start_time or 'è‡ªåŠ¨æ¨å¯¼'}ï¼Œç»“æŸ={request.end_time or 'å½“å‰æ—¶é—´'}",
                            )
                        range_start_time = request.start_time
                        range_end_time = request.end_time
                        if request.last_days and not range_start_time:
                            now_bj = datetime.now(timezone(timedelta(hours=8)))
                            range_start_time = (now_bj - timedelta(days=max(1, request.last_days))).isoformat()
                        safe_max_items = max(1, int(request.max_items or 500))
                        res = crawler.crawl_time_range(
                            start_time=range_start_time,
                            end_time=range_end_time,
                            max_items=safe_max_items,
                            per_page=request.per_page or 20,
                        )
                    elif request.mode == "all":
                        res = crawler.crawl_all_historical(per_page=request.per_page or 20, auto_confirm=True)
                    else:
                        add_task_log(task_id, f"   âš ï¸ æœªçŸ¥çš„é‡‡é›†æ¨¡å¼: {request.mode}")
                        continue

                    if not isinstance(res, dict):
                        res = {"new_topics": 0, "updated_topics": 0, "errors": 1}

                    if res.get("expired"):
                        add_task_log(task_id, f"   âŒ ç¾¤ç»„ {group_id} ä¼šå‘˜å·²è¿‡æœŸ: {res.get('message')}")
                    elif res.get("stopped"):
                        add_task_log(task_id, f"   ğŸ›‘ ç¾¤ç»„ {group_id} é‡‡é›†å·²åœæ­¢")
                    else:
                        add_task_log(
                            task_id,
                            f"   âœ… ç¾¤ç»„ {group_id} é‡‡é›†å®Œæˆ! æ–°å¢: {res.get('new_topics', 0)}, æ›´æ–°: {res.get('updated_topics', 0)}",
                        )
                        processed_groups += 1
                except Exception as ge:
                    add_task_log(task_id, f"   âŒ ç¾¤ç»„ {group_id} é‡‡é›†å¼‚å¸¸: {ge}")

                if i < len(groups) and not is_task_stopped(task_id):
                    sleep_time = random.uniform(2.0, 5.0)
                    add_task_log(task_id, f"â³ ç­‰å¾… {sleep_time:.1f} ç§’åé‡‡é›†ä¸‹ä¸€ä¸ªç¾¤ç»„...")
                    time.sleep(sleep_time)

            if is_task_stopped(task_id):
                update_task(task_id, "cancelled", "å…¨åŒºé‡‡é›†å·²åœæ­¢")
            else:
                add_task_log(task_id, "")
                add_task_log(task_id, "=" * 50)
                add_task_log(task_id, f"ğŸ‰ å…¨åŒºé‡‡é›†å®Œæˆï¼å…±å¤„ç† {processed_groups}/{len(groups)} ä¸ªç¾¤ç»„")
                update_task(task_id, "completed", f"å…¨åŒºé‡‡é›†å®Œæˆ: {processed_groups} ä¸ªç¾¤ç»„")
        except Exception as e:
            add_task_log(task_id, f"âŒ å…¨åŒºé‡‡é›†å¼‚å¸¸: {e}")
            update_task(task_id, "failed", f"å…¨åŒºé‡‡é›†å¤±è´¥: {e}")

