import time
import os
from typing import Any, Callable, Dict, List, Set
from datetime import datetime, timedelta

from modules.shared.db_path_manager import get_db_path_manager
from modules.analyzers.global_analyzer import get_global_analyzer
from modules.analyzers.stock_analyzer import StockAnalyzer
from modules.analyzers.market_data_sync import MarketDataSyncService
from api.services.group_filter_service import apply_group_scan_filter, format_group_filter_summary
from modules.shared.logger_config import log_warning


class GlobalAnalyzePerformanceService:
    """å…¨åŒºæ”¶ç›Šè®¡ç®—æœåŠ¡ï¼ˆä» main.py æ‹†å‡ºä¸šåŠ¡æµç¨‹ï¼‰ã€‚"""

    def _collect_pending_stock_codes(
        self,
        analyzer: StockAnalyzer,
        calc_window_days: int,
    ) -> Set[str]:
        """æ”¶é›†ç¾¤ç»„ä¸­å¾…è®¡ç®—æ”¶ç›Šçš„è‚¡ç¥¨ä»£ç é›†åˆï¼ˆä»…æœ¬åœ° SQL æŸ¥è¯¢ï¼‰ã€‚"""
        since_date = (datetime.now() - timedelta(days=calc_window_days)).strftime("%Y-%m-%d")
        conn = analyzer._get_conn()
        cursor = conn.cursor()
        cursor.execute(
            """
            SELECT DISTINCT sm.stock_code
            FROM stock_mentions sm
            LEFT JOIN mention_performance mp ON sm.id = mp.mention_id
            WHERE (mp.mention_id IS NULL OR mp.freeze_level IS NULL OR mp.freeze_level < 3)
              AND sm.mention_date >= ?
            """,
            (since_date,),
        )
        codes = {str(row[0]) for row in cursor.fetchall()}
        conn.close()
        return codes

    def run(
        self,
        task_id: str,
        add_task_log: Callable[[str, str], None],
        update_task: Callable[..., Any],
        is_task_stopped: Callable[[str], bool],
        calc_window_days: int = 365,
    ) -> None:
        """æ‰§è¡Œå…¨åŒºæ”¶ç›Šè®¡ç®—ä¸»æµç¨‹ã€‚"""
        try:
            update_task(task_id, "running", "å‡†å¤‡å¼€å§‹å…¨åŒºæ”¶ç›Šè®¡ç®—...")
            add_task_log(task_id, "ğŸš€ å¼€å§‹å…¨åŒºæåŠæ”¶ç›Šåˆ·æ–°")
            since_date = (datetime.now() - timedelta(days=max(int(calc_window_days or 1), 1))).strftime("%Y-%m-%d")
            add_task_log(task_id, f"ğŸ—“ï¸ æ”¶ç›Šè®¡ç®—çª—å£: since={since_date} (calc_window_days={int(calc_window_days or 1)})")

            manager = get_db_path_manager()
            all_groups = manager.list_all_groups()
            filtered = apply_group_scan_filter(all_groups)
            groups = filtered["included_groups"]
            excluded_groups = filtered["excluded_groups"]
            reason_counts = filtered["reason_counts"]
            default_action = filtered["default_action"]

            for line in format_group_filter_summary(
                all_groups,
                groups,
                excluded_groups,
                reason_counts,
                default_action,
            ):
                add_task_log(task_id, line)

            if not groups:
                update_task(task_id, "completed", "å…¨åŒºæ”¶ç›Šè®¡ç®—å®Œæˆ: è¿‡æ»¤åæ— å¯æ‰«æç¾¤ç»„")
                return

            processed_groups = 0
            groups_with_auto_extract = 0
            mentions_extracted_total = 0
            performance_processed_total = 0

            # â”€â”€ Phase 1: å…¨ç¾¤æå– + æ”¶é›†å¾…ç®—è‚¡ç¥¨ä»£ç  â”€â”€
            add_task_log(task_id, "")
            add_task_log(task_id, "â•" * 40)
            add_task_log(task_id, "ğŸ“‹ Phase 1: å…¨ç¾¤å¢é‡æå– & æ”¶é›†å¾…ç®—è‚¡ç¥¨")
            all_pending_stocks: Set[str] = set()
            group_analyzers: Dict[str, StockAnalyzer] = {}

            for i, group in enumerate(groups, 1):
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                    break

                group_id = str(group["group_id"])
                analyzer = StockAnalyzer(
                    group_id,
                    stop_check=lambda: is_task_stopped(task_id),
                )
                group_analyzers[group_id] = analyzer

                # å¢é‡æå–
                extract_res = analyzer.extract_only()
                extracted_mentions = int(extract_res.get("mentions_extracted", 0) or 0)
                new_topics = int(extract_res.get("new_topics", 0) or 0)
                if new_topics > 0 or extracted_mentions > 0:
                    groups_with_auto_extract += 1
                mentions_extracted_total += extracted_mentions

                # æ”¶é›†å¾…ç®—è‚¡ç¥¨ä»£ç 
                pending_codes = self._collect_pending_stock_codes(analyzer, calc_window_days)
                all_pending_stocks |= pending_codes

                add_task_log(
                    task_id,
                    f"   [{i}/{len(groups)}] ç¾¤ {group_id}: æå– {extracted_mentions} æ¡, å¾…ç®—è‚¡ç¥¨ {len(pending_codes)} åª",
                )

            # â”€â”€ Phase 1.5: å…¨å±€è¡Œæƒ…é¢„çƒ­ï¼ˆä¸€æ¬¡æ€§ï¼‰ â”€â”€
            if all_pending_stocks and not is_task_stopped(task_id):
                add_task_log(task_id, "")
                add_task_log(task_id, f"ğŸ§° å…¨å±€è¡Œæƒ…é¢„çƒ­: å…± {len(all_pending_stocks)} åªå”¯ä¸€è‚¡ç¥¨")
                prewarm_started = time.perf_counter()
                try:
                    market_sync = MarketDataSyncService()
                    history_days = max(20, int(calc_window_days) + 20)
                    end_date = datetime.now().strftime("%Y-%m-%d")
                    start_date = (datetime.now() - timedelta(days=history_days)).strftime("%Y-%m-%d")

                    # ä¼˜å…ˆå°è¯• tushare æŒ‰æ—¥æœŸæ‰¹é‡æ¨¡å¼ï¼ˆ~14æ¬¡API vs ~1700æ¬¡ï¼‰
                    batch_res = market_sync.sync_daily_by_dates(
                        start_date=start_date,
                        end_date=end_date,
                        symbols=sorted(all_pending_stocks),
                        include_index=True,
                    )
                    if batch_res.get("success") or batch_res.get("upserted", 0) > 0:
                        prewarm_elapsed = time.perf_counter() - prewarm_started
                        add_task_log(
                            task_id,
                            f"ğŸ§° å…¨å±€é¢„çƒ­å®Œæˆ (æ‰¹é‡æ—¥æœŸæ¨¡å¼): api_calls={batch_res.get('api_calls', 0)}, "
                            f"upserted={batch_res.get('upserted', 0)}, elapsed={prewarm_elapsed:.1f}s",
                        )
                    else:
                        # å›é€€åˆ°é€è‚¡åˆ†ç‰‡æ¨¡å¼
                        add_task_log(
                            task_id,
                            f"âš ï¸ æ‰¹é‡æ—¥æœŸæ¨¡å¼ä¸å¯ç”¨ ({batch_res.get('message', '')}), å›é€€åˆ°é€è‚¡é¢„çƒ­",
                        )
                        chunk_size = max(1, int(os.environ.get("PERF_PREWARM_CHUNK_SIZE", "200")))
                        symbols_sorted = sorted(all_pending_stocks)
                        prewarm_chunks = [
                            symbols_sorted[j : j + chunk_size]
                            for j in range(0, len(symbols_sorted), chunk_size)
                        ]
                        prewarm_ok = 0
                        prewarm_fail = 0
                        for idx, chunk in enumerate(prewarm_chunks, 1):
                            if is_task_stopped(task_id):
                                add_task_log(task_id, "ğŸ›‘ é¢„çƒ­é˜¶æ®µåœæ­¢")
                                break
                            try:
                                res = market_sync.sync_daily_incremental(
                                    history_days=history_days,
                                    symbols=chunk,
                                    include_index=(idx == 1),
                                    finalize_today=False,
                                )
                                if res.get("success"):
                                    prewarm_ok += 1
                                else:
                                    prewarm_fail += 1
                            except Exception as e:
                                prewarm_fail += 1
                                log_warning(f"å…¨å±€é¢„çƒ­åˆ†ç‰‡å¼‚å¸¸ chunk={idx}/{len(prewarm_chunks)}: {e}")
                        prewarm_elapsed = time.perf_counter() - prewarm_started
                        add_task_log(
                            task_id,
                            f"ğŸ§° å…¨å±€é¢„çƒ­å®Œæˆ (é€è‚¡æ¨¡å¼): chunks={len(prewarm_chunks)}, ok={prewarm_ok}, "
                            f"fail={prewarm_fail}, elapsed={prewarm_elapsed:.1f}s",
                        )
                except Exception as e:
                    prewarm_elapsed = time.perf_counter() - prewarm_started
                    add_task_log(task_id, f"âš ï¸ å…¨å±€é¢„çƒ­å¼‚å¸¸: {e} (elapsed={prewarm_elapsed:.1f}s)")

            # â”€â”€ Phase 2: é€ç¾¤æ”¶ç›Šè®¡ç®—ï¼ˆè·³è¿‡ç¾¤å†…é¢„çƒ­ï¼‰ â”€â”€
            add_task_log(task_id, "")
            add_task_log(task_id, "â•" * 40)
            add_task_log(task_id, "ğŸ“ˆ Phase 2: é€ç¾¤æ”¶ç›Šè®¡ç®—")

            for i, group in enumerate(groups, 1):
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                    break

                group_id = str(group["group_id"])
                add_task_log(task_id, "")
                add_task_log(task_id, f"ğŸ‘‰ [{i}/{len(groups)}] æ­£åœ¨è®¡ç®—ç¾¤ {group_id} çš„æ”¶ç›Š...")

                try:
                    analyzer = group_analyzers.get(group_id)
                    if analyzer is None:
                        analyzer = StockAnalyzer(
                            group_id,
                            stop_check=lambda: is_task_stopped(task_id),
                        )

                    backlog = analyzer._get_analysis_backlog_stats(calc_window_days=calc_window_days)
                    add_task_log(
                        task_id,
                        f"   ğŸ§© é¢„æ£€æŸ¥: mentions={backlog.get('mentions_total', 0)}, pending={backlog.get('pending_total', 0)}",
                    )

                    last_log_time = 0.0
                    last_log_percent = -1
                    progress_log_interval = max(
                        1.0, float(os.environ.get("PERF_PROGRESS_LOG_INTERVAL_SECONDS", "15"))
                    )

                    def progress_cb(current: int, total: int, status: str):
                        nonlocal last_log_time, last_log_percent
                        now = time.time()
                        percent = int((current * 100) / total) if total > 0 else 100
                        if (
                            current in {1, total}
                            or percent >= (last_log_percent + 1)
                            or (now - last_log_time) >= progress_log_interval
                        ):
                            add_task_log(task_id, f"   â³ è¿›åº¦: {current}/{total} - {status}")
                            last_log_time = now
                            last_log_percent = percent

                    # å·²å…¨å±€é¢„çƒ­ï¼Œè·³è¿‡ç¾¤å†…é¢„çƒ­
                    original_prewarm = analyzer.PERF_PREWARM_ENABLED
                    analyzer.PERF_PREWARM_ENABLED = False
                    try:
                        res = analyzer.calc_pending_performance(
                            calc_window_days=calc_window_days,
                            progress_callback=progress_cb,
                        )
                    finally:
                        analyzer.PERF_PREWARM_ENABLED = original_prewarm

                    if bool(res.get("aborted")) or is_task_stopped(task_id):
                        add_task_log(task_id, f"   ğŸ›‘ ç¾¤ç»„ {group_id} æ”¶ç›Šè®¡ç®—å·²åœæ­¢")
                        break
                    processed_count = int(res.get("processed", 0) or 0)
                    skipped_count = int(res.get("skipped", 0) or 0)
                    error_count = int(res.get("errors", 0) or 0)
                    performance_processed_total += processed_count
                    add_task_log(
                        task_id,
                        f"   âœ… ç¾¤ç»„ {group_id} æ”¶ç›Šè®¡ç®—å®Œæˆ! processed={processed_count}, skipped={skipped_count}, errors={error_count}",
                    )
                    processed_groups += 1
                except Exception as ge:
                    add_task_log(task_id, f"   âŒ ç¾¤ç»„ {group_id} è®¡ç®—å¼‚å¸¸: {ge}")

            if is_task_stopped(task_id):
                update_task(task_id, "cancelled", "å…¨åŒºè®¡ç®—å·²åœæ­¢")
            else:
                add_task_log(task_id, "")
                add_task_log(task_id, "=" * 50)
                add_task_log(task_id, f"ğŸ‰ å…¨åŒºæ”¶ç›Šè®¡ç®—å®Œæˆï¼å…±å¤„ç† {processed_groups}/{len(groups)} ä¸ªç¾¤ç»„")
                add_task_log(
                    task_id,
                    f"ğŸ“Š è‡ªåŠ¨æå–ç¾¤ç»„: {groups_with_auto_extract}, è‡ªåŠ¨æå–æåŠ: {mentions_extracted_total}, æ”¶ç›Šå¤„ç†æ¡æ•°: {performance_processed_total}",
                )

                try:
                    get_global_analyzer().invalidate_cache()
                    add_task_log(task_id, "ğŸ”„ å…¨å±€ç»Ÿè®¡ç¼“å­˜å·²åˆ·æ–°")
                except Exception:
                    pass

                update_task(
                    task_id,
                    "completed",
                    f"å…¨åŒºæ”¶ç›Šè®¡ç®—å®Œæˆ: {processed_groups} ä¸ªç¾¤ç»„",
                    {
                        "groups_processed": processed_groups,
                        "groups_total": len(groups),
                        "groups_with_auto_extract": groups_with_auto_extract,
                        "mentions_extracted_total": mentions_extracted_total,
                        "performance_processed_total": performance_processed_total,
                    },
                )

        except Exception as e:
            add_task_log(task_id, f"âŒ å…¨åŒºè®¡ç®—å¼‚å¸¸: {e}")
            update_task(task_id, "failed", f"å…¨åŒºè®¡ç®—å¤±è´¥: {e}")

