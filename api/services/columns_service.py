from __future__ import annotations

import asyncio
import json
import os
import random
import time
from typing import Any, Dict, Optional

import requests
from fastapi import HTTPException

from modules.accounts.accounts_sql_manager import get_accounts_sql_manager
from modules.shared.db_path_manager import get_db_path_manager
from modules.shared.logger_config import log_debug, log_error, log_exception, log_info
from modules.zsxq.zsxq_columns_database import ZSXQColumnsDatabase
from modules.zsxq.zsxq_interactive_crawler import load_config
from api.schemas.models import ColumnsSettingsRequest
from api.services.task_facade import TaskFacade


class ColumnsService:
    def __init__(self):
        self.tasks = TaskFacade()

    def _get_columns_db(self, group_id: str) -> ZSXQColumnsDatabase:
        path_manager = get_db_path_manager()
        db_path = path_manager.get_columns_db_path(group_id)
        return ZSXQColumnsDatabase(db_path)

    def _resolve_cookie_for_group(self, group_id: str) -> str:
        manager = get_accounts_sql_manager()
        account = manager.get_account_for_group(group_id, mask_cookie=False)
        if account and account.get("cookie"):
            return str(account["cookie"]).strip()

        first = manager.get_first_account(mask_cookie=False)
        if first and first.get("cookie"):
            return str(first["cookie"]).strip()

        cfg = load_config() or {}
        cookie = str((cfg.get("auth", {}) or {}).get("cookie", "")).strip()
        return cookie

    def _build_stealth_headers(self, cookie: str) -> Dict[str, str]:
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        ]
        return {
            "Accept": "application/json, text/plain, */*",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Cache-Control": "no-cache",
            "Cookie": cookie,
            "Origin": "https://wx.zsxq.com",
            "Pragma": "no-cache",
            "Referer": "https://wx.zsxq.com/",
            "Sec-Ch-Ua": '\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '\"Windows\"',
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-site",
            "User-Agent": random.choice(user_agents),
            "X-Request-Id": f"dcc5cb6ab-1bc3-8273-cc26-{random.randint(100000000000, 999999999999)}",
            "X-Timestamp": str(int(time.time())),
            "X-Version": "2.77.0",
        }

    def get_group_columns(self, group_id: str) -> Dict[str, Any]:
        db = self._get_columns_db(group_id)
        try:
            columns = db.get_columns(int(group_id))
            stats = db.get_stats(int(group_id))
            return {"columns": columns, "stats": stats}
        finally:
            db.close()

    def get_column_topics(self, group_id: str, column_id: int) -> Dict[str, Any]:
        db = self._get_columns_db(group_id)
        try:
            topics = db.get_column_topics(column_id)
            column = db.get_column(column_id)
            return {"column": column, "topics": topics}
        finally:
            db.close()

    def get_column_topic_detail(self, group_id: str, topic_id: int) -> Dict[str, Any]:
        db = self._get_columns_db(group_id)
        try:
            detail = db.get_topic_detail(topic_id)
        finally:
            db.close()

        if not detail:
            raise HTTPException(status_code=404, detail="æ–‡ç« è¯¦æƒ…ä¸å­˜åœ¨")

        if detail.get("raw_json"):
            try:
                raw_data = json.loads(detail["raw_json"])
                topic_type = raw_data.get("type", "")
                if topic_type == "q&a":
                    question = raw_data.get("question", {})
                    answer = raw_data.get("answer", {})
                    detail["question"] = {
                        "text": question.get("text", ""),
                        "owner": question.get("owner"),
                        "images": question.get("images", []),
                    }
                    detail["answer"] = {
                        "text": answer.get("text", ""),
                        "owner": answer.get("owner"),
                        "images": answer.get("images", []),
                    }
                    if not detail.get("full_text") and answer.get("text"):
                        detail["full_text"] = answer.get("text", "")
                elif topic_type == "talk":
                    talk = raw_data.get("talk", {})
                    if not detail.get("full_text") and talk.get("text"):
                        detail["full_text"] = talk.get("text", "")
            except (json.JSONDecodeError, TypeError):
                pass

        return detail

    def get_group_columns_summary(self, group_id: str) -> Dict[str, Any]:
        cookie = self._resolve_cookie_for_group(group_id)
        if not cookie:
            return {"has_columns": False, "title": None, "error": "æœªæ‰¾åˆ°å¯ç”¨Cookie"}

        headers = self._build_stealth_headers(cookie)
        url = f"https://api.zsxq.com/v2/groups/{group_id}/columns/summary"
        try:
            response = requests.get(url, headers=headers, timeout=30)
        except requests.RequestException as e:
            return {"has_columns": False, "title": None, "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}

        if response.status_code != 200:
            return {"has_columns": False, "title": None, "error": f"HTTP {response.status_code}"}

        data = response.json()
        if not data.get("succeeded"):
            return {"has_columns": False, "title": None, "error": data.get("error_message", "APIè¿”å›å¤±è´¥")}

        resp_data = data.get("resp_data", {})
        return {"has_columns": resp_data.get("has_columns", False), "title": resp_data.get("title", None)}



    def start_fetch(self, group_id: str, request: Any, background_tasks: Any) -> Dict[str, Any]:
        task_id = self.tasks.create_task("columns_fetch", f"ä¸“æ é‡‡é›† (ç¾¤ç»„: {group_id})")
        # keep historical prefix for easier frontend filtering
        try:
            now = self.tasks.tasks.get(task_id, {})
            now["task_id"] = task_id
            now["group_id"] = group_id
        except Exception:
            pass
        background_tasks.add_task(self._fetch_columns_task, task_id, group_id, request)
        return {"success": True, "task_id": task_id, "message": "ä¸“æ é‡‡é›†ä»»åŠ¡å·²å¯åŠ¨"}

    async def _fetch_columns_task(self, task_id: str, group_id: str, settings: ColumnsSettingsRequest):
        """ä¸“æ é‡‡é›†åå°ä»»åŠ¡"""
        log_id = None
        db = None

        try:
            # è·å–é…ç½®å‚æ•°
            crawl_interval_min = settings.crawlIntervalMin or 2.0
            crawl_interval_max = settings.crawlIntervalMax or 5.0
            long_sleep_min = settings.longSleepIntervalMin or 30.0
            long_sleep_max = settings.longSleepIntervalMax or 60.0
            items_per_batch = settings.itemsPerBatch or 10
            download_files = settings.downloadFiles if settings.downloadFiles is not None else True
            download_videos = settings.downloadVideos if settings.downloadVideos is not None else True
            cache_images = settings.cacheImages if settings.cacheImages is not None else True
            incremental_mode = settings.incrementalMode if settings.incrementalMode is not None else False

            self.tasks.append_log(task_id, f"ğŸ“š å¼€å§‹é‡‡é›†ç¾¤ç»„ {group_id} çš„ä¸“æ å†…å®¹")
            self.tasks.append_log(task_id, "=" * 50)
            self.tasks.append_log(task_id, "âš™ï¸ é‡‡é›†é…ç½®:")
            self.tasks.append_log(task_id, f"   â±ï¸ è¯·æ±‚é—´éš”: {crawl_interval_min}~{crawl_interval_max} ç§’")
            self.tasks.append_log(task_id, f"   ğŸ˜´ é•¿ä¼‘çœ é—´éš”: {long_sleep_min}~{long_sleep_max} ç§’")
            self.tasks.append_log(task_id, f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {items_per_batch} ä¸ªè¯·æ±‚")
            self.tasks.append_log(task_id, f"   ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {'æ˜¯' if download_files else 'å¦'}")
            self.tasks.append_log(task_id, f"   ğŸ¬ ä¸‹è½½è§†é¢‘: {'æ˜¯' if download_videos else 'å¦'}")
            self.tasks.append_log(task_id, f"   ğŸ–¼ï¸ ç¼“å­˜å›¾ç‰‡: {'æ˜¯' if cache_images else 'å¦'}")
            self.tasks.append_log(task_id, f"   ğŸ”„ å¢é‡æ¨¡å¼: {'æ˜¯ï¼ˆè·³è¿‡å·²å­˜åœ¨ï¼‰' if incremental_mode else 'å¦ï¼ˆå…¨é‡é‡‡é›†ï¼‰'}")
            self.tasks.append_log(task_id, "=" * 50)

            cookie = self._resolve_cookie_for_group(group_id)
            if not cookie:
                raise Exception("æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·")

            headers = self._build_stealth_headers(cookie)
            db = self._get_columns_db(group_id)
            log_id = db.start_crawl_log(int(group_id), 'full_fetch')

            columns_count = 0
            topics_count = 0
            details_count = 0
            files_count = 0
            images_count = 0
            videos_count = 0
            skipped_count = 0  # å¢é‡æ¨¡å¼è·³è¿‡çš„æ–‡ç« æ•°
            files_skipped = 0  # è·³è¿‡çš„æ–‡ä»¶æ•°ï¼ˆå·²å­˜åœ¨ï¼‰
            videos_skipped = 0  # è·³è¿‡çš„è§†é¢‘æ•°ï¼ˆå·²å­˜åœ¨ï¼‰
            request_count = 0  # è¯·æ±‚è®¡æ•°å™¨ï¼Œç”¨äºè§¦å‘é•¿ä¼‘çœ 

            # 1. è·å–ä¸“æ ç›®å½•åˆ—è¡¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            self.tasks.append_log(task_id, "ğŸ“‚ è·å–ä¸“æ ç›®å½•åˆ—è¡¨...")
            columns_url = f"https://api.zsxq.com/v2/groups/{group_id}/columns"
            max_retries = 10
            columns = None

            for retry in range(max_retries):
                if self.tasks.is_task_stopped(task_id):
                    break

                try:
                    resp = requests.get(columns_url, headers=headers, timeout=30)
                    request_count += 1
                except Exception as req_err:
                    log_exception(f"è·å–ä¸“æ ç›®å½•è¯·æ±‚å¼‚å¸¸: group_id={group_id}, url={columns_url}")
                    if retry < max_retries - 1:
                        wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                        self.tasks.append_log(task_id, f"   âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                        await asyncio.sleep(wait_time)
                        continue
                    raise Exception(f"è·å–ä¸“æ ç›®å½•è¯·æ±‚å¼‚å¸¸: {req_err}")

                if resp.status_code != 200:
                    log_error(f"è·å–ä¸“æ ç›®å½•å¤±è´¥: group_id={group_id}, HTTP {resp.status_code}, response={resp.text[:500] if resp.text else 'empty'}")
                    if retry < max_retries - 1:
                        wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                        self.tasks.append_log(task_id, f"   âš ï¸ HTTP {resp.status_code}ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                        await asyncio.sleep(wait_time)
                        continue
                    raise Exception(f"è·å–ä¸“æ ç›®å½•å¤±è´¥: HTTP {resp.status_code}")

                try:
                    data = resp.json()
                except Exception as json_err:
                    log_exception(f"è§£æä¸“æ ç›®å½•JSONå¤±è´¥: group_id={group_id}, response={resp.text[:500] if resp.text else 'empty'}")
                    raise Exception(f"è§£æä¸“æ ç›®å½•å¤±è´¥: {json_err}")

                if not data.get('succeeded'):
                    error_code = data.get('code')
                    error_msg = data.get('error_message', 'æœªçŸ¥é”™è¯¯')

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸ
                    if 'expired' in error_msg.lower() or data.get('resp_data', {}).get('expired'):
                        raise Exception(f"ä¼šå‘˜å·²è¿‡æœŸ: {error_msg}")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
                    if error_code == 1059:
                        if retry < max_retries - 1:
                            wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                            self.tasks.append_log(task_id, f"   âš ï¸ é‡åˆ°åçˆ¬æœºåˆ¶ (é”™è¯¯ç 1059)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            log_error(f"è·å–ä¸“æ ç›®å½•é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: group_id={group_id}, code={error_code}")
                            raise Exception(f"è·å–ä¸“æ ç›®å½•å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é‡åˆ°åçˆ¬é™åˆ¶")
                    else:
                        log_error(f"è·å–ä¸“æ ç›®å½•APIå¤±è´¥: group_id={group_id}, code={error_code}, message={error_msg}, response={json.dumps(data, ensure_ascii=False)[:500]}")
                        raise Exception(f"APIè¿”å›å¤±è´¥: {error_msg} (code={error_code})")
                else:
                    # æˆåŠŸè·å–
                    columns = data.get('resp_data', {}).get('columns', [])
                    if retry > 0:
                        self.tasks.append_log(task_id, f"   âœ… é‡è¯•æˆåŠŸ (ç¬¬{retry+1}æ¬¡å°è¯•)")
                    break

            if columns is None:
                raise Exception("è·å–ä¸“æ ç›®å½•å¤±è´¥")
            self.tasks.append_log(task_id, f"âœ… è·å–åˆ° {len(columns)} ä¸ªä¸“æ ç›®å½•")

            if len(columns) == 0:
                self.tasks.append_log(task_id, "â„¹ï¸ è¯¥ç¾¤ç»„æ²¡æœ‰ä¸“æ å†…å®¹")
                self.tasks.update_task(task_id, "completed", "è¯¥ç¾¤ç»„æ²¡æœ‰ä¸“æ å†…å®¹")
                db.close()
                return

            # 2. éå†æ¯ä¸ªä¸“æ 
            for col_idx, column in enumerate(columns, 1):
                if self.tasks.is_task_stopped(task_id):
                    self.tasks.append_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                    break

                column_id = column.get('column_id')
                column_name = column.get('name', 'æœªå‘½å')
                column_topics_count = column.get('statistics', {}).get('topics_count', 0)
                db.insert_column(int(group_id), column)
                columns_count += 1

                self.tasks.append_log(task_id, "")
                self.tasks.append_log(task_id, f"ğŸ“ [{col_idx}/{len(columns)}] ä¸“æ : {column_name}")
                self.tasks.append_log(task_id, f"   ğŸ“Š é¢„è®¡æ–‡ç« æ•°: {column_topics_count}")

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é•¿ä¼‘çœ 
                if request_count > 0 and request_count % items_per_batch == 0:
                    sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                    self.tasks.append_log(task_id, f"   ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                    await asyncio.sleep(sleep_time)

                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(crawl_interval_min, crawl_interval_max)
                self.tasks.append_log(task_id, f"   â³ ç­‰å¾… {delay:.1f} ç§’åè·å–æ–‡ç« åˆ—è¡¨...")
                await asyncio.sleep(delay)

                # è·å–ä¸“æ æ–‡ç« åˆ—è¡¨
                topics_url = f"https://api.zsxq.com/v2/groups/{group_id}/columns/{column_id}/topics?count=100&sort=default&direction=desc"
                try:
                    topics_resp = requests.get(topics_url, headers=headers, timeout=30)
                    request_count += 1
                except Exception as req_err:
                    log_exception(f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨è¯·æ±‚å¼‚å¸¸: column_id={column_id}, url={topics_url}")
                    self.tasks.append_log(task_id, f"   âš ï¸ è·å–æ–‡ç« åˆ—è¡¨è¯·æ±‚å¼‚å¸¸: {req_err}")
                    continue

                if topics_resp.status_code != 200:
                    log_error(f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨å¤±è´¥: column_id={column_id}, HTTP {topics_resp.status_code}, response={topics_resp.text[:500] if topics_resp.text else 'empty'}")
                    self.tasks.append_log(task_id, f"   âš ï¸ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: HTTP {topics_resp.status_code}")
                    continue

                try:
                    topics_data = topics_resp.json()
                except Exception as json_err:
                    log_exception(f"è§£æä¸“æ æ–‡ç« åˆ—è¡¨JSONå¤±è´¥: column_id={column_id}, response={topics_resp.text[:500] if topics_resp.text else 'empty'}")
                    self.tasks.append_log(task_id, f"   âš ï¸ è§£ææ–‡ç« åˆ—è¡¨å¤±è´¥: {json_err}")
                    continue

                if not topics_data.get('succeeded'):
                    error_code = topics_data.get('code', 'unknown')
                    error_message = topics_data.get('error_message', 'æœªçŸ¥é”™è¯¯')
                    log_error(f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨å¤±è´¥: column_id={column_id}, code={error_code}, message={error_message}")
                    self.tasks.append_log(task_id, f"   âš ï¸ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {error_message} (code={error_code})")
                    continue

                topics_list = topics_data.get('resp_data', {}).get('topics', [])
                self.tasks.append_log(task_id, f"   ğŸ“ è·å–åˆ° {len(topics_list)} ç¯‡æ–‡ç« ")

                # 3. éå†æ¯ç¯‡æ–‡ç« 
                for topic_idx, topic in enumerate(topics_list, 1):
                    if self.tasks.is_task_stopped(task_id):
                        break

                    topic_id = topic.get('topic_id')
                    topic_title = topic.get('title', 'æ— æ ‡é¢˜')[:30]
                    db.insert_column_topic(column_id, int(group_id), topic)
                    topics_count += 1

                    # å¢é‡æ¨¡å¼ï¼šæ£€æŸ¥æ–‡ç« è¯¦æƒ…æ˜¯å¦å·²å­˜åœ¨
                    if incremental_mode and db.topic_detail_exists(topic_id):
                        self.tasks.append_log(task_id, f"   ğŸ“„ [{topic_idx}/{len(topics_list)}] {topic_title}... â­ï¸ è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰")
                        skipped_count += 1
                        continue

                    self.tasks.append_log(task_id, f"   ğŸ“„ [{topic_idx}/{len(topics_list)}] {topic_title}...")

                    # è·å–æ–‡ç« è¯¦æƒ…ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                    max_retries = 10
                    topic_detail = None

                    for retry in range(max_retries):
                        if self.tasks.is_task_stopped(task_id):
                            break

                        # æ£€æŸ¥æ˜¯å¦éœ€è¦é•¿ä¼‘çœ 
                        if request_count > 0 and request_count % items_per_batch == 0:
                            sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                            self.tasks.append_log(task_id, f"      ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                            await asyncio.sleep(sleep_time)

                        # éšæœºå»¶è¿Ÿ
                        delay = random.uniform(crawl_interval_min, crawl_interval_max)
                        await asyncio.sleep(delay)

                        # è·å–æ–‡ç« è¯¦æƒ…
                        detail_url = f"https://api.zsxq.com/v2/topics/{topic_id}/info"
                        try:
                            detail_resp = requests.get(detail_url, headers=headers, timeout=30)
                            request_count += 1
                        except Exception as req_err:
                            log_exception(f"è·å–æ–‡ç« è¯¦æƒ…è¯·æ±‚å¼‚å¸¸: topic_id={topic_id}, url={detail_url}")
                            self.tasks.append_log(task_id, f"      âš ï¸ è·å–è¯¦æƒ…è¯·æ±‚å¼‚å¸¸: {req_err}")
                            continue

                        if detail_resp.status_code != 200:
                            log_error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: topic_id={topic_id}, HTTP {detail_resp.status_code}, response={detail_resp.text[:500] if detail_resp.text else 'empty'}")
                            self.tasks.append_log(task_id, f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: HTTP {detail_resp.status_code}")
                            continue

                        try:
                            detail_data = detail_resp.json()
                        except Exception as json_err:
                            log_exception(f"è§£ææ–‡ç« è¯¦æƒ…JSONå¤±è´¥: topic_id={topic_id}, response={detail_resp.text[:500] if detail_resp.text else 'empty'}")
                            self.tasks.append_log(task_id, f"      âš ï¸ è§£æè¯¦æƒ…å¤±è´¥: {json_err}")
                            continue

                        if not detail_data.get('succeeded'):
                            error_code = detail_data.get('code')
                            error_message = detail_data.get('error_message', 'æœªçŸ¥é”™è¯¯')

                            # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
                            if error_code == 1059:
                                if retry < max_retries - 1:
                                    # æ™ºèƒ½ç­‰å¾…æ—¶é—´ç­–ç•¥ï¼šå‰å‡ æ¬¡çŸ­ç­‰å¾…ï¼Œåé¢é€æ¸å¢åŠ 
                                    if retry < 3:
                                        wait_time = 2  # å‰3æ¬¡ç­‰å¾…2ç§’
                                    elif retry < 6:
                                        wait_time = 5  # ç¬¬4-6æ¬¡ç­‰å¾…5ç§’
                                    else:
                                        wait_time = 10  # ç¬¬7-10æ¬¡ç­‰å¾…10ç§’

                                    self.tasks.append_log(task_id, f"      âš ï¸ é‡åˆ°åçˆ¬æœºåˆ¶ (é”™è¯¯ç 1059)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                                    await asyncio.sleep(wait_time)
                                    continue
                                else:
                                    log_error(f"è·å–æ–‡ç« è¯¦æƒ…é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: topic_id={topic_id}, code={error_code}, message={error_message}")
                                    self.tasks.append_log(task_id, f"      âŒ é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {error_message} (code={error_code})")
                                    break
                            else:
                                log_error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: topic_id={topic_id}, code={error_code}, message={error_message}, full_response={json.dumps(detail_data, ensure_ascii=False)[:500]}")
                                self.tasks.append_log(task_id, f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: {error_message} (code={error_code})")
                                break
                        else:
                            # æˆåŠŸè·å–è¯¦æƒ…
                            topic_detail = detail_data.get('resp_data', {}).get('topic', {})
                            if retry > 0:
                                self.tasks.append_log(task_id, f"      âœ… é‡è¯•æˆåŠŸ (ç¬¬{retry+1}æ¬¡å°è¯•)")
                            break

                    # å¦‚æœæ²¡æœ‰è·å–åˆ°è¯¦æƒ…ï¼Œè·³è¿‡åç»­å¤„ç†
                    if not topic_detail:
                        continue
                    db.insert_topic_detail(int(group_id), topic_detail, json.dumps(topic_detail, ensure_ascii=False))
                    details_count += 1

                    # å¤„ç†æ–‡ä»¶ä¸‹è½½
                    if download_files:
                        talk = topic_detail.get('talk', {})
                        topic_files = talk.get('files', [])
                        content_voice = topic_detail.get('content_voice')

                        all_files = topic_files.copy()
                        if content_voice:
                            all_files.append(content_voice)

                        for file_info in all_files:
                            if self.tasks.is_task_stopped(task_id):
                                break

                            file_id = file_info.get('file_id')
                            file_name = file_info.get('name', '')
                            file_size = file_info.get('size', 0)

                            if file_id:
                                self.tasks.append_log(task_id, f"      ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {file_name[:40]}...")

                                # æ£€æŸ¥æ˜¯å¦éœ€è¦é•¿ä¼‘çœ 
                                if request_count > 0 and request_count % items_per_batch == 0:
                                    sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                                    self.tasks.append_log(task_id, f"      ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                                    await asyncio.sleep(sleep_time)

                                delay = random.uniform(crawl_interval_min, crawl_interval_max)
                                await asyncio.sleep(delay)

                                try:
                                    result = await self._download_column_file(
                                        group_id, file_id, file_name, file_size,
                                        topic_id, db, headers, task_id
                                    )
                                    if result == "downloaded":
                                        files_count += 1
                                        request_count += 1
                                        self.tasks.append_log(task_id, f"         âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ")
                                    elif result == "skipped":
                                        files_skipped += 1
                                    # "skipped" æ—¶æ—¥å¿—å·²åœ¨å‡½æ•°å†…è¾“å‡º
                                except Exception as fe:
                                    log_exception(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: file_id={file_id}, file_name={file_name}, topic_id={topic_id}")
                                    self.tasks.append_log(task_id, f"         âš ï¸ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {fe}")

                    # ç¼“å­˜å›¾ç‰‡
                    if cache_images:
                        talk = topic_detail.get('talk', {}) if 'talk' in topic_detail else {}
                        topic_images = talk.get('images', [])

                        for image in topic_images:
                            if self.tasks.is_task_stopped(task_id):
                                break

                            original_url = image.get('original', {}).get('url')
                            image_id = image.get('image_id')

                            if original_url and image_id:
                                try:
                                    cache_manager = get_image_cache_manager(group_id)
                                    success, local_path, error_msg = cache_manager.download_and_cache(original_url)
                                    if success and local_path:
                                        db.update_image_local_path(image_id, str(local_path))
                                        images_count += 1
                                    elif error_msg:
                                        self.tasks.append_log(task_id, f"      âš ï¸ å›¾ç‰‡ç¼“å­˜å¤±è´¥: {error_msg}")
                                except Exception as ie:
                                    log_exception(f"å›¾ç‰‡ç¼“å­˜å¤±è´¥: image_id={image_id}, url={original_url}")
                                    self.tasks.append_log(task_id, f"      âš ï¸ å›¾ç‰‡ç¼“å­˜å¤±è´¥: {ie}")

                    # å¤„ç†è§†é¢‘
                    talk_for_video = topic_detail.get('talk', {}) if 'talk' in topic_detail else {}
                    video = talk_for_video.get('video')

                    if video and video.get('video_id'):
                        video_id = video.get('video_id')
                        video_size = video.get('size', 0)
                        video_duration = video.get('duration', 0)
                        cover = video.get('cover', {})
                        cover_url = cover.get('url')

                        self.tasks.append_log(task_id, f"      ğŸ¬ å‘ç°è§†é¢‘: ID={video_id}, å¤§å°={video_size/(1024*1024):.1f}MB, æ—¶é•¿={video_duration}ç§’")

                        # ç¼“å­˜è§†é¢‘å°é¢ï¼ˆè·Ÿéšå›¾ç‰‡ç¼“å­˜é€‰é¡¹ï¼‰
                        if cache_images and cover_url:
                            try:
                                cache_manager = get_image_cache_manager(group_id)
                                success, cover_local, error_msg = cache_manager.download_and_cache(cover_url)
                                if success and cover_local:
                                    db.update_video_cover_path(video_id, str(cover_local))
                                    self.tasks.append_log(task_id, f"      âœ… è§†é¢‘å°é¢ç¼“å­˜æˆåŠŸ")
                                elif error_msg:
                                    log_warning(f"è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: video_id={video_id}, url={cover_url}, error={error_msg}")
                                    self.tasks.append_log(task_id, f"      âš ï¸ è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: {error_msg}")
                            except Exception as ve:
                                log_exception(f"è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: video_id={video_id}, url={cover_url}")
                                self.tasks.append_log(task_id, f"      âš ï¸ è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: {ve}")

                        # ä¸‹è½½è§†é¢‘ï¼ˆå•ç‹¬æ§åˆ¶ï¼‰
                        if download_videos:
                            if request_count > 0 and request_count % items_per_batch == 0:
                                sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                                self.tasks.append_log(task_id, f"      ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                                await asyncio.sleep(sleep_time)

                            delay = random.uniform(crawl_interval_min, crawl_interval_max)
                            await asyncio.sleep(delay)

                            try:
                                result = await self._download_column_video(
                                    group_id, video_id, video_size, video_duration,
                                    topic_id, db, headers, task_id
                                )
                                if result == "downloaded":
                                    videos_count += 1
                                    request_count += 1
                                elif result == "skipped":
                                    videos_skipped += 1
                                # "skipped" æ—¶æ—¥å¿—å·²åœ¨å‡½æ•°å†…è¾“å‡º
                            except Exception as ve:
                                log_exception(f"è§†é¢‘ä¸‹è½½å¤±è´¥: video_id={video_id}, topic_id={topic_id}, size={video_size}")
                                self.tasks.append_log(task_id, f"      âš ï¸ è§†é¢‘ä¸‹è½½å¤±è´¥: {ve}")
                        else:
                            self.tasks.append_log(task_id, f"      â­ï¸ è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰")

                    # æ›´æ–°è¿›åº¦
                    self.tasks.update_task(task_id, "running", f"è¿›åº¦: {details_count} ç¯‡æ–‡ç« , {files_count} ä¸ªæ–‡ä»¶, {videos_count} ä¸ªè§†é¢‘, {images_count} å¼ å›¾ç‰‡")

            # å®Œæˆ
            self.tasks.append_log(task_id, "")
            self.tasks.append_log(task_id, "=" * 50)
            self.tasks.append_log(task_id, "ğŸ‰ ä¸“æ é‡‡é›†å®Œæˆï¼")
            self.tasks.append_log(task_id, f"ğŸ“Š ç»Ÿè®¡:")
            self.tasks.append_log(task_id, f"   ğŸ“ ä¸“æ ç›®å½•: {columns_count} ä¸ª")
            self.tasks.append_log(task_id, f"   ğŸ“ æ–‡ç« åˆ—è¡¨: {topics_count} ç¯‡")
            self.tasks.append_log(task_id, f"   ğŸ“„ æ–‡ç« è¯¦æƒ…: {details_count} ç¯‡ï¼ˆæ–°å¢ï¼‰")
            if skipped_count > 0:
                self.tasks.append_log(task_id, f"   â­ï¸ è·³è¿‡å·²å­˜åœ¨æ–‡ç« : {skipped_count} ç¯‡")
            self.tasks.append_log(task_id, f"   ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {files_count} ä¸ª" + (f" (è·³è¿‡ {files_skipped} ä¸ªå·²å­˜åœ¨)" if files_skipped > 0 else ""))
            self.tasks.append_log(task_id, f"   ğŸ¬ ä¸‹è½½è§†é¢‘: {videos_count} ä¸ª" + (f" (è·³è¿‡ {videos_skipped} ä¸ªå·²å­˜åœ¨)" if videos_skipped > 0 else ""))
            self.tasks.append_log(task_id, f"   ğŸ–¼ï¸ ç¼“å­˜å›¾ç‰‡: {images_count} å¼ ")
            self.tasks.append_log(task_id, f"   ğŸ“¡ æ€»è¯·æ±‚æ•°: {request_count} æ¬¡")
            self.tasks.append_log(task_id, "=" * 50)

            db.update_crawl_log(log_id, columns_count=columns_count, topics_count=topics_count,
                              details_count=details_count, files_count=files_count, status='completed')
            db.close()

            skipped_info = f", è·³è¿‡ {skipped_count} ç¯‡" if skipped_count > 0 else ""
            result_msg = f"é‡‡é›†å®Œæˆ: {columns_count} ä¸ªä¸“æ , {details_count} ç¯‡æ–°æ–‡ç« {skipped_info}, {files_count} ä¸ªæ–‡ä»¶, {videos_count} ä¸ªè§†é¢‘"
            self.tasks.update_task(task_id, "completed", result_msg)

        except Exception as e:
            error_msg = str(e)
            self.tasks.append_log(task_id, "")
            self.tasks.append_log(task_id, f"âŒ é‡‡é›†å¤±è´¥: {error_msg}")
            self.tasks.update_task(task_id, "failed", f"é‡‡é›†å¤±è´¥: {error_msg}")

            try:
                if db and log_id:
                    db.update_crawl_log(log_id, status='failed', error_message=error_msg)
                    db.close()
            except:
                pass


    async def _download_column_file(self, group_id: str, file_id: int, file_name: str, file_size: int,
                                    topic_id: int, db: ZSXQColumnsDatabase, headers: dict, task_id: str = None) -> str:
        """ä¸‹è½½ä¸“æ æ–‡ä»¶

        Returns:
            str: "downloaded" è¡¨ç¤ºæ–°ä¸‹è½½, "skipped" è¡¨ç¤ºå·²å­˜åœ¨è·³è¿‡, æˆ–æŠ›å‡ºå¼‚å¸¸
        """
        # å…ˆæ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_dir(group_id)
        downloads_dir = os.path.join(group_dir, "column_downloads")
        local_path = os.path.join(downloads_dir, file_name)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°åŒ¹é…ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(local_path):
            existing_size = os.path.getsize(local_path)
            if existing_size == file_size or (file_size == 0 and existing_size > 0):
                db.update_file_download_status(file_id, 'completed', local_path)
                if task_id:
                    self.tasks.append_log(task_id, f"         â­ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ ({existing_size/(1024*1024):.2f}MB)")
                return "skipped"

        # è·å–ä¸‹è½½URLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        download_url = f"https://api.zsxq.com/v2/files/{file_id}/download_url"
        max_retries = 10
        real_url = None

        for retry in range(max_retries):
            try:
                resp = requests.get(download_url, headers=headers, timeout=30)
            except Exception as req_err:
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    await asyncio.sleep(wait_time)
                    continue
                log_exception(f"è·å–ä¸‹è½½é“¾æ¥è¯·æ±‚å¼‚å¸¸: file_id={file_id}")
                raise Exception(f"è·å–ä¸‹è½½é“¾æ¥è¯·æ±‚å¼‚å¸¸: {req_err}")

            if resp.status_code != 200:
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    await asyncio.sleep(wait_time)
                    continue
                error_msg = f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: HTTP {resp.status_code}, URL={download_url}, Response={resp.text[:500] if resp.text else 'empty'}"
                log_error(error_msg)
                raise Exception(error_msg)

            data = resp.json()
            if not data.get('succeeded'):
                error_code = data.get('code')
                error_message = data.get('error_message', 'æœªçŸ¥é”™è¯¯')

                # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
                if error_code == 1059:
                    if retry < max_retries - 1:
                        wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        log_error(f"è·å–ä¸‹è½½é“¾æ¥é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: file_id={file_id}, code={error_code}")
                        raise Exception(f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é‡åˆ°åçˆ¬é™åˆ¶")
                else:
                    error_msg = f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: code={error_code}, message={error_message}, file_id={file_id}, file_name={file_name}"
                    log_error(error_msg)
                    raise Exception(f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {error_message} (code={error_code})")
            else:
                real_url = data.get('resp_data', {}).get('download_url')
                break

        if not real_url:
            raise Exception("ä¸‹è½½é“¾æ¥ä¸ºç©º")

        # åˆ›å»ºä¸‹è½½ç›®å½•ï¼ˆdownloads_dir å’Œ local_path åœ¨å‡½æ•°å¼€å¤´å·²å®šä¹‰ï¼‰
        os.makedirs(downloads_dir, exist_ok=True)

        # ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œå¤„ç† SSL é”™è¯¯ç­‰ç½‘ç»œé—®é¢˜ï¼‰
        download_retries = 3
        last_error = None

        for download_attempt in range(download_retries):
            try:
                file_resp = requests.get(real_url, headers=headers, stream=True, timeout=300)
                if file_resp.status_code == 200:
                    with open(local_path, 'wb') as f:
                        for chunk in file_resp.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)

                    db.update_file_download_status(file_id, 'completed', local_path)
                    return "downloaded"
                else:
                    last_error = f"HTTP {file_resp.status_code}"
                    if download_attempt < download_retries - 1:
                        log_warning(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥ (å°è¯• {download_attempt + 1}/{download_retries}): {last_error}, file_id={file_id}")
                        await asyncio.sleep(2 * (download_attempt + 1))  # é€’å¢ç­‰å¾…
                        continue
            except requests.exceptions.SSLError as ssl_err:
                last_error = f"SSLé”™è¯¯: {ssl_err}"
                if download_attempt < download_retries - 1:
                    log_warning(f"æ–‡ä»¶ä¸‹è½½SSLé”™è¯¯ (å°è¯• {download_attempt + 1}/{download_retries}): file_id={file_id}, error={ssl_err}")
                    await asyncio.sleep(3 * (download_attempt + 1))  # SSLé”™è¯¯ç­‰å¾…æ›´ä¹…
                    continue
            except requests.exceptions.RequestException as req_err:
                last_error = f"ç½‘ç»œé”™è¯¯: {req_err}"
                if download_attempt < download_retries - 1:
                    log_warning(f"æ–‡ä»¶ä¸‹è½½ç½‘ç»œé”™è¯¯ (å°è¯• {download_attempt + 1}/{download_retries}): file_id={file_id}, error={req_err}")
                    await asyncio.sleep(2 * (download_attempt + 1))
                    continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        db.update_file_download_status(file_id, 'failed')
        raise Exception(f"ä¸‹è½½å¤±è´¥ (é‡è¯•{download_retries}æ¬¡): {last_error}")


    async def _download_column_video(self, group_id: str, video_id: int, video_size: int, video_duration: int,
                                     topic_id: int, db: ZSXQColumnsDatabase, headers: dict, task_id: str = None) -> str:
        """ä¸‹è½½ä¸“æ è§†é¢‘ï¼ˆm3u8æ ¼å¼ï¼‰

        Returns:
            str: "downloaded" è¡¨ç¤ºæ–°ä¸‹è½½, "skipped" è¡¨ç¤ºå·²å­˜åœ¨è·³è¿‡, æˆ–æŠ›å‡ºå¼‚å¸¸
        """
        import subprocess
        import re

        # å…ˆæ£€æŸ¥æœ¬åœ°è§†é¢‘æ˜¯å¦å·²å­˜åœ¨
        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_dir(group_id)
        videos_dir = os.path.join(group_dir, "column_videos")
        video_filename = f"video_{video_id}.mp4"
        local_path = os.path.join(videos_dir, video_filename)

        # å¦‚æœè§†é¢‘å·²å­˜åœ¨ä¸”å¤§å°>0ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(local_path):
            existing_size = os.path.getsize(local_path)
            if existing_size > 0:
                db.update_video_download_status(video_id, 'completed', '', local_path)
                if task_id:
                    self.tasks.append_log(task_id, f"         â­ï¸ è§†é¢‘å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ ({existing_size/(1024*1024):.1f}MB)")
                return "skipped"

        # è·å–è§†é¢‘URLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        video_url_api = f"https://api.zsxq.com/v2/videos/{video_id}/url"
        max_retries = 10
        m3u8_url = None

        for retry in range(max_retries):
            try:
                resp = requests.get(video_url_api, headers=headers, timeout=30)
            except Exception as req_err:
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    await asyncio.sleep(wait_time)
                    continue
                log_exception(f"è·å–è§†é¢‘é“¾æ¥è¯·æ±‚å¼‚å¸¸: video_id={video_id}")
                raise Exception(f"è·å–è§†é¢‘é“¾æ¥è¯·æ±‚å¼‚å¸¸: {req_err}")

            if resp.status_code != 200:
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    await asyncio.sleep(wait_time)
                    continue
                error_msg = f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥: HTTP {resp.status_code}, URL={video_url_api}, Response={resp.text[:500] if resp.text else 'empty'}"
                log_error(error_msg)
                raise Exception(error_msg)

            data = resp.json()
            if not data.get('succeeded'):
                error_code = data.get('code')
                error_message = data.get('error_message', 'æœªçŸ¥é”™è¯¯')

                # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
                if error_code == 1059:
                    if retry < max_retries - 1:
                        wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        log_error(f"è·å–è§†é¢‘é“¾æ¥é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: video_id={video_id}, code={error_code}")
                        raise Exception(f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é‡åˆ°åçˆ¬é™åˆ¶")
                else:
                    error_msg = f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥: code={error_code}, message={error_message}, video_id={video_id}, topic_id={topic_id}"
                    log_error(error_msg)
                    raise Exception(f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥: {error_message} (code={error_code})")
            else:
                m3u8_url = data.get('resp_data', {}).get('url')
                break

        if not m3u8_url:
            raise Exception("è§†é¢‘é“¾æ¥ä¸ºç©º")

        # åˆ›å»ºè§†é¢‘ä¸‹è½½ç›®å½•ï¼ˆvideos_dir å’Œ local_path åœ¨å‡½æ•°å¼€å¤´å·²å®šä¹‰ï¼‰
        os.makedirs(videos_dir, exist_ok=True)

        # æ›´æ–°çŠ¶æ€ä¸ºä¸‹è½½ä¸­
        db.update_video_download_status(video_id, 'downloading', m3u8_url)

        # ä½¿ç”¨ffmpegä¸‹è½½m3u8è§†é¢‘
        try:
            # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
            ffmpeg_check = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)
            if ffmpeg_check.returncode != 0:
                raise Exception("ffmpeg not available")

            # æ„å»º HTTP headers å­—ç¬¦ä¸²ç»™ ffmpeg
            # ffmpeg éœ€è¦çš„æ ¼å¼æ˜¯ "Header1: Value1\r\nHeader2: Value2\r\n"
            ffmpeg_headers = ""
            if headers.get('Cookie'):
                ffmpeg_headers += f"Cookie: {headers['Cookie']}\r\n"
            if headers.get('cookie'):
                ffmpeg_headers += f"Cookie: {headers['cookie']}\r\n"
            ffmpeg_headers += "Referer: https://wx.zsxq.com/\r\n"
            ffmpeg_headers += "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\r\n"
            ffmpeg_headers += "Origin: https://wx.zsxq.com\r\n"

            # ä½¿ç”¨ffmpegä¸‹è½½ï¼ˆå¸¦è¯·æ±‚å¤´å’Œè¿›åº¦æ˜¾ç¤ºï¼‰
            cmd = [
                'ffmpeg', '-y',
                '-headers', ffmpeg_headers,
                '-i', m3u8_url,
                '-c', 'copy',
                '-bsf:a', 'aac_adtstoasc',
                '-progress', 'pipe:1',  # è¾“å‡ºè¿›åº¦ä¿¡æ¯åˆ° stdout
                local_path
            ]

            log_info(f"å¼€å§‹ä¸‹è½½è§†é¢‘: video_id={video_id}, url={m3u8_url[:100]}...")
            if task_id:
                self.tasks.append_log(task_id, f"         ğŸ¬ å¼€å§‹ä¸‹è½½è§†é¢‘ (é¢„è®¡æ—¶é•¿: {video_duration}ç§’, å¤§å°: {video_size/(1024*1024):.1f}MB)")

            # ä½¿ç”¨ Popen å®æ—¶è¯»å–è¿›åº¦
            # åœ¨ Windows ä¸Šéœ€è¦ç‰¹æ®Šå¤„ç†ç®¡é“ç¼“å†²
            import threading
            import queue

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )

            stderr_output = []
            stdout_queue = queue.Queue()

            # ä½¿ç”¨çº¿ç¨‹è¯»å– stdoutï¼Œé¿å…é˜»å¡
            def read_stdout():
                try:
                    for line in iter(process.stdout.readline, ''):
                        if line:
                            stdout_queue.put(line)
                        if process.poll() is not None:
                            break
                except:
                    pass

            # ä½¿ç”¨çº¿ç¨‹è¯»å– stderr
            def read_stderr():
                try:
                    for line in iter(process.stderr.readline, ''):
                        if line:
                            stderr_output.append(line)
                except:
                    pass

            stdout_thread = threading.Thread(target=read_stdout, daemon=True)
            stderr_thread = threading.Thread(target=read_stderr, daemon=True)
            stdout_thread.start()
            stderr_thread.start()

            last_log_time = time.time()
            start_time = time.time()

            # è¯»å–è¿›åº¦ä¿¡æ¯
            try:
                while process.poll() is None:
                    # éé˜»å¡æ–¹å¼è·å–è¿›åº¦
                    try:
                        line = stdout_queue.get(timeout=1)

                        # è§£æ ffmpeg è¿›åº¦ä¿¡æ¯
                        # æ ¼å¼: out_time_ms=123456789
                        if line.startswith('out_time_ms='):
                            try:
                                time_ms = int(line.split('=')[1].strip())
                                current_seconds = time_ms / 1000000

                                # æ¯ 3 ç§’æ›´æ–°ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…åˆ·å±
                                now = time.time()
                                if task_id and (now - last_log_time) >= 3:
                                    if video_duration > 0:
                                        progress_pct = min(100, (current_seconds / video_duration) * 100)
                                        # ç”Ÿæˆè¿›åº¦æ¡
                                        bar_length = 20
                                        filled = int(bar_length * progress_pct / 100)
                                        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                                        self.tasks.append_log(task_id, f"         ğŸ“Š ä¸‹è½½è¿›åº¦: [{bar}] {progress_pct:.1f}% ({current_seconds:.0f}s/{video_duration}s)")
                                    else:
                                        self.tasks.append_log(task_id, f"         ğŸ“Š ä¸‹è½½è¿›åº¦: {current_seconds:.0f}ç§’")
                                    last_log_time = now
                            except:
                                pass
                    except queue.Empty:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºç­‰å¾…ä¸­çš„è¿›åº¦
                        now = time.time()
                        elapsed = now - start_time
                        if task_id and (now - last_log_time) >= 5:
                            self.tasks.append_log(task_id, f"         â³ ä¸‹è½½ä¸­... (å·²ç”¨æ—¶ {elapsed:.0f}ç§’)")
                            last_log_time = now
                        continue

                # ç­‰å¾…çº¿ç¨‹ç»“æŸ
                stdout_thread.join(timeout=5)
                stderr_thread.join(timeout=5)

            except Exception as e:
                process.kill()
                raise Exception(f"è§†é¢‘ä¸‹è½½å¼‚å¸¸: {e}")

            returncode = process.returncode
            stderr_text = ''.join(stderr_output)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸä¸‹è½½ï¼ˆffmpeg å¯èƒ½è¿”å›é 0 ä½†æ–‡ä»¶å·²æˆåŠŸä¸‹è½½ï¼‰
            if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
                db.update_video_download_status(video_id, 'completed', m3u8_url, local_path)
                final_size = os.path.getsize(local_path)
                log_info(f"è§†é¢‘ä¸‹è½½æˆåŠŸ: video_id={video_id}, path={local_path}, size={final_size}")
                if task_id:
                    self.tasks.append_log(task_id, f"         âœ… è§†é¢‘ä¸‹è½½å®Œæˆ ({final_size/(1024*1024):.1f}MB)")
                return "downloaded"
            else:
                db.update_video_download_status(video_id, 'failed', m3u8_url)
                # ä» stderr ä¸­æå–çœŸæ­£çš„é”™è¯¯ä¿¡æ¯ï¼ˆè·³è¿‡ç‰ˆæœ¬ä¿¡æ¯ç­‰ï¼‰
                stderr_lines = stderr_text.strip().split('\n')
                # æŸ¥æ‰¾åŒ…å« "error" æˆ– "failed" çš„è¡Œ
                error_lines = [line for line in stderr_lines if 'error' in line.lower() or 'failed' in line.lower() or 'invalid' in line.lower()]
                if error_lines:
                    error_msg = '; '.join(error_lines[-3:])  # å–æœ€å 3 æ¡é”™è¯¯
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°æ˜ç¡®é”™è¯¯ï¼Œå–æœ€åå‡ è¡Œ
                    error_msg = '; '.join(stderr_lines[-3:]) if stderr_lines else 'unknown error'
                log_error(f"ffmpegä¸‹è½½å¤±è´¥: video_id={video_id}, returncode={returncode}, error={error_msg}")
                raise Exception(f"ffmpegä¸‹è½½å¤±è´¥: {error_msg[:300]}")

        except FileNotFoundError:
            # ffmpegä¸å¯ç”¨ï¼Œä¿å­˜m3u8é“¾æ¥ä¾›æ‰‹åŠ¨ä¸‹è½½
            db.update_video_download_status(video_id, 'pending_manual', m3u8_url)
            # ä¿å­˜m3u8é“¾æ¥åˆ°æ–‡ä»¶
            m3u8_link_file = os.path.join(videos_dir, f"video_{video_id}.m3u8.txt")
            with open(m3u8_link_file, 'w', encoding='utf-8') as f:
                f.write(f"Video ID: {video_id}\n")
                f.write(f"Duration: {video_duration} seconds\n")
                f.write(f"Size: {video_size} bytes\n")
                f.write(f"M3U8 URL: {m3u8_url}\n")
            raise Exception("ffmpegæœªå®‰è£…ï¼Œå·²ä¿å­˜m3u8é“¾æ¥åˆ°æ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½")
        except subprocess.TimeoutExpired:
            db.update_video_download_status(video_id, 'failed', m3u8_url)
            raise Exception("è§†é¢‘ä¸‹è½½è¶…æ—¶")


    # migrated to api/routers/columns.py: 

    def get_columns_stats(self, group_id: str) -> Dict[str, Any]:
        db = self._get_columns_db(group_id)
        try:
            return db.get_stats(int(group_id))
        finally:
            db.close()

    def delete_all_columns(self, group_id: str) -> Dict[str, Any]:
        db = self._get_columns_db(group_id)
        try:
            stats = db.clear_all_data(int(group_id))
            return {"success": True, "message": "å·²æ¸…ç©ºä¸“æ æ•°æ®", "deleted": stats}
        finally:
            db.close()

    def get_column_topic_full_comments(self, group_id: str, topic_id: int) -> Dict[str, Any]:
        cookie = self._resolve_cookie_for_group(group_id)
        if not cookie:
            raise HTTPException(status_code=400, detail="No valid account found for this group")

        headers = self._build_stealth_headers(cookie)
        comments_url = f"https://api.zsxq.com/v2/topics/{topic_id}/comments?sort=asc&count=30&with_sticky=true"
        log_info(f"Fetching comments from: {comments_url}")

        resp = requests.get(comments_url, headers=headers, timeout=30)
        if resp.status_code != 200:
            log_error(f"Failed to fetch comments: HTTP {resp.status_code}, response={resp.text[:500] if resp.text else 'empty'}")
            raise HTTPException(status_code=resp.status_code, detail=f"Failed to fetch comments: HTTP {resp.status_code}")

        data = resp.json()
        log_debug(
            f"Comments API response: succeeded={data.get('succeeded')}, resp_data keys={list(data.get('resp_data', {}).keys()) if data.get('resp_data') else 'None'}"
        )
        if not data.get("succeeded"):
            resp_data = data.get("resp_data", {})
            error_msg = resp_data.get("message") or resp_data.get("error_msg") or data.get("error_msg") or data.get("message")
            error_code = resp_data.get("code") or resp_data.get("error_code") or data.get("code")
            log_error(f"Comments API failed: code={error_code}, message={error_msg}, full_response={json.dumps(data, ensure_ascii=False)[:500]}")
            raise HTTPException(status_code=400, detail=f"API error: {error_msg or 'Request failed'} (code: {error_code})")

        comments = data.get("resp_data", {}).get("comments", [])
        processed_comments = []
        for comment in comments:
            processed = {
                "comment_id": comment.get("comment_id"),
                "parent_comment_id": comment.get("parent_comment_id"),
                "text": comment.get("text", ""),
                "create_time": comment.get("create_time"),
                "likes_count": comment.get("likes_count", 0),
                "rewards_count": comment.get("rewards_count", 0),
                "replies_count": comment.get("replies_count", 0),
                "sticky": comment.get("sticky", False),
                "owner": comment.get("owner"),
                "repliee": comment.get("repliee"),
                "images": comment.get("images", []),
            }
            replied_comments = comment.get("replied_comments", [])
            if replied_comments:
                processed["replied_comments"] = [
                    {
                        "comment_id": rc.get("comment_id"),
                        "parent_comment_id": rc.get("parent_comment_id"),
                        "text": rc.get("text", ""),
                        "create_time": rc.get("create_time"),
                        "likes_count": rc.get("likes_count", 0),
                        "owner": rc.get("owner"),
                        "repliee": rc.get("repliee"),
                        "images": rc.get("images", []),
                    }
                    for rc in replied_comments
                ]
            processed_comments.append(processed)

        try:
            db = self._get_columns_db(group_id)
            saved_count = db.import_comments(topic_id, processed_comments)
            db.close()
            log_info(f"Saved {saved_count} comments to database for topic {topic_id}")
        except Exception as e:
            log_error(f"Failed to save comments to database: {e}")

        total_count = sum(1 + len(c.get("replied_comments", [])) for c in processed_comments)
        return {"success": True, "comments": processed_comments, "total": total_count}
